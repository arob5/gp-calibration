\documentclass[12pt]{article}
\RequirePackage[l2tabu, orthodox]{nag}
\usepackage[main=english]{babel}
\usepackage[rm={lining,tabular},sf={lining,tabular},tt={lining,tabular,monowidth}]{cfr-lm}
\usepackage{amsthm,amssymb,latexsym,gensymb,mathtools,mathrsfs}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{epstopdf,enumitem,microtype,dcolumn,booktabs,hyperref,url,fancyhdr}
\usepackage{algorithmic}
\usepackage[ruled,vlined,commentsnumbered,titlenotnumbered]{algorithm2e}

% Plotting
\usepackage{pgfplots}
\usepackage{xinttools} % for the \xintFor***
\usepgfplotslibrary{fillbetween}
\pgfplotsset{compat=1.8}
\usepackage{tikz}

% Custom Commands
\newcommand*{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand*{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand*{\suchthat}{\,\mathrel{\big|}\,}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\Ker}{\mathrm{Ker}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Prob}{\mathbb{P}}
\DeclarePairedDelimiterX\innerp[2]{(}{)}{#1\delimsize\vert\mathopen{}#2}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}

\setlist{topsep=1ex,parsep=1ex,itemsep=0ex}
\setlist[1]{leftmargin=\parindent}
\setlist[enumerate,1]{label=\arabic*.,ref=\arabic*}
\setlist[enumerate,2]{label=(\alph*),ref=(\alph*)}

% For embedding images
\graphicspath{ {./images/} }

% Specifically for paper formatting 
\renewcommand{\baselinestretch}{1.2} % Spaces manuscript for easy reading

% Formatting definitions, propositions, etc. 
\newtheorem{definition}{Definition}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{corollary}{Corollary}

% Title and author
\title{Calibration of Terrestrial Carbon Models}
\author{Andrew Roberts}

\begin{document}

\maketitle
\tableofcontents
\newpage

\section{General Setting: Computer Model Calibration}
TODO

\section{Motivating Example: Very Simple Ecosystem Model}
The goal of this paper is to discuss the unique challenges of calibrating complex process-based ecosystem models. To motivate this problem, we introduce 
a simplified vegetation model of carbon dynamics; namely, the \textit{Very Simple Vegetation Model} (VSEM) introduced by Hartig et al \cite{Hartig} and implemented 
in the \href{https://github.com/florianhartig/BayesianTools}{\textit{BayesianTools}} R package. 
This simple model consists of a system of ordinary differential equations (ODEs) describing the fluxes of carbon between three different pools (states): 
above-ground vegetation, below-ground vegetation, and soil organic matter. This is coupled with a single forcing variable, the quantity of photosynthetically 
active radiation (PAR), which represents the portion of the light spectrum usable by plants for photosynthesis. 
Let $C_v(t)$ (\textbf{v}egetation, above ground), $C_r(t)$ (\textbf{r}oots), and $C_s(t)$ (\textbf{s}oil) denote the quantity of carbon (kg $C/m^2$) in each of the three respective pools at time $t$. 
The dynamics describing the carbon fluxes between these pools depend on $NPP(t)$, the Net Primary Productivity (NPP) ($\text{kg } C/m^2/\text{day}$) at time $t$, 
which is calculated as the Gross Primary Productivity (GPP) minus carbon released due to autotrophic respiration, where GPP quantifies the amount of carbon 
fixed by vegetation during photosynthesis. Given NPP, the VSEM model calculates Net Ecosystem Exchange (NEE), which is (aside from a sign change) 
NPP minus heterotrophic 
respiration. Thus, in the VSEM model this means 
\begin{align}
-\text{NEE} = \text{ GPP } - \text{ Plant Respiration } - \text{ Soil Respiration } 
\end{align}
The sign convention is that positive NPP indicates a flux into the ecosystem, while a positive NEE indicates a flux into the atmosphere, hence the addition of the negative. 
The state equations describing the carbon dynamics are then given by
\begin{align}
\dot{C}_v(t) &= \alpha_v \times \text{NPP}(t) - \frac{C_v(t)}{\tau_v} \label{VSEM_ODE_system} \\
\dot{C}_r(t) &= (1.0 - \alpha_v) \times \text{NPP}(t) - \frac{C_r(t)}{\tau_r} \nonumber \\
\dot{C}_s(t) &= \frac{C_r(t)}{\tau_r} + \frac{C_v(t)}{\tau_v} - \frac{C_s(t)}{\tau_s} \nonumber 
\end{align}
where the parameters $\tau_v$, $\tau_r$, and $\tau_s$ are residence times or longevity parameters for above-ground vegetation, below-ground vegetation, and soil organic matter, 
respectively. In particular, $\tau_v$ and $\tau_r$ represent the longevity of above and below ground biomass, respectively, while $\tau_s$ is the residence time of organic matter 
in the soil. Carbon is thus assumed to be lost from the plant pools to the soil pool at fixed turnover rates, and similarly from the soil pool to the atmosphere. VSEM also makes the simplifying assumption that a fixed proportion of NPP is allocated to above and below ground vegetation, where $\alpha_v$ is the fraction
allocated to the former. 

The dynamics [\ref{VSEM_ODE_system}] are driven by the forcing effect of PAR, which drives the values of NPP and GPP over time. VSEM assumes a simple calculation, 
where GPP is given by a product of three factors:
\begin{enumerate}
\item The amount of light available for photosynthesis (PAR) (MJ/$m^2$/day).
\item Light-use efficiency (LUE), a measure of the efficiency at which vegetation can use light for photosynthesis. 
\item The rate at which the available light decays as it passes downwards through a canopy of leaves. 
\end{enumerate}
The rate described in the third item above is modeled by the Beer-Lambert law, which yields an exponential decay rate $e^{-k*\text{LAI}}$, 
where LAI is the \textit{leaf-area index}, defined as the ratio of one-sided leaf area per unit of ground area. LAI at time $t$ is assumed to be given by the product of a fixed 
leaf-area ratio (LAR) and $C_v(t)$. The constant $k$ is a fixed extinction rate controlling the rate of exponential decay. The full calculations for NPP are given below. 
\begin{align}
\text{LAI}(t) &= \text{LAR} \times C_v(t) \\
\text{GPP}(t) &= \text{PAR}(t) \times \text{LUE} \times \left(1 -  \exp\left(-k \times \text{LAI}(t) \right) \right) \nonumber \\
\text{NPP}(t) &= (1 - \gamma) \times \text{GPP}(t) \nonumber
\end{align}
As seen above, NPP is assumed to be a fixed fraction $1 - \gamma$ of GPP.

Potential calibration parameters $\theta$ of this model include $\alpha_v$, $\tau_v$, $\tau_r$, $\tau_s$, $\text{LAR}$, $k$, and $\gamma$, but it is common to fix some of the parameters at their 
nominal values and calibrate the remaining subset. 
Once the parameters $\theta$ and a specified number of time steps (days) $n$ are fixed, the ODE system can be solved, yielding daily time series of $n$
observations for NEE, $C_v$, $C_r$, and $C_s$. The forward model $f$ in this setting is given by an execution of the ODE solve, and represents
the mapping from the inputs to these four resulting time series. A more 
detailed discussion of the forward model is given in the following section. 

\subsection{Relation to the General Computer Model Calibration Setting}
We now consider how ecosystem models such as VSEM relate to, and differ from, the KOH formulation. As previously discussed, the KOH model assumes that
computer simulators take the form $f(x, \theta)$, requiring specification of control inputs $x$ and calibration parameters $\theta$. The calibration parameters 
in the case of VSEM were stated in the previous section. But are there control inputs in this setting? It depends how we view the model. If we let 
$x = (i, j)$, where $i$ indexes the time series output and $j$ indexes the output variable (NEE, $C_v$, etc.) then $f(x, \theta) = f(i, j, \theta)$ can indeed be thought of in the 
classical KOH sense. Since $f(i, j, \theta)$ maps to the value of a single output $j$ at a single time step $i$ then it is a scalar-valued computer model. Relating the 
model to field data will yield statistical models of the form 
\begin{align}
y_{ij} &= f(i, j, \theta) + \epsilon_{ij} \label{ij_model}
\end{align}
Models for the error process $\epsilon_{ij}$ might consider time-invariant correlation across outputs $j$, as well as autocorrelation across time $i$. 

For a different perspective, let us now conceptualize the computer model as $f(i, \theta)$, a vector-valued simulator that outputs values for the four outputs
at time $i$. We can now consider an observational model of the form 
\begin{align}
y_{i} &= f(i, \theta) + \epsilon_{i} \label{i_model}
\end{align}
where $\epsilon_{i}$ is a four-dimensional vector of errors. We can take this a step further and consider a computer model with no control inputs
$f(\theta)$ that outputs a matrix of dimensions $n \times 4$ representing the full time-series for the four outputs. The corresponding statistical model looks 
like
\begin{align}
y = f(\theta) + \epsilon \label{no_control_input_model}
\end{align}
where $\epsilon$ is now a large matrix of errors. On the face of it, these definitions appear to hold only conceptual value. If we wanted to fit a simple 
statistical model assuming independent observation error equal in magnitude across outputs and time then we could consider 
$\epsilon_{ij} \overset{iid}{\sim} N(0, \sigma_\epsilon^2)$, 
$\epsilon_i \overset{iid}{\sim} N_4(0, \sigma_\epsilon^2 I_4)$, and $\epsilon \sim N_{n \times 4}(0, \sigma_\epsilon^2 I_{n \times 4})$ 
in the three respective models, and the three resulting models would be identical. However, in the setting where $f$ is sufficiently costly 
as to require emulation, the distinction between the three formulations has vastly different practical ramifications. 

We now briefly consider what model emulation might look like in each of the three cases; a thorough discussion of the costs and benefits of each 
is delayed until later in this paper. This discussion parallels one presented in 
Conti and O'Hagan \cite{Conti}, which also happened to be motivated by the problem of calibrating a vegetation model to study 
carbon dynamics. However, their study presents the case of a single time-series output, while we have been considering multiple. 
Note that in moving from [\ref{ij_model}] to 
[\ref{i_model}] and then to [\ref{no_control_input_model}], we simply conceptually shifted model \textit{inputs} (indicators for time and 
output variable) to model \textit{outputs}. This yields a scalar-valued model in [\ref{ij_model}] and ultimately a model with a very large 
number of outputs in [\ref{no_control_input_model}]. 

Starting with the first model, $f(i, j, \theta)$, we can consider a single emulator that maps $(i, j, \theta) \mapsto \R$. In particular, the input space of this 
emulator is $\{1, \dots, n\} \times \{1, \dots, p\} \times \mathcal{D}_{\theta}$, where I have returned to the generic notation $p$ to represent the number 
of outputs ($p = 4$ for VSEM). Although only a single emulator must be built in this formulation, it must be able to accurately capture the dynamics of the
simulation over time; in other words, the dynamics are baked into the emulator itself. Conti and O'Hagan \cite{Conti} refer to this as the 
\textit{time input (TI) emulator}, although in our setting the output index $j$ is also an input in addition to time. While the apparent simplicity of fitting a 
single univariate-output emulator is attractive, there may be some issues with this approach. Fitting an emulator that accurately re-creates the changing dynamics
over the full time series for each output at different $\theta$ values seems a potentially daunting task. Conti and O'Hagan present an alternative 
\textit{many single output (MS) emulators} scheme, in which $n$ independent emulators are constructed, one for each time step. When $n$ is large, this has the 
obvious drawback of requiring fitting a very large number of emulators. Turning to the second model $f(i, \theta)$, we can explore the same exact ideas, with slight modifications. 
The TI approach now requires a single multi-output 
emulator with $p$ outputs, which predicts the $p$ outputs at each time step. The MS scheme now requires $n$ $p$-output emulators. 

For the final model $f(\theta)$, we require a single emulator that maps $\theta \mapsto \R^{n \times p}$, which Conti and O'Hagan term the 
\textit{multi-output (MO) emulator} approach. In most applications, $n$ is large and so fitting an emulator 
with such a high-dimensional output space is completely infeasible. The situation can sometimes be ameliorated via dimensionality reduction techniques such as 
Principal Components Analysis (PCA). Alternatively, we might abandon emulation of $f$ directly and instead emulate the likelihood directly. Under a Gaussian error
model this will look something like $p(y|\theta, \Sigma_\epsilon) = N_{n \times p}(f(\theta), \Sigma_\epsilon)$. If we only care about the computer model insofar as 
the error in its predictions relative to field observations $y$, then targeting the likelihood for emulation drastically simplifies the problem from an $n \times p$ 
dimensional to a univariate output space! Conceptually, in this setting we are still viewing the time series as outputs (as opposed to the model $f(i, j, \theta)$) but now 
we are projecting this high-dimensional output space onto a one-dimensional space by mapping the time series through the likelihood function. As with all of the other
methods, there are still issues with this approach, and a more thorough comparison is explored later in the paper. On a notational note, we will continue to utilize 
$f(i, j, \theta)$, $f(i, \theta)$, and $f(\theta)$ as a practical means to describe algorithms, regardless of whether time and output dimension are conceptually thought 
of as inputs or outputs of the model. In particular, $f(i, j, \theta) \in \R$ is a scalar denoting the value of output $j$ at time $i$, $f(i, \theta) \in \R^p$ is a column vector 
of the $p$ outputs at time $i$, and $f(\theta) \in \R^{n \times p}$ is a matrix of all outputs at all times (where the specific set of times will be made clear by context). 

To conclude the conceptual framework 
explored here, we emphasize the characteristics of ecosystem models that made calibration challenging relative to the classical KOH formulation. In particular, KOH originally 
considered computer models with univariate outputs, which was subsequently generalized to a (typically modest) number of outputs. On the other hand, 
vegetation models like VSEM produce multiple time series of outputs, yielding a very high-dimensional output space. Alterations to classical approaches are therefore
required to deal with the complexity in this setting.  

\subsection{A Note on Notation and the Matrix Normal Distribution}
As illustrated in the above discussion, our current analysis is concerned with correlation across time and output variables (we will later consider spatial correlation as well). The 
notation for considering statistical models across these dimensions can be cumbersome, so we take a moment to clearly define notation here. Restricting our attention to Gaussian 
likelihoods will place the primary modeling focus on the covariance matrix summarizing associations across time and output variables. With $n$-length time series and $p$ output 
variables in general we may consider any $np \times np$ covariance matrix, but we typically entertain specific covariance structures for computational tractability. To motivate the 
following definitions, consider the specific model in which we assume independence across time but represent output variable covariances through the matrix $\Sigma_\epsilon \in \R^{p \times p}$. 
In the notation established in the previous section, this may be written
\begin{align}
y_i \overset{ind}{\sim} N_p(f(i, \theta), \Sigma_\epsilon) \label{example_assumption}
\end{align}
The full $np \times np$ covariance matrix implied by this model is simply block-diagonal $\text{diag}(\Sigma_\epsilon, \dots, \Sigma_\epsilon)$, which can be equivalently written as the 
Kronecker product $I_n \otimes \Sigma_\epsilon$. The random vector associated with this large covariance matrix is the vectorized version of $Y \in \R^{n \times p}$, constructed 
by appending its \textit{rows} in order to create the large vector $Y_{\text{vec}} \in \R^{np}$. We can similarly 
vectorize $f(\theta) \in \R^{n \times p}$ to produce $f_{\text{vec}}(\theta) \in \R^{np}$. With this notation, we may re-write [\ref{example_assumption}] as
\begin{align}
Y_{\text{vec}} &\sim N_{np}(f_{\text{vec}}(\theta), I_n \otimes \Sigma_\epsilon)
\end{align}
Yet another equivalent representation that doesn't require re-shaping $Y$ and $f(\theta)$ is to use the \textit{matrix normal distribution}
\begin{align}
Y &\sim N_{n \times p}(f(\theta), \Sigma_\epsilon, I_n)
\end{align}
which yields a parameterization in terms of the location $f(\theta)$ and two scale parameters $\Sigma_\epsilon$ and $I_n$.

An obvious generalization in the above model is to replace the diagonal matrix $I_n$ with an arbitrary covariance matrix $\Sigma_T \in \R^{n \times n}$ correlations across time. 
This yields a general model structure 
\begin{align}
Y &\sim N_{n \times p}(f(\theta), \Sigma_\epsilon, \Sigma_T)
\end{align}
which will be explored throughout this paper. The time-output variable covariance matrix implied by this model is
\begin{align}
\Sigma_T \otimes \Sigma_\epsilon &= \begin{pmatrix} (\Sigma_T)_{11}\Sigma_\epsilon & \hdots & (\Sigma_T)_{1n}\Sigma_\epsilon \\
										   \vdots & \hdots & \vdots \\
										   (\Sigma_T)_{n1}\Sigma_\epsilon & \hdots & (\Sigma_T)_{nn}\Sigma_\epsilon \end{pmatrix}
\end{align}
Such structure allows a time effect to modify the output covariance across time.  


\section{Parameter Calibration Basics}
In this section, we consider the basic parameter calibration problem under the assumption that the model $f$ is sufficiently fast as to not require emulation. 

\subsection{Basic Model}
TODO

\subsection{Correlated Outputs} \label{corr_outputs}
In the previous section, we assumed independence between model outputs, which resulted in a convenient likelihood. To be clear, we applied the independence assumption
\begin{align}
p(y|\theta, \Sigma_\epsilon) = \prod_{j = 1}^{p} p(y_j|\theta, \sigma_{\epsilon_j}^2)
\end{align}
where $y \in \R^p$ is a single observation of the $p$ outputs. In the case that this assumption is inappropriate, the previous models may lead to inaccurate 
quantification of uncertainties. In this case, we consider an explicit model for the covariance between model outputs. We begin by introducing a simple model 
that generalizes $\Sigma_\epsilon = \text{diag}\left(\sigma_{\epsilon_1}^2, \dots, \sigma_{\epsilon_p}^2\right)$ from a diagonal to an arbitrary, dense covariance 
matrix. Since $\Sigma_\epsilon$ is now parameterized by $\frac{p(p+1)}{2}$ parameters, this model is only computational feasible when the number of outputs is 
quite small. Nevertheless, it is a good starting point to motivate more complicated models. 

Under a Gaussian observation model, the likelihood for a single observation $y_i \in \R^p$ now becomes 
\begin{align}
p(y_i|\theta, \Sigma_\epsilon) &= N_p(y_i|f(i, \theta), \Sigma_\epsilon).
\end{align}
The likelihood across all observations is thus
\begin{align}
p(Y|\theta, \Sigma_\epsilon) &= \prod_{i = 1}^{n} N_p(y_i|f(i, \theta), \Sigma_\epsilon). \label{corr_output_lik}
\end{align}
We should emphasize that we are still assuming independent observation noise across \textit{time}; this model only accounts for correlations across outputs 
independent of time. The assumption that this output covariance $\Sigma_\epsilon$ is constant across time can be relaxed, at the cost of further increasing the 
complexity of the model. A numerically stable implementation of the log of [\ref{corr_output_lik}] is provided in the appendix [\ref{corr_output_lik_implementation}].

\subsection{Fully-Worked Example and MCMC Algorithm}
We now consider inference for the model [\ref{corr_output_lik}], which assumes independence across time but models correlation between outputs. Since we are taking a Bayesian approach, 
to complete the model we must assign priors on $\theta$ and $\Sigma_\epsilon$. We will consider a generic prior on $\theta$, but choose an inverse Wishart prior for $\Sigma_\epsilon$, as it
provides conjugate structure that can be exploited to yield a Metropolis-within-Gibbs sampling scheme. The full model can thus be written
\begin{align}
y_i|\theta, \Sigma_\epsilon &\overset{ind}{\sim} N_p(f(i, \theta), \Sigma_\epsilon), \ i = 1, \dots, n \\
\Sigma_\epsilon |\theta &\sim \text{Inv-Wishart}(\Sigma_0, \nu) \nonumber \\
\theta &\sim \pi_0 \nonumber
\end{align}
To sample from the joint posterior $p(\theta, \Sigma_\epsilon|Y)$ we consider the conditionals $p(\theta|\Sigma_\epsilon, Y)$ and $p(\Sigma_\epsilon |\theta, Y)$, where only the latter is available 
in closed-form. Therefore, we consider a Metropolis-within-Gibbs approach where each MCMC consists of a Metropolis step to sample $\theta$ conditional on $\Sigma_\epsilon$, and then a Gibbs
step to sample from $\Sigma_\epsilon$ conditional on the new $\theta$ value. 




\section{Surrogate Modeling}

\subsection{Multiple Time Series Outputs}
Here we consider formulating a calibration model with multiple time series outputs that 1.) accounts for correlation between output variables, 
and 2.) builds an emulator to overcome the long compute time of $f$. We retain the assumption that, within each time series, the observation 
errors are iid Gaussian. Essentially, we seek a reformulation of the model [\ref{corr_outputs}] that adjusts to the expensive computer model 
regime. Recall the likelihood from this model:
\begin{align}
p(Y|\theta, \Sigma_\epsilon) &= \prod_{i = 1}^{n} N_p(y_i|f(i, \theta), \Sigma_\epsilon). \label{corr_output_lik}
\end{align}

\subsubsection{Likelihood Emulation}
To begin, consider the assumption that the output variables are independent. Then the log-likelihood assumes the form
\begin{align}
\log p(Y|\theta, \Sigma_\epsilon) &= \sum_{i = 1}^{n} \sum_{j = 1}^{p} \log N(y_{ij}|f(i, j, \theta), \sigma^2_{\epsilon_j}) \\
					  	  &= -\frac{np}{2} \log(2\pi \sigma^2_{\epsilon_j})  - \frac{1}{2} \sum_{i = 1}^{n} \sum_{j = 1}^{p} \frac{1}{\sigma^2_{\epsilon_j}} (y_{ij} - f(i, j, \theta))^2 \nonumber \\
						  &= -\frac{np}{2} \log(2\pi \sigma^2_{\epsilon_j}) - \frac{1}{2} \sum_{j = 1}^{p} \frac{1}{\sigma^2_{\epsilon_j}} \sum_{i = 1}^{n} (y_{ij} - f(i, j, \theta))^2 \nonumber
\end{align}
The iid Gaussian assumption is quite convenient, as it decouples the simulator $f(i, j, \theta)$ from the observation variances $\sigma^2_{\epsilon_j}$ in the log-likelihood. 
Therefore, instead of emulating the entire log-likelihood (which is a function of the unknown variance parameters) we can simply emulate the $p$ functions
\begin{align}
T_j(\theta) &= \sum_{i = 1}^{n} (y_{ij} - f(i, j, \theta))^2, \ j = 1, \dots, p \label{SSR}
\end{align}
which do not depend on the $\sigma^2_{\epsilon_j}$. Since the $T_j$ define the sum of squared residuals for each output variable, they are strictly non-negative 
functions. Therefore, to constrain the GP emulators we can instead emulate the functions defined by $L_j(\theta) := \log(T_j(\theta))$. 
Letting $L_j^*$, $j = 1, \dots, p$ denote the fitted $p$ univariate, independent GP emulators for these functions, then the approximate log-likelihood becomes
\begin{align}
\log p_{T^*(\theta)}(Y|\theta, \Sigma_\epsilon) &:= -\frac{np}{2} \log(2\pi \sigma^2_{\epsilon_j})  - \frac{1}{2} \sum_{j = 1}^{p} \frac{1}{\sigma^2_{\epsilon_j}} \exp\left(L^*_j(\theta)\right) \label{approx_emulator_llik_indep}
\end{align}
Note that the $L^*_j(\theta)$ are random approximations; integrating out these quantities requires numerical methods due to the exponentiation, or a stochastic emulator 
(i.e. sampling) approach may be used. In either case, the exact log-likelihood may then be replaced by the approximation in an MCMC or optimization procedure. We 
emphasize that the specific structure of the independent Gaussian allows the (log) sum of squared residuals to be emulated, and thus allows inference over both the 
observation variances $\sigma^2_{\epsilon_j}$ and the calibration parameters $\theta$. 

This convenient decoupling of $\sigma^2_{\epsilon_j}$ and $\theta$ is lost when 
more complicated error structures are considered. For example, suppose that the assumption of independence between the output variables is inappropriate. We thus 
consider the model [\ref{corr_output_lik}] where $\Sigma_\epsilon$ has non-diagonal structure. For now we consider the case of a small number of outputs so that 
it is reasonable to allow $\Sigma_\epsilon$ to have dense structure. The log-likelihood under this model is 
\begin{align}
\log p(Y|\theta, \Sigma_\epsilon) &= -\frac{1}{2} \log(2\pi) -\frac{n}{2} \log \det(\Sigma_\epsilon) - \frac{1}{2} \sum_{i = 1}^{n} (y_i - f(i, \theta))^T \Sigma_\epsilon^{-1} (y - f(i, \theta)) \label{log_lik_corr_outputs}
\end{align}
where we now see that there is no sufficient statistic (as in the sum of squared residuals) that can be emulated separately from $\Sigma_\epsilon$. If $\Sigma_\epsilon$
is known a priori, then it can be treated as fixed and we can consider the target of emulation to be 
$\ell(\theta) := \log p(Y|\theta, \Sigma_\epsilon): \mathcal{D}_\theta \to \R$. In the more common situation where $\Sigma_\epsilon$ is not known, then the requirement of 
fixing $\Sigma_\epsilon$ prior to emulation is unsatisfactory from an uncertainty quantification perspective. An alternative idea is to consider $\Sigma_\epsilon$ as an 
additional input to the emulator, and thus emulate the unnormalized negative log-likelihood
\begin{align}
\Phi(\theta, \Sigma_\epsilon) &:= \frac{n}{2} \log \det(\Sigma_\epsilon)  \frac{1}{2} \sum_{i = 1}^{n} (y_i - f(i, \theta))^T \Sigma_\epsilon^{-1} (y - f(i, \theta))
\end{align} 
which represents a mapping $\mathcal{D}_\theta \times \mathcal{S}^+_p \to \R$, where $\mathcal{S}^+_p$ is the space of $p \times p$ positive definite matrices. 
Due to the symmetry of the covariance matrices, this yields a $d_{\theta} + \frac{p(p + 1)}{2}$ input space. For moderate numbers of calibration parameters 
$d_\theta$ and small numbers of output variables $p$ this may be reasonable, but clearly will struggle to scale to larger problems. We can also consider parameterizing 
the covariance matrix is forms more conducive to learning, e.g. via Cholesky factors. Letting $\Phi^* \sim \mathcal{GP}(\mu_\Phi^*, k_\Phi^*)$ denote the fit emulator for
$\Phi(\theta, \Sigma_\epsilon)$, then the approximate likelihood is given by 
\begin{align}
p_{\Phi^*}(Y|\theta, \Sigma_\epsilon) &:= \exp\left\{-\Phi^*(\theta, \Sigma_\epsilon)\right\}
\end{align}
This random approximation to the true negative log-likelihood thus has a log-normal distribution 
\begin{align}
p_{\Phi^*}(Y|\theta, \Sigma_\epsilon) &\sim \text{LN}\left(\mu^*_\Phi(\theta, \Sigma_\epsilon), k^*_\Phi(\theta, \Sigma_\epsilon) \right)
\end{align}
and so $\Phi^*$ can be analytically integrated out as
\begin{align}
\E_{\Phi^*}\left[p_{\Phi^*}(Y|\theta, \Sigma_\epsilon) \right] &= \exp\left\{-\mu^*_\Phi(\theta, \Sigma_\epsilon) - \frac{1}{2}k^*_\Phi(\theta, \Sigma_\epsilon) \right\}
\end{align}

\section{Multi-Site Hierarchical Models}

\section{Appendix}

\subsection{Correlated Gaussian Likelihood Implementation} \label{corr_output_lik_implementation}
We consider coding the log of [\ref{corr_output_lik}] efficiently, which is especially important when it will need to be evaluated at each iteration of an
MCMC algorithm. The full log-likelihood is given by 
\begin{align}
\log p(Y|\theta, \Sigma_\epsilon) &= -\frac{1}{2} \log(2\pi) -\frac{n}{2} \log \det(\Sigma_\epsilon) - \frac{1}{2} \sum_{i = 1}^{n} (y_i - f(i, \theta))^T \Sigma_\epsilon^{-1} (y - f(i, \theta)) \label{log_lik_corr_outputs}
\end{align}
Due to the assumption that the output covariance $\Sigma_\epsilon$ is constant across observations, we need only calculate the Cholesky decomposition 
$\Sigma_\epsilon = LL^T$ a single time per likelihood evaluation. Denoting $e_i := y - f(i, \theta) \in \R^p$, the summation in the third term can then be computed as
\begin{align}
\sum_{i = 1}^{n} e_i^T \Sigma_\epsilon^{-1} e_i &= \sum_{i = 1}^{n} e_i^T (L L^T)^{-1} e_i \\
									&= \sum_{i = 1}^{n} (L^{-1} e_i)^T (L^{-1} e_i) \nonumber \\
									&=  \sum_{i = 1}^{n} \alpha_i^T \alpha_i, \text{ where } \alpha_i := L^{-1}e_i, \nonumber \\
									&= \alpha^T \alpha, \text{ where } \alpha := (\alpha_1^T, \dots, \alpha_n^T)^T \in \R^{np}, \nonumber
\end{align}
where the $\alpha_i$ can be computed efficiently using forward substitution. Define $E \in \R^{p \times n}$ as the matrix with columns $e_1, \dots, e_n$. 
Then $L^{-1}E$ has columns $\alpha_1, \dots, \alpha_n$. Thus, we can form $\alpha$ by stacking the columns of $L^{-1}E$ into a long vector 
columnwise. Note that $\alpha$ is conceptually helpful, but in code we do not actually need to re-shape the matrix; the squared Euclidean norm of each column 
of $L^{-1}E$ can be computed and then the resulting squared norms can be summed. We again emphasize that $L^{-1}E$ should be computed using forward 
substitution. 

Recall that the inference algorithm considered for the correlated output model consisted of a Metropolis-within-Gibbs sampler. In this algorithm, the likelihood 
[\ref{log_lik_corr_outputs}] must only be evaluated during the $\theta$ sampling step, conditional on $\Sigma_\epsilon$. Therefore, the second term containing the 
determinant may be treated as a constant and need not be evaluated. However, for completeness we provide the computation of this term using the Cholesky 
factor $L$ below. 
\begin{align}
\log \det(\Sigma_\epsilon) = \log \det(LL^T) &= \log \det(L)^2 \\
								  &= 2 \log \prod_{j = 1}^{p} L_{jj} \\
								  &= 2 \sum_{j = 1}^{p} \log L_{jj}
\end{align}
The full, normalized log-likelihood can thus be implemented as
\begin{align}
\log p(Y|\theta, \Sigma_\epsilon) &= -\frac{1}{2} \log(2\pi) - n\sum_{j = 1}^{p} \log L_{jj}  - \frac{1}{2} \alpha^T \alpha
\end{align}


\section{TODOs}
\begin{itemize}
\item Contrast setting with KOH setting that has both $X$ and $\theta$ variables. 
\item Standardize my notation in how I'm subscripting vs. superscripting variables to index observations vs. outputs. Also how to 
index $f(\theta)$ when referring to outputs and observations at different time steps. 
\item Establish notation $y_i$ for observations and $Y_j$ for time series of output $j$. 
\end{itemize}

\begin{thebibliography}{20}
\bibitem{Kennedy} Kennedy, M.C. and O'Hagan, A. (2001), Bayesian calibration of computer models. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 63: 425-464. https://doi.org/10.1111/1467-9868.00294.
s11222-019-09886-w.
\bibitem{Hartig} Florian Hartig, Francesco Minunno and Stefan Paul (2017). BayesianTools: General-Purpose MCMC and SMC Samplers and Tools for Bayesian Statistics. R package version 0.1.4. https://github.com/florianhartig/BayesianTools.
\bibitem{Conti} Stefano Conti, Anthony Oâ€™Hagan, (2010), Bayesian emulation of complex multi-output and dynamic computer models, Journal of Statistical Planning and Inference,
Volume 140, Issue 3, 640-651, 0378-3758, https://doi.org/10.1016/j.jspi.2009.08.006.
\end{thebibliography}


\end{document}


