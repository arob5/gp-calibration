\documentclass[12pt]{article}
\RequirePackage[l2tabu, orthodox]{nag}
\usepackage[main=english]{babel}
\usepackage[rm={lining,tabular},sf={lining,tabular},tt={lining,tabular,monowidth}]{cfr-lm}
\usepackage{amsthm,amssymb,latexsym,gensymb,mathtools,mathrsfs}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{epstopdf,enumitem,microtype,dcolumn,booktabs,hyperref,url,fancyhdr}
\usepackage{algorithmic}
\usepackage[ruled,vlined,commentsnumbered,titlenotnumbered]{algorithm2e}

% Plotting
\usepackage{pgfplots}
\usepackage{xinttools} % for the \xintFor***
\usepgfplotslibrary{fillbetween}
\pgfplotsset{compat=1.8}
\usepackage{tikz}

% Custom Commands
\newcommand*{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand*{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand*{\suchthat}{\,\mathrel{\big|}\,}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\Ker}{\mathrm{Ker}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Prob}{\mathbb{P}}
\DeclarePairedDelimiterX\innerp[2]{(}{)}{#1\delimsize\vert\mathopen{}#2}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}

\setlist{topsep=1ex,parsep=1ex,itemsep=0ex}
\setlist[1]{leftmargin=\parindent}
\setlist[enumerate,1]{label=\arabic*.,ref=\arabic*}
\setlist[enumerate,2]{label=(\alph*),ref=(\alph*)}

% For embedding images
\graphicspath{ {./images/} }

% Specifically for paper formatting 
\renewcommand{\baselinestretch}{1.2} % Spaces manuscript for easy reading

% Formatting definitions, propositions, etc. 
\newtheorem{definition}{Definition}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{corollary}{Corollary}

% Title and author
\title{Calibration of Terrestrial Carbon Models}
\author{Andrew Roberts}

\begin{document}

\maketitle
\tableofcontents
\newpage

\section{General Setting: Computer Model Calibration}
\section{Motivating Example: Very Simple Ecosystem Model}
The goal of this paper is to discuss the unique challenges of calibrating complex process-based ecosystem models. To motivate this problem, we introduce 
a simplified vegetation model of carbon dynamics; namely, the \textit{Very Simple Vegetation Model} (VSEM) introduced by Hartig et al \cite{Hartig} and implemented 
in the \href{https://github.com/florianhartig/BayesianTools}{\textit{BayesianTools}} R package. 
This simple model consists of a system of ordinary differential equations (ODEs) describing the fluxes of carbon between three different pools (states): 
above-ground vegetation, below-ground vegetation, and soil organic matter. This is coupled with a single forcing variable, the quantity of photosynthetically 
available radiation (PAR). Let $C_v(t)$ (\textbf{v}egetation, above ground), $C_r(t)$ (\textbf{r}oots), and $C_s(t)$ (\textbf{s}oil) denote the quantity of carbon (kg $C/m^2$) in each of the three respective pools at time $t$. 
The dynamics describing the carbon fluxes between these pools depend on $NPP(t)$, the Net Primary Productivity (NPP) ($\text{kg } C/m^2/\text{day}$) at time $t$, 
which is calculated as the Gross Primary Productivity (GPP) minus carbon released due to autotrophic respiration, where GPP quantifies the amount of carbon 
fixed by vegetation during photosynthesis. Given NPP, the VSEM model calculates Net Ecosystem Exchange (NEE), which is NPP minus heterotrophic 
respiration. Thus, in the VSEM model this means 
\begin{align}
\text{NPP} = \text{ GPP } - \text{ Plant Respiration } - \text{ Soil Respiration } 
\end{align}
The state equations describing the carbon dynamics are then given by 
\begin{align}
\dot{C}_v(t) &= \alpha_v NPP(t) - \frac{C_v(t)}{\tau_v} \\
\dot{C}_r(t) &= (1.0 - \alpha_v) NPP(t) - \frac{C_r(t)}{\tau_r} \nonumber \\
\dot{C}_s(t) &= \frac{C_r(t)}{\tau_r} + \frac{C_v(t)}{\tau_v} - \frac{C_s(t)}{\tau_s} \nonumber 
\end{align}


Once the parameters $\theta$ and a specified number of time steps (days) $n$ are fixed, the ODE system can be solved, yielding daily time series of $n$
observations for NEE, $C_v$, $C_r$, and $C_s$. The forward model $f$ in this setting is mapping from these inputs to the four resulting time series. A more 
detailed discussion of the forward model is given in the following section. 

\subsection{Relation to the General Setting}
We now consider how ecosystem models such as VSEM relate to, and differ from, the KOH formulation. As previously discussed, the KOH model assumes that
computer simulators take the form $f(x, \theta)$, requiring specification of control inputs $x$ and calibration parameters $\theta$. The calibration parameters 
in the case of VSEM were stated in the previous section. But are there control inputs in this setting? It depends how we view the model. If we let 
$x = (i, j)$, where $i$ indexes the time series output and $j$ indexes the output variable (NEE, $C_v$, etc.) then $f(x, \theta) = f(i, j, \theta)$ can indeed be thought of in the 
classical KOH sense. Since $f(i, j, \theta)$ maps to the value of a single output $j$ at a single time step $i$ then it is a scalar-valued computer model. Relating the 
model to field data will yield statistical models of the form 
\begin{align}
y_{ij} &= f(i, j, \theta) + \epsilon_{ij} \label{ij_model}
\end{align}
Models for the error process $\epsilon_{ij}$ might consider time-invariant correlation across outputs $j$, as well as autocorrelation across time $i$. 

For a different perspective, let us now conceptualize the computer model as $f(i, \theta)$, a vector-valued simulator that outputs values for the four outputs
at time $i$. We can now consider an observational model of the form 
\begin{align}
y_{i} &= f(i, \theta) + \epsilon_{i} \label{i_model}
\end{align}
where $\epsilon_{i}$ is a four-dimensional vector of errors. We can take this a step further and consider a computer model with no control inputs
$f(\theta)$ that outputs a matrix of dimensions $n \times 4$ representing the full time-series for the four outputs. The corresponding statistical model looks 
like
\begin{align}
y = f(\theta) + \epsilon \label{no_control_input_model}
\end{align}
where $\epsilon$ is now a large matrix of errors. On the face of it, these definitions appear to hold only conceptual value. If we wanted to fit a simple 
statistical model assuming independent observation error equal in magnitude across outputs and time then we could consider 
$\epsilon_{ij} \overset{iid}{\sim} N(0, \sigma_\epsilon^2)$, 
$\epsilon_i \overset{iid}{\sim} N_4(0, \sigma_\epsilon^2 I_4)$, and $\epsilon \sim N_{n \times 4}(0, \sigma_\epsilon^2 I_{n \times 4})$ 
in the three respective models, and the three resulting models would be identical. However, in the setting where $f$ is sufficiently costly 
as to require emulation, the distinction between the three formulations has vastly different practical ramifications. 

We now briefly consider what model emulation might look like in each of the three cases; a thorough discussion of the costs and benefits of each 
is delayed until later in this paper. This discussion parallels one presented in 
Conti and O'Hagan \cite{Conti}, which also happened to be motivated by the problem of calibrating a vegetation model to study 
carbon dynamics. However, their study presents the case of a single time-series output, while we have been considering multiple. 
Note that in moving from [\ref{ij_model}] to 
[\ref{i_model}] and then to [\ref{no_control_input_model}], we simply conceptually shifted model \textit{inputs} (indicators for time and 
output variable) to model \textit{outputs}. This yields a scalar-valued model in [\ref{ij_model}] and ultimately a model with a very large 
number of outputs in [\ref{no_control_input_model}]. 

Starting with the first model, $f(i, j, \theta)$, we can consider a single emulator that maps $(i, j, \theta) \mapsto \R$. In particular, the input space of this 
emulator is $\{1, \dots, n\} \times \{1, \dots, p\} \times \mathcal{D}_{\theta}$, where I have returned to the generic notation $p$ to represent the number 
of outputs ($p = 4$ for VSEM). Although only a single emulator must be built in this formulation, it must be able to accurately capture the dynamics of the
simulation over time; in other words, the dynamics are baked into the emulator itself. Conti and O'Hagan \cite{Conti} refer to this as the 
\textit{time input (TI) emulator}, although in our setting the output index $j$ is also an input in addition to time. While the apparent simplicity of fitting a 
single univariate-output emulator is attractive, there may be some issues with this approach. Fitting an emulator that accurately re-creates the changing dynamics
over the full time series for each output at different $\theta$ values seems a potentially daunting task. Conti and O'Hagan present an alternative 
\textit{many single output (MS) emulators} scheme, in which $n$ independent emulators are constructed, one for each time step. When $n$ is large, this has the 
obvious drawback of requiring fitting a very large number of emulators. Turning to the second model $f(i, \theta)$, we can explore the same exact ideas, with slight modifications. 
The TI approach now requires a single multi-output 
emulator with $p$ outputs, which predicts the $p$ outputs at each time step. The MS scheme now requires $n$ $p$-output emulators. 

For the final model $f(\theta)$, we require a single emulator that maps $\theta \mapsto \R^{n \times p}$, which Conti and O'Hagan term the 
\textit{multi-output (MO) emulator} approach. In most applications, $n$ is large and so fitting an emulator 
with such a high-dimensional output space is completely infeasible. The situation can sometimes be ameliorated via dimensionality reduction techniques such as 
Principal Components Analysis (PCA). Alternatively, we might abandon emulation of $f$ directly and instead emulate the likelihood directly. Under a Gaussian error
model this will look something like $p(y|\theta, \Sigma_\epsilon) = N_{n \times p}(f(\theta), \Sigma_\epsilon)$. If we only care about the computer model insofar as 
the error in its predictions relative to field observations $y$, then targeting the likelihood for emulation drastically simplifies the problem from an $n \times p$ 
dimensional to a univariate output space! Conceptually, in this setting we are still viewing the time series as outputs (as opposed to the model $f(i, j, \theta)$) but now 
we are projecting this high-dimensional output space onto a one-dimensional space by mapping the time series through the likelihood function. As with all of the other
methods, there are still issues with this approach, and a more thorough comparison is explored later in the paper. On a notational note, we will continue to utilize 
$f(i, j, \theta)$, $f(i, \theta)$, and $f(\theta)$ as a practical means to describe algorithms, regardless of whether time and output dimension are conceptually thought 
of as inputs or outputs of the model. In particular, $f(i, j, \theta) \in \R$ is a scalar denoting the value of output $j$ at time $i$, $f(i, \theta) \in \R^p$ is a column vector 
of the $p$ outputs at time $i$, and $f(\theta) \in \R^{n \times p}$ is a matrix of all outputs at all times (where the specific set of times will be made clear by context). 

To conclude the conceptual framework 
explored here, we emphasize the characteristics of ecosystem models that made calibration challenging relative to the classical KOH formulation. In particular, KOH originally 
considered computer models with univariate outputs, which was subsequently generalized to a (typically modest) number of outputs. On the other hand, 
vegetation models like VSEM produce multiple time series of outputs, yielding a very high-dimensional output space. Alterations to classical approaches are therefore
required to deal with the complexity in this setting.  


\section{Parameter Calibration Basics}
\subsection{Correlated Outputs}
In the previous section, we assumed independence between model outputs, which resulted in a convenient likelihood. To be clear, we applied the independence assumption
\begin{align}
p(y|\theta, \Sigma_\epsilon) = \prod_{j = 1}^{p} p(y_j|\theta, \sigma_{\epsilon_j}^2)
\end{align}
where $y \in \R^p$ is a single observation of the $p$ outputs. In the case that this assumption is inappropriate, the previous models may lead to inaccurate 
quantification of uncertainties. In this case, we consider an explicit model for the covariance between model outputs. We begin by introducing a simple model 
that generalizes $\Sigma_\epsilon = \text{diag}\left(\sigma_{\epsilon_1}^2, \dots, \sigma_{\epsilon_p}^2\right)$ from a diagonal to an arbitrary, dense covariance 
matrix. Since $\Sigma_\epsilon$ is now parameterized by $\frac{p(p+1)}{2}$ parameters, this model is only computational feasible when the number of outputs is 
quite small. Nevertheless, it is a good starting point to motivate more complicated models. 

Under a Gaussian observation model, the likelihood for a single observation $y_i \in \R^p$ now becomes 
\begin{align}
p(y_i|\theta, \Sigma_\epsilon) &= N_p(y_i|f(i, \theta), \Sigma_\epsilon).
\end{align}
The likelihood across all observations is thus
\begin{align}
p(Y|\theta, \Sigma_\epsilon) &= \prod_{i = 1}^{n} N_p(y_i|f(i, \theta), \Sigma_\epsilon). \label{corr_output_lik}
\end{align}
We should emphasize that we are still assuming independent observation noise across \textit{time}; this model only accounts for correlations across outputs 
independent of time. The assumption that this output covariance $\Sigma_\epsilon$ is constant across time can be relaxed, at the cost of further increasing the 
complexity of the model. A numerically stable implementation of the log of [\ref{corr_output_lik}] is provided in the appendix [\ref{corr_output_lik_implementation}].


\section{Surrogate Modeling}
\section{Multi-Site Hierarchical Models}

\section{Appendix}

\subsection{Correlated Gaussian Likelihood Implementation} \label{corr_output_lik_implementation}
We consider coding the log of [\ref{corr_output_lik}] efficiently, which is especially important when it will need to be evaluated at each iteration of an
MCMC algorithm. The full log-likelihood is given by 
\begin{align}
\log p(Y|\theta, \Sigma_\epsilon) &= -\frac{1}{2} \log(2\pi) -\frac{n}{2} \log \det(\Sigma_\epsilon) - \frac{1}{2} \sum_{i = 1}^{n} (y_i - f(i, \theta))^T \Sigma_\epsilon^{-1} (y - f(i, \theta)) \label{log_lik_corr_outputs}
\end{align}
Due to the assumption that the output covariance $\Sigma_\epsilon$ is constant across observations, we need only calculate the Cholesky decomposition 
$\Sigma_\epsilon = LL^T$ a single time per likelihood evaluation. Denoting $e_i := y - f(i, \theta) \in \R^p$, the summation in the third term can then be computed as
\begin{align}
\sum_{i = 1}^{n} e_i^T \Sigma_\epsilon^{-1} e_i &= \sum_{i = 1}^{n} e_i^T (L L^T)^{-1} e_i \\
									&= \sum_{i = 1}^{n} (L^{-1} e_i)^T (L^{-1} e_i) \nonumber \\
									&=  \sum_{i = 1}^{n} \alpha_i^T \alpha_i, \text{ where } \alpha_i := L^{-1}e_i, \nonumber \\
									&= \alpha^T \alpha, \text{ where } \alpha := (\alpha_1^T, \dots, \alpha_n^T)^T \in \R^{np}, \nonumber
\end{align}
where the $\alpha_i$ can be computed efficiently using forward substitution. Define $E \in \R^{p \times n}$ as the matrix with columns $e_1, \dots, e_n$. 
Then $L^{-1}E$ has columns $\alpha_1, \dots, \alpha_n$. Thus, we can form $\alpha$ by stacking the columns of $L^{-1}E$ into a long vector 
columnwise. Note that $\alpha$ is conceptually helpful, but in code we do not actually need to re-shape the matrix; the squared Euclidean norm of each column 
of $L^{-1}E$ can be computed and then the resulting squared norms can be summed. We again emphasize that $L^{-1}E$ should be computed using forward 
substitution. 

Recall that the inference algorithm considered for the correlated output model consisted of a Metropolis-within-Gibbs sampler. In this algorithm, the likelihood 
[\ref{log_lik_corr_outputs}] must only be evaluated during the $\theta$ sampling step, conditional on $\Sigma_\epsilon$. Therefore, the second term containing the 
determinant may be treated as a constant and need not be evaluated. However, for completeness we provide the computation of this term using the Cholesky 
factor $L$ below. 
\begin{align}
\log \det(\Sigma_\epsilon) = \log \det(LL^T) &= \log \det(L)^2 \\
								  &= 2 \log \prod_{j = 1}^{p} L_{jj} \\
								  &= 2 \sum_{j = 1}^{p} \log L_{jj}
\end{align}
The full, normalized log-likelihood can thus be implemented as
\begin{align}
\log p(Y|\theta, \Sigma_\epsilon) &= -\frac{1}{2} \log(2\pi) - n\sum_{j = 1}^{p} \log L_{jj}  - \frac{1}{2} \alpha^T \alpha
\end{align}


\section{TODOs}
\begin{itemize}
\item Contrast setting with KOH setting that has both $X$ and $\theta$ variables. 
\item Standardize my notation in how I'm subscripting vs. superscripting variables to index observations vs. outputs. Also how to 
index $f(\theta)$ when referring to outputs and observations at different time steps. 
\item Establish notation $y_i$ for observations and $Y_j$ for time series of output $j$. 
\end{itemize}

\begin{thebibliography}{20}
\bibitem{Kennedy} Kennedy, M.C. and O'Hagan, A. (2001), Bayesian calibration of computer models. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 63: 425-464. https://doi.org/10.1111/1467-9868.00294.
s11222-019-09886-w.
\bibitem{Hartig} Florian Hartig, Francesco Minunno and Stefan Paul (2017). BayesianTools: General-Purpose MCMC and SMC Samplers and Tools for Bayesian Statistics. R package version 0.1.4. https://github.com/florianhartig/BayesianTools.
\bibitem{Conti} Stefano Conti, Anthony O’Hagan, (2010), Bayesian emulation of complex multi-output and dynamic computer models, Journal of Statistical Planning and Inference,
Volume 140, Issue 3, 640-651, 0378-3758, https://doi.org/10.1016/j.jspi.2009.08.006.
\end{thebibliography}


\end{document}


