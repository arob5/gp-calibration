
% ---------------------------------------------------------------------------------------------------
% Emulating dynamic simulators / simulators with high-dimensional output
% ---------------------------------------------------------------------------------------------------

% Higdon et al PCA basis function approach to high-dim outputs. 
@article{Higdon,
author = {Dave Higdon and James Gattiker and Brian Williams and Maria Rightley},
title = {Computer Model Calibration Using High-Dimensional Output},
journal = {Journal of the American Statistical Association},
volume = {103},
number = {482},
pages = {570-583},
year  = {2008},
publisher = {Taylor & Francis},
doi = {10.1198/016214507000000888},
URL = {   
        https://doi.org/10.1198/016214507000000888
},
eprint = { 
    
        https://doi.org/10.1198/016214507000000888
}
}

@article{MOHAMMADI2019178,
title = {Emulating dynamic non-linear simulators using Gaussian processes},
journal = {Computational Statistics & Data Analysis},
volume = {139},
pages = {178-196},
year = {2019},
issn = {0167-9473},
doi = {https://doi.org/10.1016/j.csda.2019.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S0167947319301173},
author = {Hossein Mohammadi and Peter Challenor and Marc Goodfellow},
keywords = {Dynamic simulators, Gaussian processes, Lorenz system, Uncertainty propagation, van der Pol model},
abstract = {The dynamic emulation of non-linear deterministic computer codes where the output is a time series, possibly multivariate, is examined. Such computer models simulate the evolution of some real-world phenomenon over time, for example models of the climate or the functioning of the human brain. The models we are interested in are highly non-linear and exhibit tipping points, bifurcations and chaotic behaviour. However, each simulation run could be too time-consuming to perform analyses that require many runs, including quantifying the variation in model output with respect to changes in the inputs. Therefore, Gaussian process emulators are used to approximate the output of the code. To do this, the flow map of the system under study is emulated over a short time period. Then, it is used in an iterative way to predict the whole time series. A number of ways are proposed to take into account the uncertainty of inputs to the emulators, after fixed initial conditions, and the correlation between them through the time series. The methodology is illustrated with two examples: the highly non-linear dynamical systems described by the Lorenz and van der Pol equations. In both cases, the predictive performance is relatively high and the measure of uncertainty provided by the method reflects the extent of predictability in each system.}
}

@article{10.1093/biomet/asp028,
    author = {Conti, S. and Gosling, J. P. and Oakley, J. E. and O'Hagan, A.},
    title = "{Gaussian process emulation of dynamic computer codes}",
    journal = {Biometrika},
    volume = {96},
    number = {3},
    pages = {663-676},
    year = {2009},
    month = {06},
    abstract = "{Computer codes are used in scientific research to study and predict the behaviour of complex systems. Their run times often make uncertainty and sensitivity analyses impractical because of the thousands of runs that are conventionally required, so efficient techniques have been developed based on a statistical representation of the code. The approach is less straightforward for dynamic codes, which represent time-evolving systems. We develop a novel iterative system to build a statistical model of dynamic computer codes, which is demonstrated on a rainfall-runoff simulator.}",
    issn = {0006-3444},
    doi = {10.1093/biomet/asp028},
    url = {https://doi.org/10.1093/biomet/asp028},
    eprint = {https://academic.oup.com/biomet/article-pdf/96/3/663/709193/asp028.pdf},
}

@article{CONTI2010640,
title = {Bayesian emulation of complex multi-output and dynamic computer models},
journal = {Journal of Statistical Planning and Inference},
volume = {140},
number = {3},
pages = {640-651},
year = {2010},
issn = {0378-3758},
doi = {https://doi.org/10.1016/j.jspi.2009.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S0378375809002559},
author = {Stefano Conti and Anthony Oâ€™Hagan},
keywords = {Bayesian inference, Computer experiments, Dynamic models, Hierarchical models},
abstract = {Computer models are widely used in scientific research to study and predict the behaviour of complex systems. The run times of computer-intensive simulators are often such that it is impractical to make the thousands of model runs that are conventionally required for sensitivity analysis, uncertainty analysis or calibration. In response to this problem, highly efficient techniques have recently been developed based on a statistical meta-model (the emulator) that is built to approximate the computer model. The approach, however, is less straightforward for dynamic simulators, designed to represent time-evolving systems. Generalisations of the established methodology to allow for dynamic emulation are here proposed and contrasted. Advantages and difficulties are discussed and illustrated with an application to the Sheffield Dynamic Global Vegetation Model, developed within the UK Centre for Terrestrial Carbon Dynamics.}
}

@article{10.1214/009053607000000163,
author = {M. J. Bayarri and D. Walsh and J. O. Berger and J. Cafeo and G. Garcia-Donato and F. Liu and J. Palomo and R. J. Parthasarathy and R. Paulo and J. Sacks},
title = {{Computer model validation with functional output}},
volume = {35},
journal = {The Annals of Statistics},
number = {5},
publisher = {Institute of Mathematical Statistics},
pages = {1874 -- 1906},
keywords = {Bayesian analysis, bias, Computer models, functional data, uncertain inputs, validation},
year = {2007},
doi = {10.1214/009053607000000163},
URL = {https://doi.org/10.1214/009053607000000163}
}

@article{doi:10.1080/01621459.2014.934453,
author = {Marian Farah and Paul Birrell and Stefano Conti and Daniela De Angelis},
title = {Bayesian Emulation and Calibration of a Dynamic Epidemic Model for A/H1N1 Influenza},
journal = {Journal of the American Statistical Association},
volume = {109},
number = {508},
pages = {1398-1411},
year  = {2014},
publisher = {Taylor & Francis},
doi = {10.1080/01621459.2014.934453},
URL = { 
    
        https://doi.org/10.1080/01621459.2014.934453
},
eprint = { 
        https://doi.org/10.1080/01621459.2014.934453
}
}

@article{doi:10.1198/016214507000000888,
author = {Dave Higdon and James Gattiker and Brian Williams and Maria Rightley},
title = {Computer Model Calibration Using High-Dimensional Output},
journal = {Journal of the American Statistical Association},
volume = {103},
number = {482},
pages = {570-583},
year  = {2008},
publisher = {Taylor & Francis},
doi = {10.1198/016214507000000888},
URL = {    
        https://doi.org/10.1198/016214507000000888
},
eprint = { 
    
        https://doi.org/10.1198/016214507000000888
}
}

@Article{acp-11-12253-2011,
AUTHOR = {Lee, L. A. and Carslaw, K. S. and Pringle, K. J. and Mann, G. W. and Spracklen, D. V.},
TITLE = {Emulation of a complex global aerosol model to quantify sensitivity to uncertain parameters},
JOURNAL = {Atmospheric Chemistry and Physics},
VOLUME = {11},
YEAR = {2011},
NUMBER = {23},
PAGES = {12253--12273},
URL = {https://acp.copernicus.org/articles/11/12253/2011/},
DOI = {10.5194/acp-11-12253-2011}
}

@article{10.1214/09-BA415,
author = {Fei Liu and Mike West},
title = {{A dynamic modelling strategy for Bayesian computer model emulation}},
volume = {4},
journal = {Bayesian Analysis},
number = {2},
publisher = {International Society for Bayesian Analysis},
pages = {393 -- 411},
keywords = {Backward sampling, computer model emulation, Dynamic linear model, Forwarding filtering, Gaussian process, Markov chain Monte Carlo, time-varying autoregression},
year = {2009},
doi = {10.1214/09-BA415},
URL = {https://doi.org/10.1214/09-BA415}
}

@article{REICHERT20111638,
title = {Mechanism-based emulation of dynamic simulation models: Concept and application in hydrology},
journal = {Computational Statistics & Data Analysis},
volume = {55},
number = {4},
pages = {1638-1655},
year = {2011},
issn = {0167-9473},
doi = {https://doi.org/10.1016/j.csda.2010.10.011},
url = {https://www.sciencedirect.com/science/article/pii/S0167947310003993},
author = {P. Reichert and G. White and M.J. Bayarri and E.B. Pitman},
keywords = {Dynamic model, Emulator, Optimization, Sensitivity analysis, Statistical inference},
abstract = {Many model-based investigation techniques, such as sensitivity analysis, optimization, and statistical inference, require a large number of model evaluations to be performed at different input and/or parameter values. This limits the application of these techniques to models that can be implemented in computationally efficient computer codes. Emulators, by providing efficient interpolation between outputs of deterministic simulation models, can considerably extend the field of applicability of such computationally demanding techniques. So far, the dominant techniques for developing emulators have been priors in the form of Gaussian stochastic processes (GASP) that were conditioned with a design data set of inputs and corresponding model outputs. In the context of dynamic models, this approach has two essential disadvantages: (i) these emulators do not consider our knowledge of the structure of the model, and (ii) they run into numerical difficulties if there are a large number of closely spaced input points as is often the case in the time dimension of dynamic models. To address both of these problems, a new concept of developing emulators for dynamic models is proposed. This concept is based on a prior that combines a simplified linear state space model of the temporal evolution of the dynamic model with Gaussian stochastic processes for the innovation terms as functions of model parameters and/or inputs. These innovation terms are intended to correct the error of the linear model at each output step. Conditioning this prior to the design data set is done by Kalman smoothing. This leads to an efficient emulator that, due to the consideration of our knowledge about dominant mechanisms built into the simulation model, can be expected to outperform purely statistical emulators at least in cases in which the design data set is small. The feasibility and potential difficulties of the proposed approach are demonstrated by the application to a simple hydrological model.}
}

@article{doi:10.1198/106186008X384032,
author = {Jonathan Rougier},
title = {Efficient Emulators for Multivariate Deterministic Functions},
journal = {Journal of Computational and Graphical Statistics},
volume = {17},
number = {4},
pages = {827-843},
year  = {2008},
publisher = {Taylor & Francis},
doi = {10.1198/106186008X384032},
URL = { 
        https://doi.org/10.1198/106186008X384032
},
eprint = {     
        https://doi.org/10.1198/106186008X384032
}
}

@article{doi:10.1137/120900915,
author = {Williamson, Daniel and Blaker, Adam T.},
title = {Evolving Bayesian Emulators for Structured Chaotic Time Series, with Application to Large Climate Models},
journal = {SIAM/ASA Journal on Uncertainty Quantification},
volume = {2},
number = {1},
pages = {1-28},
year = {2014},
doi = {10.1137/120900915},
URL = { 
        https://doi.org/10.1137/120900915
},
eprint = {   
        https://doi.org/10.1137/120900915
},
    abstract = { We develop Bayesian dynamic linear model Gaussian processes for emulation of time series output for computer models that may exhibit chaotic behavior, but where this behavior retains some underlying structure. The statistical technology is particularly suited to emulating the time series output of large climate models that exhibit this feature and where we want samples from the posterior of the emulator to evolve in the same way as dynamic processes in the computer model do. The methodology combines key features of good uncertainty quantification (UQ) methods such as using complex mean functions to capture large-scale signals within parameter space, with dynamic linear models in a way that allows UQ to borrow strength from the Bayesian time series literature. We present an MCMC algorithm for sampling from the posterior of the emulator parameters when the roughness lengths of the Gaussian process are unknown. We discuss an interpretation of the results of this algorithm that allows us to use MCMC to fix the correlation lengths, making future online samples from the emulator tractable when used in practical applications where online MCMC is infeasible. We apply this methodology to emulate the Atlantic Meridional Overturning Circulation (AMOC) as a time series output of the fully coupled non--flux-adjusted atmosphere-ocean general circulation model HadCM3. }
}

% Emulate coefficients in KL expansion, then use delayed-acceptance MCMC. 
@Article{psf2021003011,
AUTHOR = {Albert, Christopher G. and Callies, Ulrich and Toussaint, Udo von},
TITLE = {Surrogate-Enhanced Parameter Inference for Function-Valued Models},
JOURNAL = {Physical Sciences Forum},
VOLUME = {3},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {11},
URL = {https://www.mdpi.com/2673-9984/3/1/11},
ISSN = {2673-9984},
ABSTRACT = {We present an approach to enhance the performance and flexibility of the Bayesian inference of model parameters based on observations of the measured data. Going beyond the usual surrogate-enhanced Monte-Carlo or optimization methods that focus on a scalar loss, we place emphasis on a function-valued output of a formally infinite dimension. For this purpose, the surrogate models are built on a combination of linear dimensionality reduction in an adaptive basis of principal components and Gaussian process regression for the map between reduced feature spaces. Since the decoded surrogate provides the full model output rather than only the loss, it is re-usable for multiple calibration measurements as well as different loss metrics and, consequently, allows for flexible marginalization over such quantities and applications to Bayesian hierarchical models. We evaluate the method&rsquo;s performance based on a case study of a toy model and a simple riverine diatom model for the Elbe river. As input data, this model uses six tunable scalar parameters as well as silica concentrations in the upper reach of the river together with the continuous time-series of temperature, radiation, and river discharge over a specific year. The output consists of continuous time-series data that are calibrated against corresponding measurements from the Geesthacht Weir station at the Elbe river. For this study, only two scalar inputs were considered together with a function-valued output and compared to an existing model calibration using direct simulation runs without a surrogate.},
DOI = {10.3390/psf2021003011}
}

% Emulate the likelihood. 
@article{LEBEL2019158,
title = {Statistical inverse identification for nonlinear train dynamics using a surrogate model in a Bayesian framework},
journal = {Journal of Sound and Vibration},
volume = {458},
pages = {158-176},
year = {2019},
issn = {0022-460X},
doi = {https://doi.org/10.1016/j.jsv.2019.06.024},
url = {https://www.sciencedirect.com/science/article/pii/S0022460X19303633},
author = {D. Lebel and C. Soize and C. FÃ¼nfschilling and G. Perrin},
keywords = {Statistical inverse problem, Bayesian calibration, Surrogate model, High-speed train dynamics, Uncertainty quantification},
abstract = {This paper presents a Bayesian calibration method for a simulation-based model with stochastic functional input and output. The originality of the method lies in an adaptation involving the representation of the likelihood function by a Gaussian process surrogate model, to cope with the high computational cost of the simulation, while avoiding the surrogate modeling of the functional output. The adaptation focuses on taking into account the uncertainty introduced by the use of a surrogate model when estimating the parameters posterior probability distribution by MCMC. To this end, trajectories of the random surrogate model of the likelihood function are drawn and injected in the MCMC algorithm. An application on a train suspension monitoring case is presented.}
}


@misc{ranjan2016inverse,
      title={Inverse problem for time-series valued computer model via scalarization}, 
      author={Pritam Ranjan and Mark Thomas and Holger Teismann and Sujay Mukhoti},
      year={2016},
      eprint={1605.09503},
      archivePrefix={arXiv},
      primaryClass={stat.ME}
}

@article{doi:10.1080/00401706.2013.775897,
author = { Matthew T.   Pratola  and  Stephan R.   Sain  and  Derek   Bingham  and  Michael   Wiltberger  and  E.   Joshua   Rigler },
title = {Fast Sequential Computer Model Calibration of Large Nonstationary Spatial-Temporal Processes},
journal = {Technometrics},
volume = {55},
number = {2},
pages = {232-242},
year  = {2013},
publisher = {Taylor & Francis},
doi = {10.1080/00401706.2013.775897},
URL = { 
        https://doi.org/10.1080/00401706.2013.775897
},
eprint = { 
    
        https://doi.org/10.1080/00401706.2013.775897
}
}

@article{PERRIN2020106728,
title = {Adaptive calibration of a computer code with time-series output},
journal = {Reliability Engineering & System Safety},
volume = {196},
pages = {106728},
year = {2020},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2019.106728},
url = {https://www.sciencedirect.com/science/article/pii/S0951832018311232},
author = {G. Perrin},
keywords = {Bayesian framework, Computer experiment, Dynamic simulator, Gaussian process, Multi-output},
abstract = {Simulation plays a major role in the conception, the optimization and the certification of complex systems. Of particular interest here is the calibration of the parameters of computer models from high-dimensional physical observations. When the run times of these computer codes is high, this work focuses on the numerical challenges associated with the statistical inference. In particular, several adaptations of the Gaussian Process Regression (GPR) to the high-dimensional or functional output case are presented for the emulation of computer codes from limited data. Then, an adaptive procedure is detailed to minimize the calibration parameters uncertainty at the minimal computational cost. The proposed method is eventually applied to two applications that are based on dynamic simulators.}
}


% ---------------------------------------------------------------------------------------------------
% Scaling GPs. 
% ---------------------------------------------------------------------------------------------------

% TODO: need to read
@misc{cole2021locally,
      title={Locally induced Gaussian processes for large-scale simulation experiments}, 
      author={D. Austin Cole and Ryan Christianson and Robert B. Gramacy},
      year={2021},
      eprint={2008.12857},
      archivePrefix={arXiv},
      primaryClass={stat.ME}
}

% TODO: need to read 
@article{doi:10.1080/10618600.2014.914442,
author = {Robert B. Gramacy and Daniel W. Apley},
title = {Local Gaussian Process Approximation for Large Computer Experiments},
journal = {Journal of Computational and Graphical Statistics},
volume = {24},
number = {2},
pages = {561-578},
year  = {2015},
publisher = {Taylor & Francis},
doi = {10.1080/10618600.2014.914442},
URL = { 
    
        https://doi.org/10.1080/10618600.2014.914442
},
eprint = {    
        https://doi.org/10.1080/10618600.2014.914442
}
}


% ---------------------------------------------------------------------------------------------------
% Sequential Design and Optimization
% ---------------------------------------------------------------------------------------------------

% Propose EIVAR-type criterion. 
@article{SinsbeckNowak,
author = {Sinsbeck, Michael and Nowak, Wolfgang},
title = {Sequential Design of Computer Experiments for the Solution of Bayesian Inverse Problems},
journal = {SIAM/ASA Journal on Uncertainty Quantification},
volume = {5},
number = {1},
pages = {640-664},
year = {2017},
doi = {10.1137/15M1047659},
URL = { 
    
        https://doi.org/10.1137/15M1047659
},
eprint = {     
        https://doi.org/10.1137/15M1047659
},
abstract = { We present a sequential design strategy for efficient sampling of model functions during the solution of Bayesian inverse problems. The model function is assumed to be computationally expensive and therefore is described by a random field (such as a Gaussian process emulator). The sequential design strategy is a greedy one-step look ahead method, minimizing the Bayes risk with respect to a loss function measuring the quadratic \$L^2\$-error in the likelihood estimate. Four numerical examples demonstrate that the proposed sampling method is more efficient than space-filling, prior-based designs. }
}

% EIVAR paper; computes EIVAR for the Higdon et al PCA-based emulation approach. 
@misc{Surer2023sequential,
      title={Sequential Bayesian experimental design for calibration of expensive simulation models}, 
      author={Ã–zge SÃ¼rer and Matthew Plumlee and Stefan M. Wild},
      year={2023},
      eprint={2305.16506},
      archivePrefix={arXiv},
      primaryClass={stat.ME}
}

% Proposes an EIVAR-like acquisition but uses KL-divergence instead of prior-weighted L2 
@inproceedings{KandasamyActiveLearning2015,
author = {Kandasamy, Kirthevasan and Schneider, Jeff and P\'{o}czos, Barnab\'{a}s},
title = {Bayesian Active Learning for Posterior Estimation},
year = {2015},
isbn = {9781577357384},
publisher = {AAAI Press},
abstract = {This paper studies active posterior estimation in a Bayesian setting when the likelihood is expensive to evaluate. Existing techniques for posterior estimation are based on generating samples representative of the posterior. Such methods do not consider efficiency in terms of likelihood evaluations. In order to be query efficient we treat posterior estimation in an active regression framework. We propose two myopic query strategies to choose where to evaluate the likelihood and implement them using Gaussian processes. Via experiments on a series of synthetic and real examples we demonstrate that our approach is significantly more query efficient than existing techniques and other heuristics for posterior estimation.},
booktitle = {Proceedings of the 24th International Conference on Artificial Intelligence},
pages = {3605â€“3611},
numpages = {7},
location = {Buenos Aires, Argentina},
series = {IJCAI'15}
}

% Classic paper on batch BayesOpt; Kriging believer and constant liar heuristics for approximating multi-points EI. 
@Inbook{Ginsbourger2010,
author="Ginsbourger, David
and Le Riche, Rodolphe
and Carraro, Laurent",
editor="Tenne, Yoel
and Goh, Chi-Keong",
title="Kriging Is Well-Suited to Parallelize Optimization",
bookTitle="Computational Intelligence in Expensive Optimization Problems",
year="2010",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="131--162",
abstract="The optimization of expensive-to-evaluate functions generally relies on metamodel-based exploration strategies. Many deterministic global optimization algorithms used in the field of computer experiments are based on Kriging (Gaussian process regression). Starting with a spatial predictor including a measure of uncertainty, they proceed by iteratively choosing the point maximizing a criterion which is a compromise between predicted performance and uncertainty. Distributing the evaluation of such numerically expensive objective functions on many processors is an appealing idea. Here we investigate a multi-points optimization criterion, the multipoints expected improvement ({\$}q-{\{}{\backslash}mathbb E{\}}I{\$}), aimed at choosing several points at the same time. An analytical expression of the {\$}q-{\{}{\backslash}mathbb E{\}}I{\$}is given when q{\thinspace}={\thinspace}2, and a consistent statistical estimate is given for the general case. We then propose two classes of heuristic strategies meant to approximately optimize the {\$}q-{\{}{\backslash}mathbb E{\}}I{\$}, and apply them to the classical Branin-Hoo test-case function. It is finally demonstrated within the covered example that the latter strategies perform as good as the best Latin Hypercubes and Uniform Designs ever found by simulation (2000 designs drawn at random for every q{\thinspace}âˆˆ [1,10]).",
isbn="978-3-642-10701-6",
doi="10.1007/978-3-642-10701-6_6",
url="https://doi.org/10.1007/978-3-642-10701-6_6"
}


% Utilize Ensemble Kalman Inversion (EKI) or Ensemble Kalman Sampling (EKI) as cheap design method. 
@article{Cleary2021,
	doi = {10.1016/j.jcp.2020.109716},
	url = {https://doi.org/10.1016\%2Fj.jcp.2020.109716},  
	year = 2021,
	month = {jan},  
	publisher = {Elsevier {BV}},  
	volume = {424},  
	pages = {109716},  
	author = {Emmet Cleary and Alfredo Garbuno-Inigo and Shiwei Lan and Tapio Schneider and Andrew M. Stuart},  
	title = {Calibrate, emulate, sample},  
	journal = {Journal of Computational Physics}
}

% Example application of Emulate, calibrate, sample approach. 
@article{https://doi.org/10.1029/2020MS002454,
author = {Dunbar, Oliver R. A. and Garbuno-Inigo, Alfredo and Schneider, Tapio and Stuart, Andrew M.},
title = {Calibration and Uncertainty Quantification of Convective Parameters in an Idealized GCM},
journal = {Journal of Advances in Modeling Earth Systems},
volume = {13},
number = {9},
pages = {e2020MS002454},
keywords = {uncertainty quantification, model calibration, machine learning, general circulation model, parametric uncertainty, inverse problem},
doi = {https://doi.org/10.1029/2020MS002454},
url = {https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2020MS002454},
eprint = {https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/2020MS002454},
note = {e2020MS002454 2020MS002454},
abstract = {Abstract Parameters in climate models are usually calibrated manually, exploiting only small subsets of the available data. This precludes both optimal calibration and quantification of uncertainties. Traditional Bayesian calibration methods that allow uncertainty quantification are too expensive for climate models; they are also not robust in the presence of internal climate variability. For example, Markov chain Monte Carlo (MCMC) methods typically require model runs and are sensitive to internal variability noise, rendering them infeasible for climate models. Here we demonstrate an approach to model calibration and uncertainty quantification that requires only model runs and can accommodate internal climate variability. The approach consists of three stages: (a) a calibration stage uses variants of ensemble Kalman inversion to calibrate a model by minimizing mismatches between model and data statistics; (b) an emulation stage emulates the parameter-to-data map with Gaussian processes (GP), using the model runs in the calibration stage for training; (c) a sampling stage approximates the Bayesian posterior distributions by sampling the GP emulator with MCMC. We demonstrate the feasibility and computational efficiency of this calibrate-emulate-sample (CES) approach in a perfect-model setting. Using an idealized general circulation model, we estimate parameters in a simple convection scheme from synthetic data generated with the model. The CES approach generates probability distributions of the parameters that are good approximations of the Bayesian posteriors, at a fraction of the computational cost usually required to obtain them. Sampling from this approximate posterior allows the generation of climate predictions with quantified parametric uncertainties.},
year = {2021}
}

% Orthogonal array based LHS; discussion of different methods for batch sequential design. Emphasis on space-filling. 
@article{LOEPPKY20101452,
title = {Batch sequential designs for computer experiments},
journal = {Journal of Statistical Planning and Inference},
volume = {140},
number = {6},
pages = {1452-1464},
year = {2010},
issn = {0378-3758},
doi = {https://doi.org/10.1016/j.jspi.2009.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S0378375809003747},
author = {Jason L. Loeppky and Leslie M. Moore and Brian J. Williams},
keywords = {Computer experiment, Gaussian process, Random function, Latin hypercube sample, Maximin distance, Entropy},
abstract = {Computer models simulating a physical process are used in many areas of science. Due to the complex nature of these codes it is often necessary to approximate the code, which is typically done using a Gaussian process. In many situations the number of code runs available to build the Gaussian process approximation is limited. When the initial design is small or the underlying response surface is complicated this can lead to poor approximations of the code output. In order to improve the fit of the model, sequential design strategies must be employed. In this paper we introduce two simple distance based metrics that can be used to augment an initial design in a batch sequential manner. In addition we propose a sequential updating strategy to an orthogonal array based Latin hypercube sample. We show via various real and simulated examples that the distance metrics and the extension of the orthogonal array based Latin hypercubes work well in practice.}
}

% Deep Gaussian Processes via MCMC sampling; estimates IMSPE and ALC criteria using the MCMC samples. 
@misc{sauer2021active,
      title={Active Learning for Deep Gaussian Process Surrogates}, 
      author={Annie Sauer and Robert B. Gramacy and David Higdon},
      year={2021},
      eprint={2012.08015},
      archivePrefix={arXiv},
      primaryClass={stat.ME}
}

% TODO: need to read
@article{PANDITA2021114007,
title = {Surrogate-based sequential Bayesian experimental design using non-stationary Gaussian Processes},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {385},
pages = {114007},
year = {2021},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2021.114007},
url = {https://www.sciencedirect.com/science/article/pii/S0045782521003388},
author = {Piyush Pandita and Panagiotis Tsilifis and Nimish M. Awalgaonkar and Ilias Bilionis and Jitesh Panchal},
keywords = {Design of experiments, Non-stationary Gaussian Processes, Expected Information Gain, Sequential designs, Bayesian inference},
abstract = {Inferring arbitrary quantities of interest (QoI) using limited computational or, in realistic scenarios, financial budgets, is a challenging problem that requires sophisticated strategies for the optimal allocation of the available resources. Bayesian optimal experimental design identifies the optimal set of design locations for the purpose of solving a parameter inference problem and the optimality criterion is typically associated with maximizing the worth of information in the experimental measurements. Sequential design strategies further identify the optimal design in a sequential manner, starting from a initial budget and iteratively selecting new optimal points until either an accuracy threshold is reached, or a cost limit is exceeded. In this paper, we present a generic sequential Bayesian experimental design framework that relies on maximizing an information theoretic design criterion, namely the Expected Information Gain, in order to infer QoIs formed as nonlinear operators acting on black-box functions. Our framework relies on modeling the underlying response function using non-stationary Gaussian Processes, thus enabling efficient sampling from the QoI in order to provide Monte Carlo estimators for the design criterion. We demonstrate the performance of our method on an engineering problem of steel wire manufacturing and compare it with two classic approaches: uncertainty sampling and expected improvement.}
}


% TODO: need to read
@article{article,
author = {Joseph, V. Roshan and Dasgupta, Tirthankar and Tuo, Rui and Wu, Chi-Fang},
year = {2014},
month = {11},
pages = {00-00},
title = {Sequential Exploration of Complex Surfaces Using Minimum Energy Designs},
volume = {57},
journal = {Technometrics},
doi = {10.1080/00401706.2014.881749}
}

% TODO: need to read
@article{Huan2013,
	doi = {10.1016/j.jcp.2012.08.013},
	url = {https://doi.org/10.1016%2Fj.jcp.2012.08.013},  
	year = 2013,
	month = {jan},  
	publisher = {Elsevier {BV}},  
	volume = {232},  
	number = {1},  
	pages = {288--317},
	author = {Xun Huan and Youssef M. Marzouk},  
	title = {Simulation-based optimal Bayesian experimental design for nonlinear systems},  
	journal = {Journal of Computational Physics}
}

% TODO: need to read
@misc{gramacy2010particle,
      title={Particle learning of Gaussian process models for sequential design and optimization}, 
      author={Robert B. Gramacy and Nicholas G. Polson},
      year={2010},
      eprint={0909.5262},
      archivePrefix={arXiv},
      primaryClass={stat.CO}
}

% TODO: need to read 
@misc{zhang2019distancedistributed,
      title={Distance-distributed design for Gaussian process surrogates}, 
      author={Boya Zhang and D. Austin Cole and Robert B. Gramacy},
      year={2019},
      eprint={1812.02794},
      archivePrefix={arXiv},
      primaryClass={stat.ME}
}

% TODO: need to read 
@misc{koermer2023active,
      title={Active Learning for Simulator Calibration}, 
      author={Scott Koermer and Justin Loda and Aaron Noble and Robert B. Gramacy},
      year={2023},
      eprint={2301.10228},
      archivePrefix={arXiv},
      primaryClass={stat.ME}
}

% Derive closed-form expression for multi-point EI. 
@inproceedings{Chevalier2013,
author = {Chevalier, ClÃ©ment},
year = {2013},
month = {01},
pages = {},
title = {Fast Computation of the Multi-Points Expected Improvement with Applications in Batch Selection},
isbn = {978-3-642-44972-7},
doi = {10.1007/978-3-642-44973-4_7}
}

% Derive an expression for the gradient of the closed-form multi-point EI. 
@inproceedings{Marmin2015,
author = {Marmin, SÃ©bastien and Chevalier, ClÃ©ment and Ginsbourger, David},
year = {2015},
month = {03},
pages = {},
title = {Differentiating the Multipoint Expected Improvement for Optimal Batch Design},
isbn = {978-3-319-27925-1},
doi = {10.1007/978-3-319-27926-8_4}
}







