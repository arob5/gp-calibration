\documentclass[12pt]{article}
\RequirePackage[l2tabu, orthodox]{nag}
\usepackage[main=english]{babel}
\usepackage[rm={lining,tabular},sf={lining,tabular},tt={lining,tabular,monowidth}]{cfr-lm}
\usepackage{amsthm,amssymb,latexsym,gensymb,mathtools,mathrsfs}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{epstopdf,enumitem,microtype,dcolumn,booktabs,hyperref,url,fancyhdr}
\usepackage{algorithmic}
\usepackage[ruled,vlined,commentsnumbered,titlenotnumbered]{algorithm2e}
\usepackage{bbm}

% Plotting
\usepackage{pgfplots}
\usepackage{xinttools} % for the \xintFor***
\usepgfplotslibrary{fillbetween}
\pgfplotsset{compat=1.8}
\usepackage{tikz}

% Local custom commands. 
\include{local-defs}

\setlist{topsep=1ex,parsep=1ex,itemsep=0ex}
\setlist[1]{leftmargin=\parindent}
\setlist[enumerate,1]{label=\arabic*.,ref=\arabic*}
\setlist[enumerate,2]{label=(\alph*),ref=(\alph*)}

% For embedding images
\graphicspath{ {./images/} }

% Specifically for paper formatting 
\renewcommand{\baselinestretch}{1.2} % Spaces manuscript for easy reading

% Formatting definitions, propositions, etc. 
\newtheorem{definition}{Definition}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{corollary}{Corollary}

% Title and author
\title{A Framework for Scalable Ecosystem Model Calibration}
\author{Andrew Roberts}

\begin{document}

\maketitle
\tableofcontents
\newpage

% Calibration of Expensive Computer Models
\section{Calibration of Expensive Computer Models}

% Ecosystem Models 
\section{Ecosystem Models}

\subsection{The Very Simple Ecosystem Model}
The goal of this paper is to discuss the unique challenges of calibrating complex process-based ecosystem models. To motivate this problem, we introduce 
a simplified vegetation model of carbon dynamics; namely, the \textit{Very Simple Vegetation Model} (VSEM) introduced by Hartig et al \cite{Hartig} and implemented 
in the \href{https://github.com/florianhartig/BayesianTools}{\textit{BayesianTools}} R package. 
This simple model consists of a system of ordinary differential equations (ODEs) describing the fluxes of carbon between three different pools (states): 
above-ground vegetation, below-ground vegetation, and soil organic matter. The carbon dynamics are forced by a single variable, the quantity of photosynthetically 
active radiation (PAR), which represents the portion of the light spectrum usable by plants for photosynthesis. 
Let $\state_v(\timeidx)$ (\textbf{v}egetation, above ground), $\state_r(\timeidx)$ (\textbf{r}oots), and $\state_s(\timeidx)$ (\textbf{s}oil) denote the quantity of carbon (kg $C/m^2$) in each of the three respective pools at time $\timeidx$. 
The dynamics describing the carbon fluxes between these pools depend on $\text{NPP}(\timeidx)$, the Net Primary Productivity (NPP) ($\text{kg } C/m^2/\text{day}$) at time $\timeidx$, 
which is calculated as the Gross Primary Productivity (GPP) minus carbon released due to autotrophic respiration, where GPP quantifies the amount of carbon 
fixed by vegetation during photosynthesis. Given NPP, the VSEM model calculates Net Ecosystem Exchange (NEE), which is (aside from a sign change) 
NPP minus heterotrophic 
respiration. Thus, in the VSEM model this means 
\begin{align}
-\text{NEE} = \text{ GPP } - \text{ Plant Respiration } - \text{ Soil Respiration } 
\end{align}
The sign convention is that positive NPP indicates a flux into the ecosystem, while a positive NEE indicates a flux into the atmosphere, hence the addition of the negative. 
The state equations describing the carbon dynamics are then given by
\begin{align}
\dot{\state}_v(\timeidx) &= \alpha_v \times \text{NPP}(\timeidx) - \frac{\state_v(\timeidx)}{\tau_v} \label{VSEM_ODE_system} \\
\dot{\state}_r(\timeidx) &= (1.0 - \alpha_v) \times \text{NPP}(\timeidx) - \frac{\state_r(\timeidx)}{\tau_r} \nonumber \\
\dot{\state}_s(\timeidx) &= \frac{\state_r(\timeidx)}{\tau_r} + \frac{\state_v(\timeidx)}{\tau_v} - \frac{\state_s(\timeidx)}{\tau_s} \nonumber 
\end{align}
where the parameters $\tau_v$, $\tau_r$, and $\tau_s$ are residence times or longevity parameters for above-ground vegetation, below-ground vegetation, and soil organic matter, 
respectively. In particular, $\tau_v$ and $\tau_r$ represent the longevity of above and below ground biomass, respectively, while $\tau_s$ is the residence time of organic matter 
in the soil. Carbon is thus assumed to be lost from the plant pools to the soil pool at fixed turnover rates, and similarly from the soil pool to the atmosphere. VSEM also makes the simplifying assumption that a fixed proportion of NPP is allocated to above and below ground vegetation, where $\alpha_v$ is the fraction
allocated to the former. 

The dynamics [\ref{VSEM_ODE_system}] are driven by the forcing effect of PAR, which drives the values of NPP and GPP over time. VSEM assumes a simple calculation, 
where GPP is given by a product of three factors:
\begin{enumerate}
\item The amount of light available for photosynthesis (PAR) (MJ/$m^2$/day).
\item Light-use efficiency (LUE), a measure of the efficiency at which vegetation can use light for photosynthesis. 
\item The rate at which the available light decays as it passes downwards through a canopy of leaves. 
\end{enumerate}
The rate described in the third item above is modeled by the Beer-Lambert law, which yields an exponential decay rate $e^{-k*\text{LAI}}$, 
where LAI is the \textit{leaf-area index}, defined as the ratio of one-sided leaf area per unit of ground area. LAI at time $\timeidx$ is assumed to be given by the product of a fixed 
leaf-area ratio (LAR) and $\state_v(\timeidx)$. The constant $k$ is a fixed extinction rate controlling the rate of exponential decay. The full calculations for NPP are given below. 
\begin{align}
\text{LAI}(\timeidx, \state_v) &= \text{LAR} \times \state_v(\timeidx) \\
\text{GPP}(\timeidx) &= \text{PAR}(\timeidx) \times \text{LUE} \times \left(1 -  \exp\left(-k \times \text{LAI}(\timeidx, \state_v) \right) \right) \nonumber \\
\text{NPP}(\timeidx) &= (1 - \gamma) \times \text{GPP}(\timeidx) \nonumber
\end{align}
As seen above, NPP is assumed to be a fixed fraction $1 - \gamma$ of GPP.

Potential calibration parameters $\theta$ of this model include $\alpha_v$, $\tau_v$, $\tau_r$, $\tau_s$, $\text{LAR}$, $k$, and $\gamma$, but it is common to fix some of the parameters at their 
nominal values and calibrate the remaining subset. 
Once the parameters $\bpar$ and a specified number of time steps (days) $n$ are fixed, the ODE system can be solved, yielding daily time series of $n$
observations for NEE, $\state_v$, $\state_r$, and $\state_s$. The forward model $\fwd$ in this setting is given by an execution of the ODE solve, and represents
the mapping from the inputs to these four resulting time series. A more detailed discussion of the forward model is given in the following section. 

\subsection{Generic Dynamical Models}
I believe this work nicely complements a series of papers on so-called \textit{dynamic} or \textit{time-series valued} emulation. The necessity of such emulators typically arise from forward models 
defined as a the solution of an autonomous system of ODEs. I will denote the state vector of such a system by 
\[\bx(t) = \left(x_1(t), \dots, x_P(t) \right)^{\top} \]
If we consider discretizing at constant time steps $\Delta t$ then since the system is autonomous the \textit{one-step map} or \textit{flow map} $g: \R^P \times \R^Q \to \R^P$ is time-invariant: 
 \[\bx(t + \Delta t) = g(\bx(t); \bw_{t + \Delta t})\]
 where $\bw_{t + \Delta t}$ is a \textit{forcing input}. The forcing inputs are assumed known, and vary across time. 
 Note that this could also be generalized to variable step sizes $\Delta t$ by considering the flow map to also be a function of the step size; i.e., $g = g(\bx(t), \Delta t)$. We can also view the 
 vector-valued $g$ as a collection of $P$ univariate flow maps
 \[g(\bx(t); \bw_{t + \Delta t}) = \left[g_1(\bx(t); \bw_{t + \Delta t}), \dots, g_P(\bx(t); \bw_{t + \Delta t}) \right]^{\top}\]
 so that $g_k: \R^P \to \R$ maps $\bx(t)$ to $\bx_p(t + \Delta t)$, the value of the $p^{\text{th}}$ state at the subsequent time step. Of course, $g$ and hence the states $\bx(t)$ depend on 
 the parameters $\bpar$. I will reflect this dependence by writing $g_{\bpar}$ and $\bx_{\bpar}(t)$. The forward model $G: \R^{\Npar} \to \R^{\Ntime \times \Nout}$ defined previously can thus be characterized 
 as 
 \[
 G(\bpar; \bx_0, \bw) = \begin{pmatrix} \bx_0^{\top} \\ \bx_\theta(\Delta t)^{\top} \\  \bx_\theta(2\Delta t)^{\top} \\ \vdots \\ \bx_\theta\left([T-1]\Delta t\right)^{\top} \end{pmatrix} = 
 \begin{pmatrix} \bx_0^{\top} \\ g_\theta(\bx_0; \bw_1)^{\top} \\  g_\theta\left(g_\theta(\bx_0; \bw_1); \bw_2 \right)^{\top} \\ \vdots \\ g_\theta^{(T-1)}(\bx_0; \bw)^{\top} \end{pmatrix}
 \]
 where I now explicitly write $G$ as a function of an initial condition $\bx_0 = \bx(0)$, which is independent of $\bpar$, and I use the shorthand $g_\theta^{(k)}$ to denote 
 the composition consisting of $k$ applications of the map $g_\theta$. I also let $\bw := \{\bw_t\}_{\timeidx = 1}^{T}$ denote the sequence of forcing inputs. 
 I have also assumed that the initial time is $0$, but some other time $t_0$ could of course be considered. 

% Bayesian Calibration 
\section{Bayesian Calibration}

\subsection{Problem Setup}
In this section we provide a brief overview of the Bayesian approach to computer model calibration, which has been recently favored by researchers due to its ability to quantify 
uncertainties in the calibration process. We begin by defining a likelihood function $p(\stateMat|\bpar)$ that relates the observed field data $\stateMat$ to the forward model prediction 
$\fwd(\bpar)$. Throughout most of this article, we assume the following Gaussian likelihood 
\begin{align}
\stateOut{\outidx}, \CovObs | \bpar \overset{\text{ind}}{\sim} \mathcal{N}_{\Ntime_{\outidx}}(\fwdOut{\outidx}(\bpar), \sdOut_{\outidx}^2 I) \label{likelihood}
\end{align}
For notational convenience, we have collected the 
variance parameters in the matrix $\CovObs := \text{diag}\left(\sdOut_1^2, \dots, \sdOut_{\Nout}^2 \right)$. 
This likelihood assumes the errors are independent across time and output variable. 
We will typically work with the log of the likelihood, denoted by 
$\llik(\bpar, \CovObs) := \log p(\stateMat|\bpar, \CovObs)$. Note that $\llik(\bpar, \CovObs)$ depends on the data $\stateMat$, 
but $\stateMat$ is constant throughout the analysis so we drop it from the notation.
Under the likelihood \ref{likelihood}, $\llik(\bpar, \CovObs)$ takes the form 
\begin{align*}
\llik(\bpar, \CovObs) &= \sum_{\outidx = 1}^{\Nout} \sum_{\timeidx = 1}^{\Ntime_{\outidx}} \log \mathcal{N}\left(\stateTimeOut{\timeidx}{\outidx} | \fwdTimeOut{\timeidx}{\outidx}(\bpar), \sdOut_{\outidx}^2 \right)
\end{align*}
Missing observations are simply ignored. For future purposes, it will be useful to expand this sum of Gaussian densities and introduce some notation. To this end, the log likelihood can be written as 
\begin{align*}
\llik(\bpar, \CovObs) &= \sum_{\outidx = 1}^{\Nout} \sum_{\timeidx = 1}^{\Ntime_{\outidx}} \log \mathcal{N}\left(\state_{\timeidx \outidx}| \fwd_{\timeidx \outidx}(\bpar), \sdOut_{\outidx}^2 \right) \\
	         &= \sum_{\outidx = 1}^{\Nout}  \sum_{\timeidx = 1}^{\Ntime_{\outidx}} \left[-\frac{1}{2} \log(2\postDens \sdOut_{\outidx}^2) - \frac{1}{2\sdOut_{\outidx}^2} (\state_{\timeidx \outidx} - \fwd_{\timeidx \outidx}(\bpar))^2\right]  \\
	         &= -\frac{1}{2} \sum_{\outidx = 1}^{\Nout} \left[\Ntime_{\outidx} \log\left(2\postDens \sdOut_{\outidx}^2 \right) + \frac{1}{\sdOut_{\outidx}^2}\sum_{\timeidx = 1}^{\Ntime_{\outidx}} \left(\state_{\timeidx \outidx} - 
	                \fwd_{\timeidx \outidx}(\bpar)\right)^2  \right] \\
	         &= -\frac{1}{2} \sum_{\outidx = 1}^{\Nout} \left[\Ntime_{\outidx} \log\left(2\postDens \sdOut_{\outidx}^2 \right) + \frac{\SSROut{\outidx}(\bpar)}{\sdOut_{\outidx}^2} \right] 
\end{align*}
where 
\begin{align}
\SSROut{\outidx}(\bpar) := \sum_{\timeidx = 1}^{\Ntime_{\outidx}} (\state_{\timeidx \outidx} - \fwd_{\timeidx \outidx}(\bpar))^2
\end{align}
is the squared Euclidean error between the observations $\stateOut{\outidx}$ and computer model predictions $\fwdOut{\outidx}(\bpar)$ for the $\outidx^{\text{th}}$ data constraint. 

With the likelihood established, we now define a prior distribution jointly over the calibration parameters $\btheta$ and likelihood variance parameters $\CovObs$. 
We denote the prior density for this distribution as $\priorDens(\bpar, \CovObs) = \priorDens(\bpar)\priorDens(\CovObs)$, assuming prior independence between $\bpar$ and $\CovObs$.  
The variances $\sdOut_{\outidx}^2$ are assigned inverse gamma priors $\sdOut_{\outidx}^2 \overset{ind}{\sim} \mathcal{IG}(\alpha_p, \beta_p)$ so that 
\[\priorDens(\CovObs) = \prod_{\outidx = 1}^{\Nout} \mathcal{IG}(\sdOut_{\outidx}^2|\alpha_p, \beta_p).\]
Combining these priors with the likelihood yields the posterior 
\begin{align}
\postDens(\bpar, \CovObs) := p(\bpar, \sdOut_{1}^2, \dots, \sdOut_{\Nout}^2|\stateMat) \propto \exp\left(\llik(\bpar, \CovObs)\right)\priorDens(\bpar, \CovObs).
\end{align}
Our main focus is on calibrating $\bpar$, while $\CovObs$ primarily act as nuisance parameters. Therefore, the primary object of interest is the marginal posterior
\begin{align*}
\postDens(\bpar|\stateMat) &= \int \postDens(\bpar, \CovObs|\stateMat) d\CovObs 
\end{align*}
The above marginalization over $\CovObs$ can be performed by drawing samples from the joint posterior and then extracting the $\bpar$ component of the samples. A 
Markov Chain Monte Carlo (MCMC) algorithm for drawing such samples is detailed below.  

\subsection{Posterior Sampling}
The likelihood and prior assumptions specified above yield a convenient form of the posterior density which can be sampled using a Metropolis-within-Gibbs procedure, which samples in an 
alternating fashion from the conditional posteriors $\postDens(\bpar|\stateMat, \CovObs)$ and $\postDens(\CovObs|\stateMat, \bpar)$. While the former conditional requires a Metropolis 
accept-reject step, the latter is conditionally conjugate and hence can easily be sampled from. These (log) conditional posterior densities are provided below, with derivations detailed in 
the appendix. When dealing with log densities, we use the proportionality sign ``$\propto$'' to indicate that additive constants have been dropped. 

\bigskip
\noindent
\textbf{$\bpar$ conditional.} \textbf{TODO}

\bigskip
\noindent
\textbf{$\CovObs$ conditional.}
The log posterior of $\CovObs$ given $\bpar$ is given by 
\begin{align*}
\log \postDens(\CovObs|\bpar, \stateMat) &\propto -\sum_{\outidx = 1}^{\Nout} \left[\left(\alpha_{\outidx} + \Ntime_{\outidx}/2 + 1 \right)\log(\sdOut_{\outidx}^2) + \frac{\beta_{\outidx} + 
								  \SSROut{\outidx}(\bpar)/2}{2} \right] \\
				      			       &\propto \sum_{\outidx = 1}^{\Nout} \log \mathcal{IG}\left(\sdOut_{\outidx}^2|\alpha_{\outidx} + \Ntime_{\outidx}/2, \beta_{\outidx} + \SSROut{\outidx}(\bpar)/2 \right)
\end{align*}
That is, 
\begin{align}
\sdOut_{\outidx}^2|\bpar, \stateMat &\overset{\text{ind}}{\sim} \mathcal{IG}\left(\sdOut_{\outidx}^2|\alpha_{\outidx} + \Ntime_{\outidx}/2, \beta_{\outidx} + \SSROut{\outidx}(\bpar)/2 \right) \label{cond_post_Cov}
\end{align}

The Metropolis-within-Gibbs procedure is outlined below. 

 \begin{algorithm}[H]
	\SetAlgoLined
	
	\textbf{Require}: Initial parameter values $\bpar^{(0)}, \CovObs^{(0)}$ \\
	\textbf{Require}: Number iterations $\NMCMC$ \\
	\textbf{Require}: Proposal Covariance $\CovProp$
		
	\bigskip
	
	\For{$t = 1, \dots, \NMCMC$} {
	\textit{MH step, sample $\bpar$}: \\[.2cm]
	Sample $\bpar^\prime \sim \mathcal{N}_{\Npar}(\bpar^{(t - 1)}, \CovProp)$ \\
	$\alpha(\bpar^{(t - 1)}, \bpar^\prime) := \min\left\{1, \frac{\postDens(\bpar^\prime|\stateMat, \CovObs^{(t-1)})}{\postDens(\bpar^{(t-1)}|\stateMat, \CovObs^{(t-1)})} \right\}$ \\

	 Sample $U \sim \mathcal{U}[0, 1]$ \\
	 \If{$U < \alpha(\bpar^{(t - 1)}, \bpar^\prime)$} {
	 	$\bpar^{(t)} := \bpar^\prime$ \\
	 } \Else {
		$\bpar^{(t)} := \bpar^{(t - 1)}$ \\
	 }
	
	\bigskip
	
	\textit{Gibbs step, sample $\CovObs$}: \\[.2cm]
	Sample $\CovObs \sim  \postDens(\CovObs | \stateMat, \bpar^{(t)})$
}
\caption{MCMC algorithm: approximately sample $\postDens(\bpar, \CovObs|\stateMat)$}
\end{algorithm}
We note that in practice the proposal covariance $\CovProp$ is typically adaptively tuned; see Haario et al (1999, 2001).  

% Emulator-Based Calibration for Dynamic Models
\section{Emulator-Based Calibration for Dynamic Models}

% Basis functions
\subsection{Basis Functions}

% Likelihood and loss emulation
\subsection{Likelihood and Loss Emulation}
\textbf{TODO: better to introduce all the GP notation in the $P = 1$ case to avoid all the superscripts. Then generalize later.}

In this section we describe the emulator used to approximate the true log likelihood $\llik(\bpar)$. The log likelihood can be written as 
\begin{align*}
\llik(\bpar) &= \sum_{\outidx = 1}^{\Nout} \sum_{\timeidx = 1}^{\Ntime_{\outidx}} \log \mathcal{N}\left(\state_{\timeidx \outidx}| \fwd_{\timeidx \outidx}(\bpar), \sdOut_{\outidx}^2 \right) \\
	         &= \sum_{\outidx = 1}^{\Nout}  \sum_{\timeidx = 1}^{\Ntime_{\outidx}} \left[-\frac{1}{2} \log(2\postDens \sdOut_{\outidx}^2) - \frac{1}{2\sdOut_{\outidx}^2} (\state_{\timeidx \outidx} - \fwd_{\timeidx \outidx}(\bpar))^2\right]  \\
	         &= -\frac{1}{2} \sum_{\outidx = 1}^{\Nout} \left[\Ntime_{\outidx} \log\left(2\postDens \sdOut_{\outidx}^2 \right) + \frac{1}{\sdOut_{\outidx}^2}\sum_{\timeidx = 1}^{\Ntime_{\outidx}} \left(\state_{\timeidx \outidx} - 
	                \fwd_{\timeidx \outidx}(\bpar)\right)^2  \right] \\
	         &= -\frac{1}{2} \sum_{\outidx = 1}^{\Nout} \left[\Ntime_{\outidx} \log\left(2\postDens \sdOut_{\outidx}^2 \right) + \frac{\SSROut{\outidx}(\bpar)}{\sdOut_{\outidx}^2} \right] 
\end{align*}
where 
\[\SSROut{\outidx}(\bpar) := \sum_{\timeidx = 1}^{\Ntime_{\outidx}} (\state_{\timeidx \outidx} - \fwd_{\timeidx \outidx}(\bpar))^2\]
is the sum of squared errors between the observed data and the model output for the $\outidx^{\text{th}}$ output variable. Note that $\SSROut{\outidx}(\bpar)$ also depends on $\stateMat$, but again we suppress this in the notation. The key observation 
is that the model evaluations $\fwd(\bpar)$ only appear in the likelihood through the $\SSROut{\outidx}(\bpar)$, which means that approximating $\SSROut{\outidx}(\bpar)$ will induce an approximation of the likelihood. The independence assumptions 
that yield the product form of the likelihood make it so that $\SSR(\bpar)$ is a sufficient statistic which is not a function of the variance parameters. This means that $\SSR(\bpar)$ can be emulated without regard for the variance parameters, 
and then inference can be performed over the variance parameters as usual. The choice to emulate the mappings $\SSROut{\outidx}: \parspace \to \R$ also reduces the problem to 
approximating $\Nout$ univariate functions, instead of approximating $\fwd(\bpar)$, which has output dimension $\Ntime \times \Nout$. On a notational note, we collect the $\SSROut{\outidx}(\bpar)$ values into a $\Nout$-dimensional 
vector $\SSR := \begin{pmatrix} \SSROut{1}(\bpar), \dots, \SSROut{\Nout}(\bpar) \end{pmatrix}^\top$.  

Our emulator of choice uses Gaussian processes (GP). In particular, we treat the $\SSROut{\outidx}$ as unknown and assign them independent GP priors

\[\SSROut{\outidx}(\cdot) \overset{\text{ind}}{\sim} \GP(\GPMeanOut{\outidx}(\cdot), \GPKerOut{\outidx}(\cdot, \cdot))\] 

where $\GPMeanOut{\outidx}: \parspace \to \R$ and $\GPKerOut{\outidx}: \parspace \times \parspace \to \R_+$ are the mean and covariance function (i.e. kernel), respectively. 
Suppose we have access to observed data 
\[\designData_{\Ndesign} = \left\{(\bpar_1, \SSR(\bpar_1)), \dots, (\bpar_{\Ndesign}, \SSR(\bpar_{\Ndesign})) \right\}\]
consisting of a set of $\Ndesign$ values for the calibration parameter and their associated values of $\SSR$ resulting from full forward model evaluations. For brevity, we will henceforth denote 
$\SSR_{\designidx} := \SSR(\bpar_{\designidx})$. Conditioning the GPs on the observed data yields the GP predictive distributions
\begin{align} 
\SSRPredOut{\Ndesign}{\outidx}(\cdot) := \SSROut{\outidx}(\cdot)|\designData_{\Ndesign} \overset{\text{ind}}{\sim} \GP(\GPMeanPredOut{\Ndesign}{\outidx}(\cdot), \GPKerPredOut{\Ndesign}{\outidx}(\cdot, \cdot)), \text{ for } \outidx = 1, \dots, \Nout
\end{align}
where 
\begin{align}
\GPMeanPredOut{\Ndesign}{\outidx}(\bpar) &= \GPMeanOut{\outidx}(\bpar) + \left(\boldsymbol{\GPKerOut{\outidx}_{\bpar}}\right)^\top \KerMat{\Ndesign}^{-1} (\SSRVecPredOut{\Ndesign}{\outidx} - \GPMeanOut{\outidx}(\bpar) \oneVec{\Ndesign}) \\ 
\GPKerPredOut{\Ndesign}{\outidx}(\bpar) &= \GPKerOut{\outidx}(\bpar) - \left(\boldsymbol{\GPKerOut{\outidx}_{\bpar}}\right)^\top \KerMat{\Ndesign}^{-1} \boldsymbol{\GPKerOut{\outidx}_{\bpar}}
\end{align}
Here we have defined 
\begin{align}
\boldsymbol{\GPKerOut{\outidx}_{\bpar}} &:= \begin{pmatrix} \GPKerOut{\outidx}(\bpar_1, \bpar), \dots, \GPKerOut{\outidx}(\bpar_{\Ndesign}, \bpar)  \end{pmatrix}^\top \\ 
\SSRVecPredOut{\Ndesign}{\outidx} &:= \begin{pmatrix} \SSRPredOut{1}{\outidx}, \dots, \SSRPredOut{\Ndesign}{\outidx} \end{pmatrix}^{\top}
\end{align}

% Induced Posterior Density Approximation
\subsection{Induced Posterior Density Approximation}
Regardless of whether one directly approximates the forward model, the likelihood, or a sufficient statistic of the likelihood, ultimately the emulator induces an approximation to the posterior 
density $\postDens(\bpar, \CovObs|\stateMat)$. In the case of Gaussian process emulators, this is a \textit{stochastic} approximation, since the GP distribution of the emulator induces 
a (potentially non-Gaussian) probability distribution over $\postDens(\bpar, \CovObs|\stateMat)$. In the case of loss emulation, recall that $\SSRPred{\designidx}(\bpar)$ denotes the random 
variable with distribution given by the GP conditioned on the first $n$ design points and evaluated at input $\btheta$. We extend this notation, writing $\llik_{\designidx}(\bpar)$ and 
$\log \postDens_{\designidx}(\bpar)$ to denote, respectively, the random approximation to the log-likelihood and log-posterior induced by the emulator (e.g. $\Phi_n(\btheta)$). The distribution of 
these induced approximations will be important when considering sequential design criterions in the next section.  


\subsection{Statistical Setting}
We denote the vector of calibration parameters by $\bpar \in \parspace \subset \R^{\Npar}$. The forward model (e.g. SIPNET or ED2) $\fwd(\bpar)$ maps calibration parameters to the model outputs. Note that the forward 
model also depends on initial conditions and model drivers, but these are fixed throughout the analysis so are suppressed in the notation. Model outputs consist 
of $T$ time steps for each of $P$ output variables, so $\fwd(\bpar) \in \mathbb{R}^{\Ntime \times \Nout}$. We have observed data $Y \in \mathbb{R}^{\Ntime \times \Nout}$, potentially with missing values. Let $\Ntime_{\outidx}$ denote the number 
of non-missing observations of output variable $p$. We index the observations of output $p$ as $\{y_{tp}\}_{\timeidx = 1}^{\Ntime_{\outidx}}$, meaning that $Y$ is then technically a ragged matrix. 
This is suitable for the model detailed below, as the ordering of the observations is inconsequential. 


% Sequential Design 
\section{Sequential Design}

\subsection{Basics of Sequential Design and Bayesian Optimization}

\subsection{Batch Sequential Design}

\subsection{Unknown $\CovObs$}
The above algorithms have treated $\CovObs$ as fixed; here we return to the general setting where $\CovObs$ is unknown. To deal with this, we consider inserting an 
optimization step into algorithm \textbf{TODO: need to add algorithm above} after each batch $\btheta$ run:
\begin{align}
\CovObs_{\designidx} &:= \argmax_{\CovObs} \postDens(\CovObs|\stateMat, \currParMax{\designidx}) \label{Cov_optimization}
\end{align}
Recalling from \ref{cond_post_Cov} that $\postDens(\CovObs|\stateMat, \currParMax{\designidx})$ is a product of inverse gamma densities, the optimum $\CovObs_{\designidx}$ can 
be computed in closed form (see appendix). The optimal variances occupying the diagonal of $\CovObs_{\designidx}$ are given by 
\begin{align}
\sdOut_{\outidx}^2 &= \frac{\SSROut{\outidx}(\currParMax{\designidx})/2 + \beta_{\outidx}}{\NTimeOut{\outidx}/2 + \alpha_{\outidx} + 1}
\end{align}

% Surrogate Modeling
\section{Surrogate Modeling}

\subsection{MCMC Algorithm}
We consider the joint posterior $p(\bpar, \Sigma, \Phi^*|Y)$ and seek to marginalize out $\Phi^*$ in order to obtain the posterior of interest $p(\bpar, \Sigma|Y)$. A Gibbs sampling approach would proceed as 
\begin{itemize}
\item $\Phi \sim p(\Phi^*|\bpar, \Sigma, Y) = p(\Phi^*)$
\item $\bpar \sim p(\bpar|\Sigma, \Phi^*, Y)$
\item $\Sigma \sim p(\Sigma|\bpar, \Phi^*, Y)$
\end{itemize}
Under the model assumptions, the $\Phi^*$ and $\Sigma$ conditionals can be directly sampled from, while the $\bpar$ conditional requires a Metropolis step. The current PEcAn algorithm modifies the above by re-sampling 
$p(\Phi^*)$ again before the final line. Thus, each sweep over the conditionals results in two samples from $\Phi^*$ and one sample each from $\bpar$ and $\Sigma$. It is also important to note that 
$\Phi^*$ is a function, and in practice we only sample the function values at the points of interest. This implies the distribution sampled from is actually the marginal distribution of $\Phi^*(\bpar)$:
\[\Phi(\bpar) \sim p(\Phi^*(\bpar))\]
A more accurate high-level view of the current algorithm is thus given by 
\begin{itemize}
\item $\Phi(\bpar) \sim p(\Phi^*(\bpar))$
\item $\bpar \sim p(\bpar|\Sigma, \Phi^*(\bpar), Y)$
\item $\Phi(\bpar) \sim p(\Phi^*(\bpar))$
\item $\Sigma \sim p(\Sigma|\bpar, \Phi^*(\bpar), Y)$
\end{itemize}

% Appendix 
\section{Appendix}

\subsection{Conditional Posterior Computations: $\postDens(\bpar|\stateMat, \CovObs)$, $\postDens(\CovObs|\stateMat, \bpar)$}
\textbf{TODO}

\subsection{Sequential Design}
\subsubsection{Optimizing $\postDens(\CovObs|\stateMat, \bpar)$}
Here we derive the solution to the optimization problem \ref{Cov_optimization}. We recall from \ref{cond_post_Cov} that 
\begin{align*}
\log \postDens(\CovObs|\stateMat, \bpar) &= \sum_{\outidx = 1}^{\Nout} \log \mathcal{IG}\left(\sdOut_{\outidx}^2|\alpha_{\outidx} + \Ntime_{\outidx}/2, \beta_{\outidx} + \SSROut{\outidx}(\bpar)/2 \right)
\end{align*}
Thus, each term can be optimized independently. We have, 
\begin{align*}
\log \mathcal{IG}\left(\sdOut_{\outidx}^2|\alpha_{\outidx} + \Ntime_{\outidx}/2, \beta_{\outidx} + \SSROut{\outidx}(\bpar)/2 \right) &\propto -[\alpha_{\outidx} + \Ntime_{\outidx}/2 + 1]\log(\sdOut^2_{\outidx})
																									       - \frac{1}{\sdOut^2_{\outidx}} [\beta_{\outidx} + \SSROut{\outidx}(\bpar)/2]
\end{align*}
so 
\begin{align*}
\frac{d}{d\sdOut_{\outidx}^2}\left[\log \mathcal{IG}\left(\sdOut_{\outidx}^2|\alpha_{\outidx} + \Ntime_{\outidx}/2, \beta_{\outidx} + \SSROut{\outidx}(\bpar)/2 \right)\right] &= -\frac{1}{\sdOut_{\outidx}^2}[\alpha_{\outidx} + \Ntime_{\outidx}/2 + 1] + \frac{1}{\sdOut_{\outidx}^4} [\beta_{\outidx} + \SSROut{\outidx}(\bpar)/2]
\end{align*}
Setting the derivative equal to zero and solving for $\sdOut^2_{\outidx}$ yields
\begin{align*}
\sdOut_{\outidx}^2 &= \frac{\SSROut{\outidx}(\bpar)/2 + \beta_{\outidx}}{\NTimeOut{\outidx}/2 + \alpha_{\outidx} + 1}
\end{align*}


\subsubsection{EIVAR Acquisition Computations}





\end{document}




