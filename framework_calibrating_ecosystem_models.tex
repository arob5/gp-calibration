\documentclass[12pt]{article}
\RequirePackage[l2tabu, orthodox]{nag}
\usepackage[main=english]{babel}
\usepackage[rm={lining,tabular},sf={lining,tabular},tt={lining,tabular,monowidth}]{cfr-lm}
\usepackage{amsthm,amssymb,latexsym,gensymb,mathtools,mathrsfs}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{epstopdf,enumitem,microtype,dcolumn,booktabs,hyperref,url,fancyhdr}
\usepackage{algorithmic}
\usepackage[ruled,vlined,commentsnumbered,titlenotnumbered]{algorithm2e}
\usepackage{bbm}

% Plotting
\usepackage{pgfplots}
\usepackage{xinttools} % for the \xintFor***
\usepgfplotslibrary{fillbetween}
\pgfplotsset{compat=1.8}
\usepackage{tikz}

% Local custom commands. 
\include{local-defs}

\setlist{topsep=1ex,parsep=1ex,itemsep=0ex}
\setlist[1]{leftmargin=\parindent}
\setlist[enumerate,1]{label=\arabic*.,ref=\arabic*}
\setlist[enumerate,2]{label=(\alph*),ref=(\alph*)}

% For embedding images
\graphicspath{ {./images/} }

% Specifically for paper formatting 
\renewcommand{\baselinestretch}{1.2} % Spaces manuscript for easy reading

% Formatting definitions, propositions, etc. 
\newtheorem{definition}{Definition}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{corollary}{Corollary}

% Title and author
\title{A Framework for Scalable Ecosystem Model Calibration}
\author{Andrew Roberts}

\begin{document}

\maketitle
\tableofcontents
\newpage

% Calibration of Expensive Computer Models
\section{Calibration of Expensive Computer Models}
We consider the problem of calibrating expensive ecosystem models in a Bayesian statistical framework, leveraging 
Gaussian process (GP) surrogate models. More generally, the framework presented here is applicable to the problem of parameter 
estimation for dynamical models taking the form of a system of ordinary differential equations (ODEs). 

% Ecosystem Models 
\section{Ecosystem Models}

\subsection{The Very Simple Ecosystem Model}
The goal of this paper is to discuss the unique challenges of calibrating complex process-based ecosystem models. To motivate this problem, we introduce 
a simplified vegetation model of carbon dynamics; namely, the \textit{Very Simple Vegetation Model} (VSEM) introduced by Hartig et al \cite{Hartig} and implemented 
in the \href{https://github.com/florianhartig/BayesianTools}{\textit{BayesianTools}} R package. 
This simple model consists of a system of ordinary differential equations (ODEs) describing the fluxes of carbon between three different pools (states): 
above-ground vegetation, below-ground vegetation, and soil organic matter. The carbon dynamics are forced by a single variable, the quantity of photosynthetically 
active radiation (PAR), which represents the portion of the light spectrum usable by plants for photosynthesis. 
Let $\state_v(\timeidx)$ (\textbf{v}egetation, above ground), $\state_r(\timeidx)$ (\textbf{r}oots), and $\state_s(\timeidx)$ (\textbf{s}oil) denote the quantity of carbon (kg $C/m^2$) in each of the three respective pools at time $\timeidx$. 
The dynamics describing the carbon fluxes between these pools depend on $\text{NPP}(\timeidx)$, the Net Primary Productivity (NPP) ($\text{kg } C/m^2/\text{day}$) at time $\timeidx$, 
which is calculated as the Gross Primary Productivity (GPP) minus carbon released due to autotrophic respiration, where GPP quantifies the amount of carbon 
fixed by vegetation during photosynthesis. Given NPP, the VSEM model calculates Net Ecosystem Exchange (NEE), which is (aside from a sign change) 
NPP minus heterotrophic 
respiration. Thus, in the VSEM model this means 
\begin{align}
-\text{NEE} = \text{ GPP } - \text{ Plant Respiration } - \text{ Soil Respiration } 
\end{align}
The sign convention is that positive NPP indicates a flux into the ecosystem, while a positive NEE indicates a flux into the atmosphere, hence the addition of the negative. 
The state equations describing the carbon dynamics are then given by
\begin{align}
\dot{\state}_v(\timeidx) &= \alpha_v \times \text{NPP}(\timeidx) - \frac{\state_v(\timeidx)}{\tau_v} \label{VSEM_ODE_system} \\
\dot{\state}_r(\timeidx) &= (1.0 - \alpha_v) \times \text{NPP}(\timeidx) - \frac{\state_r(\timeidx)}{\tau_r} \nonumber \\
\dot{\state}_s(\timeidx) &= \frac{\state_r(\timeidx)}{\tau_r} + \frac{\state_v(\timeidx)}{\tau_v} - \frac{\state_s(\timeidx)}{\tau_s} \nonumber 
\end{align}
where the parameters $\tau_v$, $\tau_r$, and $\tau_s$ are residence times or longevity parameters for above-ground vegetation, below-ground vegetation, and soil organic matter, 
respectively. In particular, $\tau_v$ and $\tau_r$ represent the longevity of above and below ground biomass, respectively, while $\tau_s$ is the residence time of organic matter 
in the soil. Carbon is thus assumed to be lost from the plant pools to the soil pool at fixed turnover rates, and similarly from the soil pool to the atmosphere. VSEM also makes the simplifying assumption that a fixed proportion of NPP is allocated to above and below ground vegetation, where $\alpha_v$ is the fraction
allocated to the former. 

The dynamics [\ref{VSEM_ODE_system}] are driven by the forcing effect of PAR, which drives the values of NPP and GPP over time. VSEM assumes a simple calculation, 
where GPP is given by a product of three factors:
\begin{enumerate}
\item The amount of light available for photosynthesis (PAR) (MJ/$m^2$/day).
\item Light-use efficiency (LUE), a measure of the efficiency at which vegetation can use light for photosynthesis. 
\item The rate at which the available light decays as it passes downwards through a canopy of leaves. 
\end{enumerate}
The rate described in the third item above is modeled by the Beer-Lambert law, which yields an exponential decay rate $e^{-k*\text{LAI}}$, 
where LAI is the \textit{leaf-area index}, defined as the ratio of one-sided leaf area per unit of ground area. LAI at time $\timeidx$ is assumed to be given by the product of a fixed 
leaf-area ratio (LAR) and $\state_v(\timeidx)$. The constant $k$ is a fixed extinction rate controlling the rate of exponential decay. The full calculations for NPP are given below. 
\begin{align}
\text{LAI}(\timeidx, \state_v) &= \text{LAR} \times \state_v(\timeidx) \\
\text{GPP}(\timeidx) &= \text{PAR}(\timeidx) \times \text{LUE} \times \left(1 -  \exp\left(-k \times \text{LAI}(\timeidx, \state_v) \right) \right) \nonumber \\
\text{NPP}(\timeidx) &= (1 - \gamma) \times \text{GPP}(\timeidx) \nonumber
\end{align}
As seen above, NPP is assumed to be a fixed fraction $1 - \gamma$ of GPP.

Potential calibration parameters $\theta$ of this model include $\alpha_v$, $\tau_v$, $\tau_r$, $\tau_s$, $\text{LAR}$, $k$, and $\gamma$, but it is common to fix some of the parameters at their 
nominal values and calibrate the remaining subset. 
Once the parameters $\bpar$ and a specified number of time steps (days) $n$ are fixed, the ODE system can be solved, yielding daily time series of $n$
observations for NEE, $\state_v$, $\state_r$, and $\state_s$. The forward model $\fwd$ in this setting is given by an execution of the ODE solve, and represents
the mapping from the inputs to these four resulting time series. A more detailed discussion of the forward model is given in the following section. 

\subsection{Generic Dynamical Models}
I believe this work nicely complements a series of papers on so-called \textit{dynamic} or \textit{time-series valued} emulation. The necessity of such emulators typically arise from forward models 
defined as a the solution of an autonomous system of ODEs. I will denote the state vector of such a system by 
\[\bx(t) = \left(x_1(t), \dots, x_P(t) \right)^{\top} \]
If we consider discretizing at constant time steps $\Delta t$ then since the system is autonomous the \textit{one-step map} or \textit{flow map} $g: \R^P \times \R^Q \to \R^P$ is time-invariant: 
 \[\bx(t + \Delta t) = g(\bx(t); \bw_{t + \Delta t})\]
 where $\bw_{t + \Delta t}$ is a \textit{forcing input}. The forcing inputs are assumed known, and vary across time. 
 Note that this could also be generalized to variable step sizes $\Delta t$ by considering the flow map to also be a function of the step size; i.e., $g = g(\bx(t), \Delta t)$. We can also view the 
 vector-valued $g$ as a collection of $P$ univariate flow maps
 \[g(\bx(t); \bw_{t + \Delta t}) = \left[g_1(\bx(t); \bw_{t + \Delta t}), \dots, g_P(\bx(t); \bw_{t + \Delta t}) \right]^{\top}\]
 so that $g_k: \R^P \to \R$ maps $\bx(t)$ to $\bx_p(t + \Delta t)$, the value of the $p^{\text{th}}$ state at the subsequent time step. Of course, $g$ and hence the states $\bx(t)$ depend on 
 the parameters $\bpar$. I will reflect this dependence by writing $g_{\bpar}$ and $\bx_{\bpar}(t)$. The forward model $G: \R^{\Npar} \to \R^{\Ntime \times \Nout}$ defined previously can thus be characterized 
 as 
 \[
 G(\bpar; \bx_0, \bw) = \begin{pmatrix} \bx_0^{\top} \\ \bx_\theta(\Delta t)^{\top} \\  \bx_\theta(2\Delta t)^{\top} \\ \vdots \\ \bx_\theta\left([T-1]\Delta t\right)^{\top} \end{pmatrix} = 
 \begin{pmatrix} \bx_0^{\top} \\ g_\theta(\bx_0; \bw_1)^{\top} \\  g_\theta\left(g_\theta(\bx_0; \bw_1); \bw_2 \right)^{\top} \\ \vdots \\ g_\theta^{(T-1)}(\bx_0; \bw)^{\top} \end{pmatrix}
 \]
 where I now explicitly write $G$ as a function of an initial condition $\bx_0 = \bx(0)$, which is independent of $\bpar$, and I use the shorthand $g_\theta^{(k)}$ to denote 
 the composition consisting of $k$ applications of the map $g_\theta$. I also let $\bw := \{\bw_t\}_{\timeidx = 1}^{T}$ denote the sequence of forcing inputs. 
 I have also assumed that the initial time is $0$, but some other time $t_0$ could of course be considered. 

% Bayesian Calibration 
\section{Bayesian Calibration}

\subsection{Problem Setup}
In this section we provide a brief overview of the Bayesian approach to computer model calibration, which has been recently favored by researchers due to its ability to quantify 
uncertainties in the calibration process. We begin by defining a likelihood function $p(\stateMat|\bpar)$ that relates the observed field data $\stateMat$ to the forward model prediction 
$\fwd(\bpar)$. Throughout most of this article, we assume the following Gaussian likelihood 
\begin{align}
\stateOut{\outidx}, \CovObs | \bpar \overset{\text{ind}}{\sim} \mathcal{N}_{\Ntime_{\outidx}}(\fwdOut{\outidx}(\bpar), \sdOut_{\outidx}^2 I) \label{likelihood}
\end{align}
For notational convenience, we have collected the 
variance parameters in the matrix $\CovObs := \text{diag}\left(\sdOut_1^2, \dots, \sdOut_{\Nout}^2 \right)$. 
This likelihood assumes the errors are independent across time and output variable. 
We will typically work with the log of the likelihood, denoted by 
$\llik(\bpar, \CovObs) := \log p(\stateMat|\bpar, \CovObs)$. Note that $\llik(\bpar, \CovObs)$ depends on the data $\stateMat$, 
but $\stateMat$ is constant throughout the analysis so we drop it from the notation.
Under the likelihood \ref{likelihood}, $\llik(\bpar, \CovObs)$ takes the form 
\begin{align*}
\llik(\bpar, \CovObs) &= \sum_{\outidx = 1}^{\Nout} \sum_{\timeidx = 1}^{\Ntime_{\outidx}} \log \mathcal{N}\left(\stateTimeOut{\timeidx}{\outidx} | \fwdTimeOut{\timeidx}{\outidx}(\bpar), \sdOut_{\outidx}^2 \right)
\end{align*}
Missing observations are simply ignored. For future purposes, it will be useful to expand this sum of Gaussian densities and introduce some notation. To this end, the log likelihood can be written as 
\begin{align}
\llik(\bpar, \CovObs) &= \sum_{\outidx = 1}^{\Nout} \sum_{\timeidx = 1}^{\Ntime_{\outidx}} \log \mathcal{N}\left(\state_{\timeidx \outidx}| \fwd_{\timeidx \outidx}(\bpar), \sdOut_{\outidx}^2 \right) \nonumber \\
	         &= \sum_{\outidx = 1}^{\Nout}  \sum_{\timeidx = 1}^{\Ntime_{\outidx}} \left[-\frac{1}{2} \log(2\postDens \sdOut_{\outidx}^2) - \frac{1}{2\sdOut_{\outidx}^2} (\state_{\timeidx \outidx} - \fwd_{\timeidx \outidx}(\bpar))^2\right] \nonumber \\
	         &= -\frac{1}{2} \sum_{\outidx = 1}^{\Nout} \left[\Ntime_{\outidx} \log\left(2\postDens \sdOut_{\outidx}^2 \right) + \frac{1}{\sdOut_{\outidx}^2}\sum_{\timeidx = 1}^{\Ntime_{\outidx}} \left(\state_{\timeidx \outidx} - 
	                \fwd_{\timeidx \outidx}(\bpar)\right)^2  \right] \nonumber \\
	         &= -\frac{1}{2} \sum_{\outidx = 1}^{\Nout} \left[\Ntime_{\outidx} \log\left(2\postDens \sdOut_{\outidx}^2 \right) + \frac{\SSROut{\outidx}(\bpar)}{\sdOut_{\outidx}^2} \right] \label{llik}
\end{align}
where 
\begin{align}
\SSROut{\outidx}(\bpar) := \sum_{\timeidx = 1}^{\Ntime_{\outidx}} (\state_{\timeidx \outidx} - \fwd_{\timeidx \outidx}(\bpar))^2
\end{align}
is the squared Euclidean error between the observations $\stateOut{\outidx}$ and computer model predictions $\fwdOut{\outidx}(\bpar)$ for the $\outidx^{\text{th}}$ data constraint. 

With the likelihood established, we now define a prior distribution jointly over the calibration parameters $\btheta$ and likelihood variance parameters $\CovObs$. 
We denote the prior density for this distribution as $\priorDens(\bpar, \CovObs) = \priorDens(\bpar)\priorDens(\CovObs)$, assuming prior independence between $\bpar$ and $\CovObs$.  
The variances $\sdOut_{\outidx}^2$ are assigned inverse gamma priors $\sdOut_{\outidx}^2 \overset{ind}{\sim} \mathcal{IG}(\alpha_p, \beta_p)$ so that 
\begin{align}
\priorDens(\CovObs) = \prod_{\outidx = 1}^{\Nout} \mathcal{IG}(\sdOut_{\outidx}^2|\alpha_p, \beta_p). \label{inv_gamma_prior}
\end{align}
Combining these priors with the likelihood yields the posterior 
\begin{align}
\postDens(\bpar, \CovObs) := p(\bpar, \sdOut_{1}^2, \dots, \sdOut_{\Nout}^2|\stateMat) \propto \exp\left(\llik(\bpar, \CovObs)\right)\priorDens(\bpar, \CovObs). \label{posterior_density}
\end{align}
Our main focus is on calibrating $\bpar$, while $\CovObs$ primarily act as nuisance parameters. Therefore, the primary object of interest is the marginal posterior
\begin{align*}
\postDens(\bpar|\stateMat) &= \int \postDens(\bpar, \CovObs|\stateMat) d\CovObs 
\end{align*}
The above marginalization over $\CovObs$ can be performed by drawing samples from the joint posterior and then extracting the $\bpar$ component of the samples. A 
Markov Chain Monte Carlo (MCMC) algorithm for drawing such samples is detailed below.  

\subsection{Posterior Sampling}
The likelihood and prior assumptions specified above yield a convenient form of the posterior density which can be sampled using a Metropolis-within-Gibbs procedure, which samples in an 
alternating fashion from the conditional posteriors $\postDens(\bpar|\stateMat, \CovObs)$ and $\postDens(\CovObs|\stateMat, \bpar)$. While the former conditional requires a Metropolis 
accept-reject step, the latter is conditionally conjugate and hence can easily be sampled from. These (log) conditional posterior densities are provided below, with derivations detailed in 
the appendix. When dealing with log densities, we use the proportionality sign ``$\propto$'' to indicate that additive constants have been dropped. 

\bigskip
\noindent
\textbf{$\bpar$ conditional.} 
The log posterior of $\bpar$ given $\CovObs$ is given by 
\begin{align*}
\log\left[\postDens(\bpar|\stateMat, \CovObs)\right] &\propto -\frac{1}{2} \sum_{\outidx = 1}^{\Nout} \left[\frac{\SSROut{\outidx}(\bpar)}{\sdOut_{\outidx}^2}\right]  + \log\left[\priorDens(\bpar)\right] 
\end{align*}

\bigskip
\noindent
\textbf{$\CovObs$ conditional.}
The log posterior of $\CovObs$ given $\bpar$ is given by 
\begin{align*}
\log \postDens(\CovObs|\bpar, \stateMat) &\propto -\sum_{\outidx = 1}^{\Nout} \left[\left(\alpha_{\outidx} + \Ntime_{\outidx}/2 + 1 \right)\log(\sdOut_{\outidx}^2) + \frac{\beta_{\outidx} + 
								  \SSROut{\outidx}(\bpar)/2}{2} \right] \\
				      			       &\propto \sum_{\outidx = 1}^{\Nout} \log \mathcal{IG}\left(\sdOut_{\outidx}^2|\alpha_{\outidx} + \Ntime_{\outidx}/2, \beta_{\outidx} + \SSROut{\outidx}(\bpar)/2 \right)
\end{align*}
That is, 
\begin{align}
\sdOut_{\outidx}^2|\bpar, \stateMat &\overset{\text{ind}}{\sim} \mathcal{IG}\left(\sdOut_{\outidx}^2|\alpha_{\outidx} + \Ntime_{\outidx}/2, \beta_{\outidx} + \SSROut{\outidx}(\bpar)/2 \right) \label{cond_post_Cov}
\end{align}

The Metropolis-within-Gibbs procedure is outlined below. 

 \begin{algorithm}[H]
	\SetAlgoLined
	
	\textbf{Require}: Initial parameter values $\bpar^{(0)}, \CovObs^{(0)}$ \\
	\textbf{Require}: Number iterations $\NMCMC$ \\
	\textbf{Require}: Proposal Covariance $\CovProp$
		
	\bigskip
	
	\For{$t = 1, \dots, \NMCMC$} {
	\textit{MH step, sample $\bpar$}: \\[.2cm]
	Sample $\bpar^\prime \sim \mathcal{N}_{\Npar}(\bpar^{(t - 1)}, \CovProp)$ \\
	$\alpha(\bpar^{(t - 1)}, \bpar^\prime) := \min\left\{1, \frac{\postDens(\bpar^\prime|\stateMat, \CovObs^{(t-1)})}{\postDens(\bpar^{(t-1)}|\stateMat, \CovObs^{(t-1)})} \right\}$ \\

	 Sample $U \sim \mathcal{U}[0, 1]$ \\
	 \If{$U < \alpha(\bpar^{(t - 1)}, \bpar^\prime)$} {
	 	$\bpar^{(t)} := \bpar^\prime$ \\
	 } \Else {
		$\bpar^{(t)} := \bpar^{(t - 1)}$ \\
	 }
	
	\bigskip
	
	\textit{Gibbs step, sample $\CovObs$}: \\[.2cm]
	Sample $\CovObs^{(t)} \sim  \postDens(\CovObs | \stateMat, \bpar^{(t)})$
}
\caption{MCMC algorithm: approximately sample $\postDens(\bpar, \CovObs|\stateMat)$}
\end{algorithm}
We note that in practice the proposal covariance $\CovProp$ is typically adaptively tuned; see Haario et al (1999, 2001).  

% Emulator-Based Calibration for Dynamic Models
\section{Emulator-Based Calibration for Dynamic Models}

% Basis functions
\subsection{Basis Functions}

% Likelihood and loss emulation
\subsection{Likelihood and Loss Emulation}
\textbf{TODO: better to introduce all the GP notation in the $P = 1$ case to avoid all the superscripts. Then generalize later.}

In this section we describe the emulator used to approximate the true log likelihood $\llik(\bpar)$. Recall from \ref{llik} that the log likelihood can be written as 
\begin{align*}
\llik(\bpar, \CovObs) &= -\frac{1}{2} \sum_{\outidx = 1}^{\Nout} \left[\Ntime_{\outidx} \log\left(2\postDens \sdOut_{\outidx}^2 \right) + \frac{\SSROut{\outidx}(\bpar)}{\sdOut_{\outidx}^2} \right] 
\end{align*}
where 
\[\SSROut{\outidx}(\bpar) := \sum_{\timeidx = 1}^{\Ntime_{\outidx}} (\state_{\timeidx \outidx} - \fwd_{\timeidx \outidx}(\bpar))^2.\]
The key observation 
is that the model evaluations $\fwd(\bpar)$ only appear in the likelihood through the $\SSROut{\outidx}(\bpar)$, which means that replacing $\SSROut{\outidx}(\bpar)$ with a
computationally cheaper approximation will induce a cheap approximation of the likelihood. The independence assumptions 
that yield the product form of the likelihood make it so that $\SSR(\bpar)$ is a sufficient statistic, and not a function of the variance parameters $\CovObs$. 
This means that $\SSR(\bpar)$ can be emulated independently of the value of $\CovObs$, unlike in the likelihood emulation setting. 
The choice to emulate the mappings $\SSROut{\outidx}: \parspace \to \R$ also reduces the problem to 
approximating $\Nout$ univariate functions, instead of approximating $\fwd(\bpar)$, which has output dimension $\Ntime \times \Nout$. On a notational note, we collect the $\SSROut{\outidx}(\bpar)$ values into a $\Nout$-dimensional 
vector $\SSR(\bpar) := \begin{pmatrix} \SSROut{1}(\bpar), \dots, \SSROut{\Nout}(\bpar) \end{pmatrix}^\top$.  

Our emulator of choice uses Gaussian processes (GP). In particular, we treat the $\SSROut{\outidx}$ as unknown and assign them independent GP priors

\[\SSROut{\outidx}(\cdot) \overset{\text{ind}}{\sim} \GP(\GPMeanOut{\outidx}(\cdot), \GPKerOut{\outidx}(\cdot, \cdot))\] 

where $\GPMeanOut{\outidx}: \parspace \to \R$ and $\GPKerOut{\outidx}: \parspace \times \parspace \to \R_+$ are the mean and covariance function (i.e. kernel), respectively. 
Suppose we have access to observed data 
\[\designData_{\Ndesign} = \left\{(\bpar_1, \SSR(\bpar_1)), \dots, (\bpar_{\Ndesign}, \SSR(\bpar_{\Ndesign})) \right\}\]
consisting of a set of $\Ndesign$ values for the calibration parameter and their associated values of $\SSR$ resulting from full forward model evaluations. For brevity, we will henceforth denote 
$\SSR_{\designidx} := \SSR(\bpar_{\designidx})$. Conditioning the GPs on the observed data yields the GP predictive distributions
\begin{align} 
\SSRPredOut{\Ndesign}{\outidx}(\cdot) := \SSROut{\outidx}(\cdot)|\designData_{\Ndesign} \overset{\text{ind}}{\sim} \GP(\GPMeanPredOut{\Ndesign}{\outidx}(\cdot), \GPKerPredOut{\Ndesign}{\outidx}(\cdot, \cdot)), \text{ for } \outidx = 1, \dots, \Nout
\end{align}
where 
\begin{align}
\GPMeanPredOut{\Ndesign}{\outidx}(\bpar) &= \GPMeanOut{\outidx}(\bpar) + \left(\boldsymbol{\GPKerOut{\outidx}_{\bpar}}\right)^\top \KerMat{\Ndesign}^{-1} (\SSRVecPredOut{\Ndesign}{\outidx} - \GPMeanOut{\outidx}(\bpar) \oneVec{\Ndesign}) \\ 
\GPKerPredOut{\Ndesign}{\outidx}(\bpar) &= \GPKerOut{\outidx}(\bpar) - \left(\boldsymbol{\GPKerOut{\outidx}_{\bpar}}\right)^\top \KerMat{\Ndesign}^{-1} \boldsymbol{\GPKerOut{\outidx}_{\bpar}}
\end{align}
Here we have defined 
\begin{align}
\boldsymbol{\GPKerOut{\outidx}_{\bpar}} &:= \begin{pmatrix} \GPKerOut{\outidx}(\bpar_1, \bpar), \dots, \GPKerOut{\outidx}(\bpar_{\Ndesign}, \bpar)  \end{pmatrix}^\top \\ 
\SSRVecPredOut{\Ndesign}{\outidx} &:= \begin{pmatrix} \SSRPredOut{1}{\outidx}, \dots, \SSRPredOut{\Ndesign}{\outidx} \end{pmatrix}^{\top}
\end{align}

% Induced Posterior Density Approximation
\subsection{Induced Posterior Density Approximation}
Regardless of whether one directly approximates the forward model, the likelihood, or a sufficient statistic of the likelihood, ultimately the emulator induces an approximation to the posterior 
density $\postDens(\bpar, \CovObs|\stateMat)$. In the case of Gaussian process emulators, this is a \textit{stochastic} approximation, since the GP distribution of the emulator induces 
a (potentially non-Gaussian) probability distribution over $\postDens(\bpar, \CovObs|\stateMat)$. In the case of loss emulation, recall that $\SSRPred{\designidx}(\bpar)$ denotes the random 
variable with distribution given by the GP conditioned on the first $n$ design points and evaluated at input $\btheta$. We extend this notation, writing $\llik_{\designidx}(\bpar, \CovObs)$ and 
$\log \postDens_{\designidx}(\bpar, \CovObs)$ to denote, respectively, the random approximation to the log-likelihood and log-posterior induced by the emulator (e.g. $\Phi_n(\btheta)$). The distribution of 
these induced approximations will be important when considering sequential design criteria in the next section. We therefore characterize the induced distributions for the various emulation 
targets below. 

\subsubsection{Loss Emulation}
We begin by considering the case where GP emulators are fit to the squared loss $\SSR(\bpar)$.
Plugging the emulator $\SSR_n(\bpar)$ in place of $\SSR(\bpar)$ in the log-likelihood
\ref{llik} yields the approximation 
\begin{align*}
\llik_{\designidx}(\bpar, \CovObs) &= -\frac{1}{2} \sum_{\outidx = 1}^{\Nout} \left[\Ntime_{\outidx} \log\left(2\postDens \sdOut_{\outidx}^2 \right) + \frac{\SSRPredOut{\designidx}{\outidx}(\bpar)}{\sdOut_{\outidx}^2} \right].
\end{align*}
Given that the above sum represents a linear combination of independent Gaussians 
$\SSRPredOut{\designidx}{\outidx}(\bpar) \overset{\text{ind}}{\sim} \mathcal{N}(\GPMeanPredOut{\designidx}{\outidx}(\bpar), \GPKerPredOut{\designidx}{\outidx}(\bpar, \bpar))$ then it follows that 
$\llik_{\designidx}(\bpar, \CovObs)$ is Gaussian, with moments 

\begin{align}
\E[\llik_{\designidx}(\bpar, \CovObs)] &= -\frac{1}{2} \sum_{\outidx = 1}^{\Nout} \left[\Ntime_{\outidx} \log\left(2\postDens \sdOut_{\outidx}^2 \right) + \frac{\GPMeanPredOut{\designidx}{\outidx}(\bpar)}{\sdOut_{\outidx}^2} \right] \label{llik_approx_mean} \\
\Var[\llik_{\designidx}(\bpar, \CovObs)] &= \frac{1}{4} \sum_{\outidx = 1}^{\Nout} \frac{\GPKerPredOut{\designidx}{\outidx}(\bpar)}{\sdOut_{\outidx}^4} \label{llik_approx_var}
\end{align}
The distribution of $\log \postDens_{\designidx}(\bpar, \CovObs)$ is identical except that $\log \priorDens(\bpar, \CovObs)$ is added to the mean. 

Since the log likelihood and unnormalized posterior random approximations are Gaussian, it follows that their non-log analogs have log-normal distributions. 
We recall that the unnormalized posterior approximation is given by 
\begin{align*}
\postDens_{\designidx}(\bpar) &= \exp\left(\llik_{\designidx}(\bpar, \CovObs) \right)\priorDens(\bpar, \CovObs)
\end{align*}
So $\postDens_{\designidx}(\bpar)$ is log-normally distributed with 
\begin{align}
\E\left[\postDens_{\designidx}(\bpar)\right] &= \exp\left\{\E[\llik_{\designidx}(\bpar, \CovObs)] + \log\left(\priorDens(\bpar, \CovObs)\right) \right\} \\
\Var\left[\postDens_{\designidx}(\bpar)\right] &= \exp\left\{\E[\llik_{\designidx}(\bpar, \CovObs)] + \frac{1}{2}\Var[\llik_{\designidx}(\bpar, \CovObs)] \right\} \label{post_approx_var}
\end{align}

% Surrogate-Assisted MCMC
\subsection{Surrogate-Assisted MCMC}
Recall that in the case of an expensive forward model $\fwd$, MCMC is often impractical due to the fact that each log-likelihood evaluation $\llik(\bpar, \CovObs)$ requires a forward model 
 run. In order to make MCMC feasible, we replace the expensive log-likelihood with the cheaper approximation $\llik_{\Ndesign}(\bpar, \CovObs)$. This implies that we will no longer be 
 sampling from the exact posterior \ref{posterior_density}, but rather the approximation
 \begin{align}
 \postDens_{\Ndesign}(\bpar, \CovObs) = \exp\left(\llik_{\Ndesign}(\bpar, \CovObs)\right)\priorDens(\bpar, \CovObs) \label{approx_posterior_density}
 \end{align}
 However, it is not immediately clear how to run MCMC on the approximate posterior given that, for each $\bpar$,  $\postDens_{\Ndesign}(\bpar, \CovObs)$ is a random variable. 

% Sequential Design 
\section{Sequential Design}

\subsection{Basics of Sequential Design and Bayesian Optimization}

\subsection{Acquisition Functions}
Here we introduce acquisition functions that are tailored to solving Bayesian inverse problems. 

\subsubsection{Expected Integrated Variance}
We first consider an acquisition introduced in Sinsbeck and Nowak (2017) and independently 
in Surer et al (2023). We adopt the latter paper's convention of referring to this as an expected integrated variance (EIVAR) criterion. 
\begin{align}
\acq{\designidx}(\tilde{\bpar}) &= \E_{\bpar \sim \rho} \E_{\tilde{\postDens}} \Var\left[\postDens_{\designidx}(\bpar) | (\tilde{\bpar}, \tilde{\postDens}) \right] \label{EIVAR}
\end{align}
The inner term is the predictive variance of the posterior approximation conditioned on $\designData_{\designidx} \cup (\tilde{\bpar}, \tilde{\postDens})$, where 
$\tilde{\postDens} := \postDens(\tilde{\bpar})$ is the hypothetical posterior density evaluation at input $\tilde{\bpar}$. Since the forward model has not yet been evaluated at 
this input, $\tilde{\postDens}$ is unknown, and hence the inner expectation $\E_{\tilde{\postDens}}$ integrates over this uncertainty. The outer expectation $\E_{\bpar \sim \rho}$
averages this expected predictive variance across the input space $\parspace$, weighted by a density $\rho$. Both of the above mentioned papers take $\rho = \priorDens$, but we 
also consider choosing $\rho$ to be an approximation to $\postDens$. We also note that we can alternatively consider $\postDens_{\designidx}(\bpar)$ to be either the 
approximate posterior or log posterior density, but we present the generic EIVAR expression without the log for brevity. The criterion proposed in Sinsbeck and Nowak (2017) actually 
considers targeting the likelihood approximation in place of the posterior. 

\bigskip
\noindent
\textbf{Loss Emulation.} The loss emulation setting yields a very convenient form of EIVAR, especially for the EIVAR version targeting the log posterior density. In this case,
$\log \postDens_{\designidx}(\bpar)$ is Gaussian distributed and $\Var\left[\log \postDens_{\designidx}(\bpar) | (\tilde{\bpar}, \tilde{\postDens}) \right]$ is 
available in closed-form (see \ref{loss_emulation_dist}). Moreover, this variance does not depend on $\tilde{\postDens}$ so the inner expectation $\E_{\tilde{\postDens}}$ vanishes. 
Therefore, the EIVAR expression simplifies to  
setting is 
\begin{align}
\acq{\designidx}(\tilde{\bpar}) &= \frac{1}{4} \sum_{\outidx = 1}^{\Nout} \frac{\E_{\bpar \sim \rho}\left[\GPKerPredOut{\designidx, \tilde{\bpar}}{\outidx}(\bpar)\right]}{\sdOut_{\outidx}^4}
\end{align}
where 
\begin{align*}
\GPKerPredOut{\designidx, \tilde{\bpar}}{\outidx}(\bpar) &= \GPKerPredOut{\designidx}{\outidx}(\bpar) - \left(\boldsymbol{\GPKerOut{\outidx}_{n,\tilde{\bpar}}}\right)^\top \left(\KerMatOut{\designidx, \tilde{\bpar}}{\outidx}\right)^{-1} \boldsymbol{\GPKerOut{\outidx}_{\designidx, \tilde{\bpar}}}
\end{align*}
and 
\begin{align*}
\boldsymbol{\GPKerOut{\outidx}_{\designidx, \tilde{\bpar}}} &= \begin{pmatrix} \GPKerOut{\outidx}(\bpar_1, \bpar), \dots, 
														       \GPKerOut{\outidx}(\bpar_{\designidx}, \bpar), 
														       \GPKerOut{\outidx}(\tilde{\bpar}, \bpar)  \end{pmatrix}^\top 
\end{align*}
and similarly $\KerMatOut{\designidx, \tilde{\bpar}}{\outidx}$ is the kernel matrix constructed from the extended design $\designData_{\designidx} \cup \{\tilde{\bpar}\}$.

\subsection{Unknown $\CovObs$}
The above algorithms have treated $\CovObs$ as fixed; here we return to the general setting where $\CovObs$ is unknown. To deal with this, we consider inserting an 
optimization step into algorithm \textbf{TODO: need to add algorithm above} after each batch $\btheta$ run:
\begin{align}
\CovObs_{\designidx} &:= \argmax_{\CovObs} \postDens(\CovObs|\stateMat, \currParMax{\designidx}) \label{Cov_optimization}
\end{align}
Recalling from \ref{cond_post_Cov} that $\postDens(\CovObs|\stateMat, \currParMax{\designidx})$ is a product of inverse gamma densities, the optimum $\CovObs_{\designidx}$ can 
be computed in closed form (see appendix). The optimal variances occupying the diagonal of $\CovObs_{\designidx}$ are given by 
\begin{align}
\sdOut_{\outidx}^2 &= \frac{\SSROut{\outidx}(\currParMax{\designidx})/2 + \beta_{\outidx}}{\NTimeOut{\outidx}/2 + \alpha_{\outidx} + 1}
\end{align}

\subsection{Batch Sequential Design}


% Appendix 
\section{Appendix}

\subsection{Posterior Computations}

\subsubsection{Gaussian-Inverse Gamma Model}
Here we calculate the conditional posterior distributions $\postDens(\bpar|\stateMat, \CovObs)$, $\postDens(\CovObs|\stateMat, \bpar)$ under the independent Gaussian 
likelihood \ref{likelihood} and inverse Gamma prior \ref{inv_gamma_prior}. We allow for an arbitrary prior $\priorDens(\bpar)$ on the calibration parameters.
Under this model, the joint log posterior over $\bpar$, $\CovObs$ has the form 
\begin{align*}
\log\left[\postDens(\bpar, \CovObs|\stateMat)\right] &\propto -\sum_{\outidx = 1}^{\Nout} \left[\frac{\Ntime_{\outidx}}{2} \log\left(2\pi \sdOut_{\outidx}^2 \right) + \frac{\SSROut{\outidx}(\bpar)}{2\sdOut_{\outidx}^2}  - (\alpha_{\outidx} + 1)\log(\sdOut_{\outidx}^{-2}) + \frac{\beta_{\outidx}}{\sdOut_{\outidx}^2} \right] + \log\left[\priorDens(\bpar)\right] 
\end{align*}
where in the first line we recall the derivation \ref{llik} for the log-likelihood. For the $\bpar$ conditional we drop terms without $\bpar$ dependence, yielding
\begin{align*}
\log\left[\postDens(\bpar|\stateMat, \CovObs)\right] &\propto -\frac{1}{2} \sum_{\outidx = 1}^{\Nout} \left[\frac{\SSROut{\outidx}(\bpar)}{\sdOut_{\outidx}^2}\right]  + \log\left[\priorDens(\bpar)\right] 
\end{align*}
We proceed similarly to derive the $\CovObs$ conditional \ref{cond_post_Cov}
\begin{align*}
\log\left[\postDens(\bpar, \CovObs|\stateMat)\right] &\propto - \sum_{\outidx = 1}^{\Nout} \left[\frac{\Ntime_{\outidx}}{2} \log\left(2\pi \sdOut_{\outidx}^2 \right) + \frac{\SSROut{\outidx}(\bpar)}{2\sdOut_{\outidx}^2}  + (\alpha_{\outidx} + 1)\log(\sdOut_{\outidx}^{2}) + \frac{\beta_{\outidx}}{\sdOut_{\outidx}^2} \right] \\
&\propto - \sum_{\outidx = 1}^{\Nout} \left[(\Ntime_{\outidx}/2 + \alpha_{\outidx} + 1)\log(\sdOut_{\outidx}^2) + \frac{\SSROut{\outidx}(\bpar)/2 + \beta_{\outidx}}{\sdOut_{\outidx}^2} \right] \\
&\propto \sum_{\outidx = 1}^{\Nout} \log \mathcal{IG}\left(\sdOut_{\outidx}^2|\alpha_{\outidx} + \Ntime_{\outidx}/2, \beta_{\outidx} + \SSROut{\outidx}(\bpar)/2 \right)
\end{align*}


\subsection{Sequential Design}
\subsubsection{Optimizing $\postDens(\CovObs|\stateMat, \bpar)$}
Here we derive the solution to the optimization problem \ref{Cov_optimization}. We recall from \ref{cond_post_Cov} that 
\begin{align*}
\log \postDens(\CovObs|\stateMat, \bpar) &= \sum_{\outidx = 1}^{\Nout} \log \mathcal{IG}\left(\sdOut_{\outidx}^2|\alpha_{\outidx} + \Ntime_{\outidx}/2, \beta_{\outidx} + \SSROut{\outidx}(\bpar)/2 \right)
\end{align*}
Thus, each term can be optimized independently. We have, 
\begin{align*}
\log \mathcal{IG}\left(\sdOut_{\outidx}^2|\alpha_{\outidx} + \Ntime_{\outidx}/2, \beta_{\outidx} + \SSROut{\outidx}(\bpar)/2 \right) &\propto -[\alpha_{\outidx} + \Ntime_{\outidx}/2 + 1]\log(\sdOut^2_{\outidx})
																									       - \frac{1}{\sdOut^2_{\outidx}} [\beta_{\outidx} + \SSROut{\outidx}(\bpar)/2]
\end{align*}
so 
\begin{align*}
\frac{d}{d\sdOut_{\outidx}^2}\left[\log \mathcal{IG}\left(\sdOut_{\outidx}^2|\alpha_{\outidx} + \Ntime_{\outidx}/2, \beta_{\outidx} + \SSROut{\outidx}(\bpar)/2 \right)\right] &= -\frac{1}{\sdOut_{\outidx}^2}[\alpha_{\outidx} + \Ntime_{\outidx}/2 + 1] + \frac{1}{\sdOut_{\outidx}^4} [\beta_{\outidx} + \SSROut{\outidx}(\bpar)/2]
\end{align*}
Setting the derivative equal to zero and solving for $\sdOut^2_{\outidx}$ yields
\begin{align*}
\sdOut_{\outidx}^2 &= \frac{\SSROut{\outidx}(\bpar)/2 + \beta_{\outidx}}{\NTimeOut{\outidx}/2 + \alpha_{\outidx} + 1}
\end{align*}


\subsubsection{EIVAR Acquisition Computations}
In this section we provide computations related to the EIVAR criterion \ref{EIVAR} in the various emulation settings. Recall that EIVAR is generally defined as 
\begin{align*}
\acq{\designidx}(\tilde{\bpar}) = \E_{\bpar \sim \rho} \E_{\tilde{\postDens}} \Var\left[\postDens_{\designidx}(\bpar) | (\tilde{\bpar}, \tilde{\postDens}) \right] 
\end{align*}
where $\tilde{\postDens}$ be also be replaced by its logarithm. 


\noindent
\textbf{Loss Emulation (log).} 
We begin by deriving the EIVAR criterion in the loss emulation setting. Recall from \ref{llik_approx_var} that 
\begin{align*}
\Var[\llik_{\designidx}(\bpar, \CovObs)] &= \frac{1}{4} \sum_{\outidx = 1}^{\Nout} \frac{\GPKerPredOut{\designidx}{\outidx}(\bpar)}{\sdOut_{\outidx}^4}
\end{align*}
Conditioning on $(\tilde{\bpar}, \tilde{\llik}_{\designidx})$ thus yields 
\begin{align*}
\Var[\llik_{\designidx}(\bpar, \CovObs) | (\tilde{\bpar}, \tilde{\llik}_{\designidx})] &= \frac{1}{4} \sum_{\outidx = 1}^{\Nout} \frac{\GPKerPredOut{\designidx, \tilde{\bpar}}{\outidx}(\bpar)}{\sdOut_{\outidx}^4}
\end{align*}
which does not depend on the unknown response $\tilde{\llik}_{\designidx}$. 

\noindent
\textbf{Loss Emulation.}
 We next consider directly targeting $\postDens_{\designidx}$ instead of its logarithm. We recall that the unnormalized posterior approximation is given 
 by 
 \begin{align*}
 \postDens_{\designidx}(\bpar) = \exp\left(\llik_{\designidx}(\bpar, \CovObs) \right)\priorDens(\bpar, \CovObs)
 \end{align*}
 Also recall from \ref{post_approx_var} that 
 \begin{align*}
 \Var\left[\postDens_{\designidx}(\bpar)\right] &= \exp\left\{\E[\llik_{\designidx}(\bpar, \CovObs)] + \frac{1}{2}\Var[\llik_{\designidx}(\bpar, \CovObs)] \right\}
\end{align*}



\section{Meeting: 7/11/2023}

\subsection{General Updates}
\begin{enumerate}
\item URBAN would allow me to take SPH EH 851: Advanced GIS for Public Health and Climate Research. 
\item Have begun reviewing PEcAn code and planning updates. 
\item Working on writing up a full exposition for the model/algorithms so far. 
\item Spent most of last week reading up on batch Bayesian optimization and sequential design. 
\end{enumerate}

\subsection{Experimental Design}
\begin{align*}
\acq{\designidx}(\tilde{\bpar}) &= \E_{\bpar \sim \rho} \E_{\tilde{\postDens}} \Var\left[\postDens_{\designidx}(\bpar) | (\tilde{\bpar}, \tilde{\postDens}) \right] 
\end{align*}

\begin{enumerate}
\item Sinsbeck and Nowak (2017) propose this criterion except targeting the likelihood approximation instead of the (unnormalized) posterior. They set $\rho = \priorDens$. 
\item Surer, Plumlee, and Wild (2023) propose the same criterion (they target the posterior, as the acquisition function is defined above). They show that the 
inner two integrals can be computed analytically in the PCA-GP setting. 
\item To generalize this to the batch setting, Surer et al utilize a kriging believer approach. i.e. the batch of points is collected in a loop; when each point $\tilde{\bpar}$ is chosen, the GP is conditioned on $(\tilde{\bpar}, \mu(\tilde{\bpar}))$ where $\mu(\cdot)$ is the GP mean. This has the effect of updating the GP predictive variance without affecting 
the GP mean. GP hyperparameters are updated only after a whole batch has been collected and the full model run at the points in the batch. Alternative batch Bayesian 
optimization heuristics could also be used here; e.g. constant liar. 
\item In our setting, if we target the unnormalized log posterior, then the variance is known in closed-form and does not depend on $\tilde{\postDens}$. We may also 
be able to compute the two inner integrals when targeting the unnormalized posterior - I need to check. 
\item Some questions I have: 
	\begin{enumerate}
	\item What is the effect of targeting the unnormalized posterior vs log posterior vs. (log) likelihood? 
	\item Instead of weighting by $\rho = \priorDens$, what if we weight by an approximation to the true posterior by using MCMC samples to approximate the outer
	integral? 
	\end{enumerate}
\end{enumerate}

\subsection{Plan for Next Week}
\begin{enumerate}
\item Implement the above acquisition as well as other common Bayes Opt/sequential design acquisitions for comparison. 
\item Continue planning updates to PEcAn code. 
\item Continue working on writing exposition; will share the document once it is at a reasonable state of completion. 
\end{enumerate}


\end{document}




