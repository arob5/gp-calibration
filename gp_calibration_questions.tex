\documentclass[12pt]{article}
\RequirePackage[l2tabu, orthodox]{nag}
\usepackage[main=english]{babel}
\usepackage[rm={lining,tabular},sf={lining,tabular},tt={lining,tabular,monowidth}]{cfr-lm}
\usepackage{amsthm,amssymb,latexsym,gensymb,mathtools,mathrsfs}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{epstopdf,enumitem,microtype,dcolumn,booktabs,hyperref,url,fancyhdr}
\usepackage{algorithmic}
\usepackage[ruled,commentsnumbered,titlenotnumbered]{algorithm2e}

% Plotting
\usepackage{pgfplots}
\usepackage{xinttools} % for the \xintFor***
\usepgfplotslibrary{fillbetween}
\pgfplotsset{compat=1.8}
\usepackage{tikz}

% Custom Commands
\newcommand*{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand*{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand*{\suchthat}{\,\mathrel{\big|}\,}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\Ker}{\mathrm{Ker}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Prob}{\mathbb{P}}
\DeclarePairedDelimiterX\innerp[2]{(}{)}{#1\delimsize\vert\mathopen{}#2}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\def\R{\mathbb{R}}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\newcommand*{\vertbar}{\rule[-1ex]{0.5pt}{2.5ex}} % For lines in matrix to represent columns
\newcommand*{\horzbar}{\rule[.5ex]{2.5ex}{0.5pt}} % For lines in matrix to represent rows

\setlist{topsep=1ex,parsep=1ex,itemsep=0ex}
\setlist[1]{leftmargin=\parindent}
\setlist[enumerate,1]{label=\arabic*.,ref=\arabic*}
\setlist[enumerate,2]{label=(\alph*),ref=(\alph*)}

% Specifically for paper formatting 
\renewcommand{\baselinestretch}{1.2} % Spaces manuscript for easy reading

% For embedding images
\graphicspath{ {./images/} }

% Formatting definitions, propositions, etc. 
\newtheorem{definition}{Definition}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{notation}{Notation}

\begin{document}

\begin{center}
\Large
Questions on GP Emulation/Calibration Methodology 
\end{center}

\section{Methodological Foundations}
I have been using Robert Gramacy's free online \href{https://bobby.gramacy.com/surrogates/}{textbook} on GPs and surrogate modeling as a primary reference on calibration of 
computer models. It is quite recent and nicely synthesizes many of the important papers on the subject. I do use my own notation here though, as I find that his can contain a somewhat
excessive number of sub and superscripts. I'll just walk through my understanding of the methodology, asking questions along the way. 

\subsection{Defining the Parameters of Interest}
The \textit{computer model} (i.e. \textit{simulator} or \textit{process-based model}, as referred to in the PEcAn papers) is deterministic and hence can be modeled as a function
$f: \mathcal{P} \to \R$, where $\mathcal{P} \subset \R^d$ is the parameter space of the computer model. For now I am assuming scalar output for simplicity, although I know that 
most of the PEcAn models have many outputs. The Kennedy and O'Hagan calibration framework (summarized in Gramacy's book) distinguishes between two types of parameters, so 
that 
\[\mathcal{P} = \mathcal{X} \times \mathcal{U}\]
where $\mathcal{X}$ is the space of \textit{observational parameters} and $\mathcal{U}$ the space of \textit{calibration parameters}. The former refers to those parameters that are 
observed in the real-world data and can also be included in the computer model. The calibration parameters cannot be observed in the real-world; this can mean that they have no 
real-world meaning (e.g. the mesh size in the computer model) or that they do exist in the real-world but cannot be directly observed. To my understanding, the observation parameters
are just like covariates in a typical regression, in the sense that the analysis will be conducted conditional on the values of the observation parameters. The calibration parameters are 
the true objects of interest; we seek to tune these parameters using field data as well as understand the uncertainty in the computer model as a function of the calibration parameters. 

\subsubsection{Questions}
\begin{itemize}
\item Is the distinction between observation and calibration parameters relevant for the PEcAn models? My understanding is that what you refer to as \textit{process-model parameters}
refers to calibration parameters. I would assume there is also some notion of observation parameters. \\
\textbf{Answer: } Yes, the distinction is relevant, but typically don't have well-constrained observable parameters. Looking at the code should help clear up how these are treated. 
\end{itemize}

\subsection{Statistical Model Relating Computer Simulation and Observational Data}
I have already defined the computer model $f$, and I will denote the outputs of this model as $y = f(x, u)$, for parameter values $(x, u) \in \mathcal{X} \times \mathcal{U} = \mathcal{P}$.
The real interest lies with the actual physical process that the computer model seeks to replicate. I will denote the value of this physical process at $x \in \mathcal{X}$ by $z^*(x)$. 
However, the field observations of this process - which I denote by $z(x)$ - are noisy. We thus assume a typical Gaussian noise model for these observations. 
\[z(x) = z^*(x) + \epsilon, \qquad \epsilon \overset{iid}{\sim} N(0, \tau^{-1})\]  
parameterized with respect to the precision parameter $\tau$, the inverse of the variance. Kennedy and O'Hagan then relate the physical process to the computer model via 
\[z^*(x) = f(x, u) + b(x)\]
where $b(\cdot)$ is a discrepancy term or ``bias''. My understanding of the current PEcAn approach is to set $b \equiv 0$, which assumes that the computer model is faithful to the physical
process (when calibrated correctly). The ``$u$'' in the above expression can thus be interpreted as the ``true'' or ``correct''  value of the calibration parameters, though I don't think this 
point is especially necessary in a Bayesian framework. 

To summarize, the Kennedy and O'Hagan model is 
\[z(x) = z^*(x) + \epsilon = f(x, u) + b(x) + \epsilon, \qquad \epsilon \overset{iid}{\sim} N(0, \tau^{-1})\] 
and assume we have observational data $\{(x_i, z_i)\}_{i = 1}^{n}$. Let $z := (z_1, \dots, z_n)^T$. 

For the remainder of this document I will set the discrepancy to $0$, as I believe that's what is done in PEcAn, which gives the working model: 
\[z_i|x_i, u \overset{ind}{\sim} N(y_i^u, \tau^{-1}), \qquad i = 1, \dots, n \]
where $y_i^u := f(x_i, u)$. 

\subsubsection{Questions}
\begin{itemize}
\item Am I correct in assuming that you are using the above model with discrepancy $b \equiv 0$? \\
\textbf{Answer: } Yes, this is correct. The future possibility of including a discrepancy term is mentioned in section 4.2 of Fer et al (2018). 
\end{itemize}

\subsection{``Brute Force'' Calibration}
You mention a brute force calibration strategy in your paper, and I just wanted to write that out below to make sure I'm on the right page. The current parameters of interest are
$u$ and $\tau$. We therefore have posterior (conditional on field observations), 
\[p(u, \tau|z) \propto p(z|u, \tau)p(u, \tau) = N_n(z|y^u, \tau^{-1} I_n)p(u, \tau)\]
for some assumed prior on $(u, \tau)$ and where $y^u := (y_1^u, \dots, y_n^u)$. Everything here is implicitly conditional on $x_1, \dots, x_n$ but I will suppress this to simplify notation. 
We can use MCMC to sample from this posterior. However, the key observation is that evaluating the likelihood $N_n(z|y^u, \tau^{-1} I_n)$ requires the computation of $y^u$, which 
involves running the entire simulation with the current calibration $u$. Therefore, the computer model must be run in full at every single step of MCMC, 
which is infeasible for computationally expensive simulations. 

\subsection{Emulator Methodology: ``Modular Approach''}
To deal with the computational issue in the brute force approach, we replace the computer model with a GP emulator/surrogate $\hat{f}(\cdot) \sim \mathcal{GP}(\mu(\cdot), k(\cdot, \cdot))$. 
Among the various emulator calibration approaches, a simple one is the ``modular'' approach described in Gramacy 8.1.2 and originally in 
\href{https://projecteuclid.org/journals/bayesian-analysis/volume-4/issue-1/Modularization-in-Bayesian-analysis-with-emphasis-on-analysis-of-computer/10.1214/09-BA404.full}{Bayarri et al (2009)}. 
The calibration approach is as follows. 
\begin{enumerate}
\item Run the full model on $N$ design points (i.e. knots): $\{(\tilde{x}_1, \tilde{u}_1), \dots, (\tilde{x}_N, \tilde{u}_N)\}$, where the tilde differentiates these points from the vector of observation parameters $x$
sampled in the field data. Presumably, the design points are chosen such that $\{\tilde{x}_1, \dots, \tilde{x}_N\} \subset \{x_1, \dots, x_n\}$; that is, the model is run at the observation parameter
values observed in the real-world, with the possibility of replicates. Let $\tilde{x}$ and $\tilde{u}$ be N-dimensional vectors containing the 
$\tilde{x}_i$ and $\tilde{u}_i$, respectively. Also let $\tilde{y} := (y_1^{\tilde{u}_1}, \dots, y_N^{\tilde{u}_N})$ be the vector of simulator outputs evaluated at the $N$ design points. 
\item Fit the GP emulator \textit{only using the computer model data} $\tilde{y}$. Denote the fit emulator by $\hat{f}$. 
\item Now essentially conduct the brute force approach described above, but with $\hat{f}$ replacing $f$. That is, we sample from posterior
\[p(u, \tau|z, \hat{f}) \propto p(z|u, \tau, \hat{f})p(u, \tau) = N_n(z|\hat{f}(x, u), \tau^{-1} I_n)p(u, \tau)\]
Now in each step of MCMC we need only evaluate the mean of the GP $\hat{f}(x, u)$ at the current proposed $u$, rather than the whole computer model $f$. 
\end{enumerate}
This approach is not the one proposed by Kennedy and O'Hagan, which is the ``fully Bayesian'' approach: they assume a GP prior for $f$, then consider the posterior over the GP parameters
in addition to the other parameters. Gramacy notes that the modular approach can help to avoid some of the pathologies sometimes present in the fully Bayesian approach. This is of course not
the approach used in PEcAn as I am still considering emulating model outputs directly here (instead of sufficient statistics), but I still find it helpful to review before considering the sufficient 
statistic approach. 

\subsection{Emulator Methodology: Emulating Likelihood}
The above section approached the problem of sampling from the posterior
\[p(u, \tau|z) \propto N_n(z|y^u, \tau^{-1} I_n)p(u, \tau)\]
by replacing the full simulator computations $y^u$ with a surrogate that approximates the full computations. Assuming the surrogate is much faster than the original simulation, then 
this can drastically speed likelihood computation. An alternative method is to essentially skip this intermediate step and directly emulate the likelihood itself. In this case, the function 
we seek to emulate is
\[L(x, z, u) := p(z|y^{u}, \tau) = N_n(z|y^{u}, \tau^{-1} I_n)\]\
To be able to  define this I believe we must consider the extended input space $\mathcal{X}^n \times \mathcal{Z}^n \times \mathcal{U}$, where $\mathcal{X}^n = \mathcal{X} \times \cdots \times \mathcal{X}$
and similarly for $\mathcal{Z}^n$. If we're essentially just treating the $x$ and $z$ values as given, then the domain might be simplified to just $\mathcal{U}$. 
Taking things a step further, we can emulate a sufficient statistic that allows full calculation of the likelihood. That is, we can emulate
\[T(x, z, u) := \sum_{i = 1}^n (z_i - y_i^{u})^2\]\
again defined on the domain $\mathcal{X}^n \times \mathcal{Z}^n \times \mathcal{U}$. Let $\hat{T}(\cdot, \cdot, \cdot)$ denote the fitted GP emulator. 
This approach again allows cheaper (approximate) likelihood computations. Indeed, the likelihood of observed data $z \in \R^n$ (implicitly conditioned on $x \in \R^n$) is given by  
\begin{align*}
p(z|u, \tau) = N_n(z|y^u, \tau^{-1}I_n) &\propto \tau^{n/2} \exp\left\{-\frac{\tau}{2} \norm{z - y^u}^2_2 \right\} \\
					         &= \tau^{n/2} \exp\left\{-\frac{\tau}{2} T(x, z, u) \right\} \\
					         &\approx \tau^{n/2} \exp\left\{-\frac{\tau}{2} \hat{T}(x, z, u) \right\} \\
\end{align*}
Thus, the likelihood can be approximated using evaluations of the GP $\hat{T}$. 
Given this, here is my current understanding of the emulator methodology (for a single site). I'm not considering the experimental design or MCMC methodologies yet at this point, so 
I take them as given. 
\begin{itemize}
\item Choose design points $\{(\tilde{x}_1, \tilde{z}_1, \tilde{u}_1), \dots (\tilde{x}_N, \tilde{z}_N, \tilde{u}_N)\}$, where $\tilde{x}_i \in \mathcal{X}^n$, $\tilde{z}_i \in \mathcal{Z}^n$, and $u_i \in \mathcal{U}$. 
\item Run full simulator at the $(\tilde{x}_i, \tilde{u}_i)$ points, producing the vector of simulator outputs $\tilde{y} \in \R^{N \times n}$. This requires $N \times n$ runs since $\tilde{x}_i$ and $\tilde{z}_i$
each have $n$ components. 
\item Fit GP emulator to the mapping $T(x, z, u) := \sum_{i = 1}^n (z_i - y_i^{u})^2$ using the dataset $\left\{(\tilde{x}_{ij}, \tilde{z}_{ij}, \tilde{u}_i), y_{ij}^{\tilde{u}_i}\right\}_{1 \leq i \leq N, 1 \leq j \leq n}$, producing fit emulator $\hat{T}(\cdot, \cdot, \cdot)$. 
\item Perform MCMC for the approximate posterior
\[p(u, \tau|z) \propto \tau^{n/2} \exp\left\{-\frac{\tau}{2} \hat{T}(x, z, u) \right\}p(u, \tau)\]
\end{itemize}

\subsubsection{GP Specifications}
Currently the PEcAn emulator package uses the R package \textit{mlegp} to fit the emulator. This package uses the standard Gaussian family of kernels, with a different lengthscale parameter for 
each dimension in the input space. The PEcAn code specifies that the nugget term be set to 0, given that the process model is deterministic. Moreover, the mean function is assumed to be constant. 
Therefore, the parameters that require estimation are: the vector of lengthscale parameters, the constant mean, and the overall GP variance. The \textit{mlegp} package proceeds by first finding
the maximum likelihood estimates of the lengthscale parameters by numerical methods, and then given these values the MLE for the mean and GP variance can be calculated in closed form. 


\subsubsection{Questions}
\begin{itemize}
\item What choices are you making for the GP parameters (mean function $\mu$ and kernel/covariance function $k$)? \\
\textbf{Answer: } Using the defaults of the \textit{mlegp} R package. I believe the default kernel is the inverse squared exponential but I need to check. 
\item Am I correct in defining the sufficient statistic function that you are emulating? 
\textbf{Answer: } Yes. 
\end{itemize}

\subsection{Priors}
In the above sections I have simply been writing $p(u, \tau)$ to indicate a joint prior distribution over the calibration parameters and the precision parameter. In the PEcAn implementation, 
this prior is specified as
\[p(u, \tau) = p(\tau|u)p(u) = p(\tau)p(u) = G(\tau|a, b)\pi_0(u)\]
where $G(a, b)$ denotes the Gamma distribution with shape and rate hyperparameters $a$ and $b$, respectively. The prior $\pi_0$ over the calibration parameters is given by a meta-analysis
of previous studies and expert opinion. 

\subsection{Other methodological components of the analysis}
I haven't thought about this too much in detail, but I'd be interested to hear more about some of the other methodological choices described in the Fer et al (2018) paper. 
\begin{itemize}
\item Calculation of effective sample size (section 2.2).
\item Scaling factors (section 2.3). 
\end{itemize}

\subsection{MCMC Algorithm}
The MCMC algorithm updates the calibration parameters $u$ and precision parameter $\tau$ iteratively. A Metropolis-within-Gibbs approach is used to sample from $p(u, \tau|z)$. The following sections
provide the specifics as to how $u$ and $\tau$ are updated. 

\subsubsection{Updating $u$}
The first conditional distribution of $p(u, \tau|z)$ we consider is $p(u|\tau, z)$. Conditional on the current value of $\tau$, $u$ is updated via an adaptive Random Walk Metropolis-Hastings (MH) algorithm. The proposal 
distribution for $u$ is given by a truncated Gaussian centered at the current value of $u$, with adaptive covariance matrix, and lower and upper bounds determined by the range for each calibration parameter taken over the design points. To simplify notation in the algorithm box, I simply denote the covariance matrix $\Sigma$ (even though it is not fixed), and suppress any notation indicating the distribution is truncated. 

The conditional distribution of interest here is given by 
\[p(u|\tau, z) \propto p(z|u, \tau)p(u|\tau) = N_n(z|y^u, \tau^{-1}I_n) \pi_0(u)\]
The likelihood evaluations of $N_n(z|y^u, \tau^{-1}I_n)$ are approximated using the GP, as discussed in the previous sections. To quantify the uncertainty created by this GP approximation, the sufficient statistic values
used to calculate the likelihoods are sampled from the GP at the current and proposed parameter values. The alternative to this would be to simply use the GP predictive mean at the parameter value. 
However, the sampling approach bakes the GP uncertainty directly into the samples, which allows for the calculation of confidence intervals, etc. that better reflect the true uncertainty. The specifics of this approach are
given in the algorithm box below. Also, the ``Future Directions'' section later in this paper considers integrating over the GP to account for this uncertainty as an alternative to sampling. 

\subsubsection{Updating $\tau$}
After updating updating $u$ conditional on $\tau$, $\tau$ is updated via a traditional Gibbs sampling approach; that is, it is sampled from $p(\tau|u, z)$. In particular, consider
\begin{align*}
p(\tau|u, z) &\propto p(z|u, \tau)p(\tau|u) \\
		 &= N_n(z|y^u, \tau^{-1}I_n)\pi_0(\tau) \\
		 &= N_n(z|y^u, \tau^{-1}I_n)G(\tau|a, b) \\
		 &\propto \tau^{n/2} \exp\left\{\frac{-\tau}{2} \norm{z - y^u}_2^2 \right\} \tau^{a - 1} e^{-b \tau} \\
		 &= \tau^{n/2} \exp\left\{\frac{-\tau}{2} T(x, z, u) \right\} \tau^{a - 1} e^{-b \tau} \\
		 &= \tau^{a + n/2 - 1} \exp\left\{-(b + T(x, z, u)/2)\tau \right\} \\
		 &= G\left(\tau|a + n/2, b + T(x, z, u)/2\right)
\end{align*}
Once again, we utilize the GP approximation so that $\tau$ is updated by sampling from
\[\tau \sim G\left(a + n/2, b + T/2\right) \]
where $T \sim \hat{T}(x, z, u)$ is a value sampled from the predictive GP distribution at the current value of $u$. 

\subsubsection{Complete Algorithm}

 \begin{algorithm}[H]
	\SetAlgoLined
	
	\textbf{Input}: 
	\begin{itemize}
	\item Initial parameter values $u^{(0)}, \tau^{(0)}$
	\item Fit emulator $\hat{T}(\cdot, \cdot, \cdot) \sim \mathcal{GP}(\mu(\cdot), k(\cdot, \cdot))$
	\item Number iterations $N_{\text{MCMC}}$
	\end{itemize}
		
	\bigskip
	
	\For{$t = 1, \dots, N_{\text{MCMC}}$} {
	\textit{MH step with GP approximation}: \\[.2cm]
	Sample $u^\prime \sim N(u^{(t - 1)}, \Sigma)$ \\
	Sample $T^\prime \sim \hat{T}(x, z, u^\prime)$, $T \sim \hat{T}(x, z, u^{(t - 1)})$ \\
	Sample $\tau^\prime \sim G(a + n/2, b + T^\prime / 2)$ \\
	Calculate approximate likelihoods: \\
	 $\hat{\mathcal{L}} := \left(\tau^{(t - 1)}\right)^{n/2} \exp\left\{-\frac{\tau^{(t - 1)}}{2} T \right\}$ \\ 
	 $\hat{\mathcal{L}}^\prime := \left(\tau^\prime \right)^{n/2} \exp\left\{-\frac{\tau^\prime}{2} T^\prime \right\}$  \\
	 $\alpha(u^{(t - 1)}, u^\prime) := \frac{\hat{\mathcal{L}}^\prime \cdot \pi_0(u^\prime) N(u^{(t - 1)}|u^\prime, \Sigma)}{\hat{\mathcal{L}} \cdot \pi_0(u^{(t - 1)}) N(u^\prime|u^{(t - 1)}, \Sigma)}$ \\
	 Sample $U \sim \mathcal{U}[0, 1]$ \\
	 \If{$U < \alpha(u^{(t - 1)}, u^\prime)$} {
	 	$u^{(t)} := u^\prime$ \\
		$T^{(t)} := T^\prime$ \\
	 } \Else {
		$u^{(t)} := u^{(t - 1)}$ \\
		$T^{(t)} := T$ \\
	 }
	
	\bigskip
	
	\textit{Gibbs step for statistical parameters with GP approximation}: \\[.2cm]
	Sample $\tau^{(t)} \sim G\left(a + n/2, b + T^{(t)}/2\right)$
	
	}

	
\caption{MCMC for Parameter Calibration}
\end{algorithm}

\subsection{Effect of Re-sampling $\tau$}
For a quick back-of-the-envelope calculation consider an example with $n = 1000$ observations and Gamma hyperparameters $a = 11, b = 1$. I've run 1d examples with this 
setup and found the GP predictive means ranging from about 90 to 580 across the parameter space, with predictive standard deviations ranging from .0001 to 28. If $tau$ is 
re-sampled in the algorithm then the current and re-sampled $\tau$ values used in the accept-reject step are sampled from: 
\begin{align*}
\tau^{(t - 1)} \sim G(a + n/2, b + T^{(t-1)}/2) = G(511, 1 + T^{(t-1)}/2) \\
\tau^\prime \sim G(a + n/2, b + T^\prime/2) = G(511, 1 + T^\prime/2) \\
\end{align*}
The difference in these distribution depends on a lot of factors: the hyperparameters (which I have fixed here), the current and proposed values of $u$, the samples for the 
current and propose sufficient statistics (which in turn depend on the predictive GP mean, variance, and possibly covariance). Given the above numbers, let's first assume a pretty 
large difference of 100 between $T$ and $T^\prime$. If the sufficient statistic values are very large (the sampler if far from the optimal u value), then the resulting Gamma distributions
will be quite close, their expectations differing by 1 or 2. For sufficient statistic values found close to the optimal $u$ value this difference can increase to 5 or 6. Instead considering 
a difference in sufficient statistic values of around 20, these numbers shrink quite a bit: the difference in the Gamma mean near the optimal $u$ being around 1. 


\subsubsection{Adapting the Proposal Covariance Matrix}
\textbf{Joint Update.} First consider the case where the Metropolis updates are conducted jointly for all dimensions at once. The covariance matrix for the Gaussian
proposal is initialized as $\Sigma_0 := \sigma_0^2 I_d$ for some chosen parameter $\sigma_0$. This covariance is then adapted at all iterations $t$ satisfying 
$t > 2$ and only when $t$ is a multiple of some chosen update frequency $U$. At iteration $t $ the previous $H$ samples are used to inform the adaptation of the covariance, 
where $H$ is a \textit{history} parameter that also must be chosen in advance. Let $a$ be the number of accepted samples since the previous iteration at which the 
covariance was adapted (this counter is re-set to $0$ each time the covariance is adapted). If $a = 0$ then we set $\Sigma_{t} := \gamma^2 \Sigma_{t - 1}$, where 
$\gamma > 0$ is fixed scaling factor. If $a \neq 0$ then the following update is conducted. Let $\hat{\sigma}_1, \dots, \hat{\sigma}_d$ be the empirical standard deviations
taken over the respective dimensions of the previous $H$ samples $u^{(t - H)}, \dots, u^{(t - 1)}$. Let $\hat{C}_{t - H:t-1}$ be the empirical correlation matrix calculated using
the previous $H$ samples. Letting $a^*$ denote the target acceptance rate, $\gamma^\prime := \max\left\{\frac{a}{a^*}, \gamma \right\}$, and
$R:= \gamma^\prime \text{diag}(\hat{\sigma}_1, \dots, \hat{\sigma}_d)$. The covariance matrix is then updated as $\Sigma_t := R\hat{C}_{t - H:t-1}R$. Note that the result is simply
a scaling of the empirical covariance matrix $R\hat{C}_{t - H:t-1}R = \left(\lambda^\prime\right)^2 \hat{\Sigma}_{t - H:t-1}$.




\subsubsection{Questions}
\begin{itemize}
\item I'd like to clarify why $\tau^\prime$ is sampled for use in the MH acceptance ratio calculation, instead of just using $\tau^{(t - 1)}$ (i.e. just conditioning on the most up-to-date value of $\tau$). 
\end{itemize}


\section{Future Directions}
\subsection{GP Modeling Improvements} (Istem's work with her student this summer)
\begin{itemize}
\item Investigating radial/isotropic covariance assumption (i.e. using separable/antisotropic kernels) (see Gramacy 5.2.5).
\item Investigating stationarity assumption. 
	\begin{itemize}
	\item Heteroskedastic modeling (see Gramacy ch 10). 
	\end{itemize}
\end{itemize}

\subsection{Experimental Design Improvements}
\begin{itemize}
\item Lots of literature in sequential design/active learning/Bayesian optimization
\end{itemize}

\subsection{Speeding up GP calculations}
There is a lot of literature in this area as well, with all of the techniques essentially trying to cope with the expensive matrix decompositions required 
to calculate the inverse and determinant in the equations used for GP prediction and inference. The introduction to chapter 9 in Gramacy's book has a 
very nice list of some of these methods (pseudo-inputs, compactly supported kernels, local neighborhoods, etc.) 

\subsection{MCMC}
Here, we focus in on specific improvements on MCMC side of things, noting that there could certainly be interplay between the previous areas of possible improvement 
and improvements to the MCMC algorithm itself. 

As a first step, Jonathan has already mentioned the potential for adopting a gradient-based approach by trying out Hamiltonian Monte Carlo. 


\subsubsection{Literature Review}
There is certainly a wide body of literature on MCMC algorithms in settings with expensive likelihood calculations. I looked around a bit for literature in this vein but 
tailored specifically to parameter calibration for computer experiments and didn't find as much as I expected. Here are some papers I did find: 
\begin{itemize}
\item \href{https://bg.copernicus.org/articles/14/4295/2017/}{Bayesian calibration of terrestrial ecosystem models: a study of advanced Markov chain Monte Carlo methods}
	\begin{itemize}
	\item Advocates Differential Evolution Adaptive Metropolis (DREAM) over Adaptive Metropolis (AM)
	\item Finds that DREAM improves model fit and predictive performance, and identifies multimodal distributions of parameters where AM fails. 
	\item Also finds that heteroskedastic Gaussian noise model is appropriate, whereas uncorrelated error model would underestimate parameter uncertainty. 
	\end{itemize}
\item \href{https://www.tandfonline.com/doi/abs/10.1198/TECH.2010.09195}{Efficient MCMC Schemes for Computationally Expensive Posterior Distributions}
	\begin{itemize}
	\item Builds on Rasmussen's work \href{http://mlg.eng.cam.ac.uk/pub/pdf/Ras03.pdf}{Gaussian Processes to Speed Up Hybrid Monte Carlo for Expensive Bayesian Integrals}
	\item HMC approach with GP approximation to log posterior.
	\item Parallel tempering for multimodal distributions.
	\item True target (i.e. using full simulation) is maintained at the lowest temperature, so stationary distribution of lowest temperature chain is exactly the posterior of interest. 
	\item ``Exploratory phase'' uses GP approximation to propose parameter values (via leapfrog discretization), and evaluates the full posterior on the final proposed values to refine the GP approximation.
	\item ``Sampling phase'' uses the GP approximation returned from the exploratory phase, then proposes param values using leapfrog discretization and GP, and accepts via MH rejection criterion.
	\end{itemize}
\item \href{https://www.sciencedirect.com/science/article/pii/S0304380021001708}{Sequential Monte-Carlo algorithms for Bayesian model calibration â€“ A review and method comparison} 
(SMC, but interesting comparison to MCMC in the calibration setting)
	\begin{itemize}
	\item SMC generally thought to be less efficient than MCMC for parameter calibration, but SMC is parallelizable; with sufficient number of cores and for runtime-intensive models, authors show that SMC can sometimes be faster
	\item Downside: SMC can be very sensitive to tuning parameters.
	\item Benchmarked SMC against Differential Evolution MCMC with snooker update
	\item They achieved best results with a mix of MCMC and SMC steps. Authors indicate that such ``mixed'' algorithms may be an interesting direction for future research.  
	\item Other future research needs: better adaptive SMC algorithms, more reliable convergence checks that can identify issues during SMC runs. 
	\end{itemize}
\end{itemize}

\subsubsection{Integrating over GP Uncertainty}
Recall that we are interested in sampling from the posterior
\[p(u, \tau|z) \propto N_n(z|y^u, \tau^{-1} I_n)p(u, \tau)\]
and we have opted for the GP approximation 
\[p(u, \tau|z) \propto \tau^{n/2} \exp\left\{-\frac{\tau}{2} \hat{T}(x, z, u) \right\}p(u, \tau)\]
To account for the uncertainty introduced by approximating $T$ with $\hat{T}$, the current algorithm samples from $\hat{T}(x, z, u) \sim N(\mu(u), k(u, u))$ in the MH 
accept-reject step. An alternative approach to incorporating uncertainty that is more conducive to HMC implementation in Stan is to average the likelihood over the GP 
interpolation uncertainty. Noting that the below expectation is with respect to the Gaussian distribution of $\hat{T}(x, z, u)$, we obtain 
\[\E \left[\tau^{n/2} \exp\left\{-\frac{\tau}{2} \hat{T}(x, z, u)\right\} \right] = \tau^{n/2} \E \left[ \exp\left\{-\frac{\tau}{2} \hat{T}(x, z, u)\right\} \right] = \tau^{n/2} \exp\left\{-\frac{\tau}{2}\mu(u) + \frac{\tau^2}{8} k(u, u)\right\}\]
where the final equality follows from the form of the Gaussian moment generating function (MGF). Alternatively, we can use the fact that 
\[-\frac{\tau}{2} \hat{T}(u) \sim N\left(-\frac{\tau}{2}\mu(u), \frac{\tau^2}{4} k(u, u) \right)\]
and hence, 
\[\exp\left\{-\frac{\tau}{2} \hat{T}(u)\right\} \sim \text{Lognormal}\left(-\frac{\tau}{2}\mu(u), \frac{\tau^2}{4} k(u, u) \right)\]
Using the expression for the mean of a lognormal distributed random variable, we then obtain the same result:
\[\E\left[\exp\left\{-\frac{\tau}{2} \hat{T}(u)\right\}\right] = \exp\left\{-\frac{\tau}{2}\mu(u) + \frac{\tau^2}{8}k(u, u)\right\}\]

Therefore, an alternative to the re-sampling MCMC approach described above is to use the posterior approximation
\begin{align*}
p(u, \tau|z) &\propto \tau^{n/2} \exp\left\{-\frac{\tau}{2}\mu(u) + \frac{\tau^2}{8} k(u, u)\right\}p(u, \tau) \\
		 &=  \tau^{n/2} \exp\left\{-\frac{\tau}{2}\left[\mu(u) - \frac{\tau}{4} k(u, u) \right]\right\}p(u, \tau)
\end{align*}
where the righthand side may now be evaluated directly to calculate the MH ratio, as the interpolation uncertainty is now baked into this closed-form expression. Note that based on my definition of the GP in earlier sections I should really be writing $\mu(x, z, u)$ and $k((x, z, u), (x, z, u))$, but I am essentially treating $x$ and $z$ as given above and focusing attention on $u$. Also note that when the variance is 0 ($k(u, u) = 0$) then 
the likelihood is calculated using the predictive mean of the GP, which corresponds to parameter values $u$ in the design matrix; that is, parameters on which the full simulation was run. Moreover, if the precision $\tau$
is low (i.e. the noise around the simulation predictions is high) then the $\frac{\tau^2}{8} k(u, u)$ term will be small, reflecting the fact that the GP approximation has more ``leeway'' given the underlying noisy process. 

As a simple example, consider the case in which the predictive GP mean is constant: $\mu(u) \equiv 0$. If we were to just use the GP mean as an approximation to the sufficient statistic then we would be approximating the 
true likelihood with a completely flat approximation. The likelihood evaluations in MCMC would then play no role, as the current and proposed values would always have the same density. However, suppose the predictive variance $k(u, u)$ is non-constant. To incorporate this uncertainty, we abandon the predictive mean approximation in favor of the above approximation that marginalizes over the GP: 
\[ \tau^{n/2} \exp\left\{-\frac{\tau}{2} \mu(u) + \frac{\tau^2}{8} k(u, u) \right\} \]
Now the $u$ that exhibit more predictive uncertainty (i.e. where $k(u, u)$ is large) will be given a larger density, due to the addition of the term $\frac{\tau^2}{8} k(u, u)$. This will mean that proposals at these more 
uncertain parameter values will be accepted more often. This, in turn, means that the posterior over $u$ will be more diffuse, reflecting the uncertainty in the GP approximation. 

\subsubsection{Modeling the log of the sufficient statistic}
The above approach suffers from the issue that the sum of squared errors is a non-negative quantity, yet it is unconstrained in the GP regression model. Thus, predictive quantities for the 
sufficient statistic $T$ can be negative, which can then lead to invalid likelihood evaluations outside of the unit interval. To get around this issue we consider here modeling $L := \log(T)$ in place 
of $T$. We now fit the GP on the dataset $(X, L)$ and denote this fit GP by $\hat{L} \sim \mathcal{GP}(\mu(\cdot), k(\cdot, \cdot))$. Note that the sufficient statistic on the original scale now follows
a log-normal process $\hat{T} = e^{\hat{L}} \sim \mathcal{LN}(\mu(\cdot), k(\cdot, \cdot))$. The likelihood approximation now takes the form
\[\tau^{n/2}\exp\left\{-\frac{\tau}{2}\hat{T} \right\} = \tau^{n/2}\exp\left\{-\frac{\tau}{2}e^{\hat{L}} \right\} \]
To integrate out the GP we must now calculate
\[\E_{\hat{T}}\left[\exp\left\{-\frac{\tau}{2}\hat{T} \right\}  \right] = \int_{0}^{\infty} \exp\left\{-\frac{\tau}{2}t \right\} \cdot LN(t; \mu, k) dt = \int_{-\infty}^{\infty} \exp\left(-\frac{\tau}{2}e^t \right) \varphi_{\mu, k}(t) dt\]
The second and third expressions are equivalent by a simple change of variable, but shows that this integral can be thought of as either a normal or log-normal expectation. In any case, there is no analytical 
solution to this integral. However, this integral is just the log-normal MGF (evaluated at $-\tau/2$) or the Laplace transform of the log-normal density (evaluated at $\tau/2$), so numerical and analytical approximations
have been well-studied. For notation, I will write 
$\mathcal{M}(s) := \E_{\hat{T}}\left[\exp\left\{-s\hat{T} \right\}  \right]$, using the MGF notation for convenience but instead opting for the Laplace transform parameterization. $\mathcal{M}(s)$ only exists 
for $s > 0$. Asmussen et al (2013) studies an analytical approximation
\[\hat{M}_{\mu, \sigma^2}(s) = \frac{\exp\left\{-\frac{W^2(se^\mu \sigma^2) + 2W(se^\mu \sigma^2)}{2\sigma^2}\right\}}{\sqrt{1 + W(se^\mu \sigma^2)}}, s \in \R^+\]
where $M_{\mu, \sigma^2}$ is the Laplace transform of a $LN(\mu, \sigma^2)$ density. Given this, the approximation of the (unnormalized) likelihood $p(z|\tau, u)$ can now be written
\[p(z|\tau, u) \propto \tau^{n/2} \hat{M}_{\mu(u), k(u,u)}(\tau/2)\]


\subsubsection{Extending Model and Considering Model Misspecification}
The first part of this section is basically just transcribing Jonathan's notes from the 9/9/2022 meeting. Begin by considering a slight generalization of the existing model: 
\begin{align*}
z_i &= y_i + \delta_i \\
y_i &= f(u, i) \\
\delta_i &\overset{iid}{\sim} N(0, \tau^{-1}) \\
\end{align*}
Equivalently, 
\[z_i \overset{ind}{\sim} N(f(u, i), \tau^{-1})\]
This has been the working model, except the additional input $i$ has been added to $f$. This term allows for things like time trends, and is key in dealing with some 
identifiability issues discussed below. Up to this point we have been considering $f(u, i) = u$, but this new model allows for examples such as
$f(u, i) = \sin(2\pi u i) + ci$

To test for misspecification, we suppose our assumption of independent errors is incorrect and instead generate the data with errors given by a AR(1) process:
\begin{align*}
&z_i = y_i + \epsilon_i \\
&y_i = f(u, i) \\
&\epsilon_i \sim N(\alpha \epsilon_{i - 1}, \sigma^2), \ \alpha \in [-1, 1] \\
\end{align*}
Settings $\alpha = 0$ recovers the independent error model, and now we can vary $\alpha$ to investigate misspecification in terms of correlated errors. 


\subsubsection{Comparing to the ``fully Bayesian'' approach}
It would be interesting to compare the current approach to the fully Bayesian approach. This would especially be true for the Laplace likelihood, as it would allow sampling of 
the Laplace parameters. 

\subsection{Targeting Specific Needs in Terrestrial Carbon Monitoring}
I included this section as a longer-term goal of better understanding the current state of the terrestrial carbon monitoring field, and how a combination 
of improvements in statistical modeling and computational techniques might help address current limitations. To this end, here are some papers I have bookmarked 
for future reading. 
\begin{itemize}
\item \href{https://www.globalcarbonproject.org/global/pdf/trudinger.07_opti_jgr.pdf}{OptIC project: An intercomparison of optimization techniques for parameter estimation in terrestrial biogeochemical models}
\item \href{https://centaur.reading.ac.uk/28464/1/Fox_etal.REFLEX.AFM.2009.pdf}{The REFLEX project: Comparing different algorithms and implementations for the inversion of a terrestrial ecosystem model against eddy covariance data}
\end{itemize}

\section{PEcAn Code}
\subsection{Suggestions/Updates}
\begin{enumerate}
\item Add a config file to specify file paths that will be read by makefile, rather than having file paths embedded in makefile itself. 
\end{enumerate}

\section{Meeting: 8/12/2022}
\subsection{1D Parameter Calibration Tests}
\subsubsection{Testing Framework}
\begin{itemize}
\item Priors on precision parameter $\tau$ and calibration parameter $u$: 
\begin{align*}
&\tau \sim \text{G}(100/9, 1) \\
&u \sim N(.5, .25)
\end{align*}
\item Set ``true'' parameters to the means of their priors:
\begin{align*}
&u^* := 0.5 \\
&\tau^* := 100/9 \implies \sigma^* = 0.3
\end{align*}
\item Choose a simple function $f(u) = u$ as a stand-in for the computer model, with the assumption that $f(u^*)$ gives the mean of the quantity of interest.
\item Simulate synthetic observed dataset: $y_i \overset{iid}{\sim} N(f(u^*), \sigma^*)$, $0 \leq i \leq n$, $n = 1000$.
\begin{center}
	\includegraphics[scale=.5]{{exact_llik}.png}
\end{center}

\item GP approximation:
\begin{itemize}
\item Choose design $X \in \R^{N \times 1}$ and evaluate model at design points: $y^{\text{m}} := f(X) \in \R^{N \times 1}$.
\item Calculate vector of sufficient statistics $T \in \R^{N \times 1}$, where $T_i := \sum_{j = 1}^{n} (y^{\text{m}}_i - y_j)^2$
\item Fit GP on training data $(X, T)$ using \textit{mlegp}. Let $\hat{T}$ denote the fitted GP model. 
\end{itemize}

\item MCMC over parameters $(u, \tau)$:
\begin{enumerate}
\item Brute force: using true Gaussian likelihood
\item GP: using ``GP integrated out'' approximate likelihood
\item GP Mean: GP approximate likelihood evaluated at the predictive mean (no uncertainty quantification)
\end{enumerate}
\end{itemize}

\subsection{Some General Notes}
\subsubsection{Numerical instability of \textit{mlegp}}
\begin{itemize}
\item Haven't investigated in detail, but seems to mainly be a problem in 1d, when design points are close together
\item \textit{mlegp}'s \textit{predict} function directly inverts the covariance matrix; does not use Cholesky decomposition

\begin{center}
	\includegraphics[scale=.5]{{gp_pred_std_err}.png}
\end{center}
\item In addition, the optimize routine seems to be highly sensitive the design.
\begin{center}
	\includegraphics[scale=.5]{{gp_pred_mean_llik_5design}.png}
\end{center}

\begin{center}
	\includegraphics[scale=.5]{{gp_pred_mean_llik_6design}.png}
\end{center}

\subsection{Some Results}
\begin{enumerate}
\item Evenly spaced design points
\item Below plots calculated using the true $\tau^*$ value. Using more extreme $\tau$ values would make the bulges even larger.

\begin{center}
	\includegraphics[scale=.5]{{gp_pred_mean_SS_5design}.png}
\end{center}

\begin{center}
	\includegraphics[scale=.5]{{gp_integrated_out_llik_5design}.png}
\end{center}


\item Why is the uncertainty penalty term so extreme? 
\begin{itemize}
\item Log penalty term: $\frac{\tau^2}{8}k(u, u)$
\item $\tau \approx 11$ so $\tau^2 \approx 121$. The values of the sufficient statistic $T$ vary in the $[100, 400]$ range or so, so even a very modest predictive 
variance $k(u, u)$ multiplied by 121 is going to make this term blow up. In this case, a reasonable predictive standard error of 19 would lead to the approximate penalty
$\frac{11^2}{8}19^2 \approx 5000$, which is why we see values above 5000 on this graph. 
\item Observation: The GP uncertainty estimates are very reasonable, yet averaging the likelihood over these uncertainty estimates leads to these unreasonable results. 
This expectation is very sensitive to both the GP uncertainty estimates and $\tau$. 
\end{itemize}
\item Given the above plots, the MCMC behavior is unsurprising. 

\begin{center}
	\includegraphics[scale=.2]{{trace.gp_5design}.png}
\end{center}
\end{enumerate}

\subsection{Next Steps}
\begin{itemize}
\item GP/MCMC work
\begin{enumerate}
\item Not as principled of an approach since you have to choose $p$, but I was looking at using quantiles of the likelihood instead of expectation
\[\frac{n}{2}\log \tau - \frac{\tau}{2}\mu(u) + \frac{\tau}{\sqrt{2}}\sqrt{k(u, u)} \cdot \text{erf}^{-1}(2p - 1)\]
\item Investigate potential bug in current PEcAn MCMC code
\item Experimental design
\end{enumerate}
\item How to close out last month of summer
\item Interested in continuing to collaborate in the Fall
\end{itemize}

\end{itemize}


\section{Meeting: 9/9/2022}
\subsection{Updates}
\begin{itemize}
\item Updated code to allow for multiple parameter tests. Currently only allows independent Gaussian priors for each parameter. 
\item Implemented interface for running GP emulator ``bakeoff''. 
\item Implemented suite of functions for visualizing/summarizing emulator and MCMC results. 
\end{itemize}

\subsection{Emulator Testing and Validation}
\begin{itemize}
\item Model comparison: sampling train and test locations
\item Metrics:
	\begin{itemize}
	\item RMSE, MAE
	\item Metrics targeting likelihood or acceptance ratio
	\end{itemize}
\item Should also take into account predictive standard errors, both average uncertainty and extreme areas of uncertainty
\end{itemize}

\subsection{Planned Parameter Calibration Tests}
\subsubsection{1D Non-Gaussian Likelihood}
\begin{itemize}
\item Gamma: $z|u \sim G(\beta f(u), \beta)$
\item Log-Normal: $z|u \sim LogNormal\left(\log(f(u)) - \frac{1}{2\tau}, \tau^{-1}\right)$
\end{itemize}

\subsubsection{Higher Dimensional Tests}
\begin{itemize}
\item Baseline Gaussian example: $z_i \overset{iid}{\sim} N(f(u), \tau^{-1})$
\item Heteroskedastic and/or correlated Gaussian examples: $z \sim N_n(f(u)1_n, \Sigma)$
\item Banana-shaped distribution (need to integrate $f(u)$ into this): 
\begin{align*}
&x_i \overset{iid}{\sim} N_2(0, \Sigma) \\
&z_i^{(1)} := \eta x_i^{(1)} \\
&z_i^{(2)} := \eta^{-1} x_i^{(2)} + \gamma((x_i^{(1)})^2 + \eta^2)
\end{align*}
\item Neal's Funnel
\end{itemize}

\subsubsection{Varying the model $f$}
\begin{itemize}
\item What kind of features to consider in tests?
\end{itemize}

\subsection{Calculating Posterior}
The working model:
\begin{align*}
&z_1, \dots, z_n \overset{iid}{\sim} N(f(u), \tau^{-1}) \\
&u_j \overset{ind}{\sim} N(\mu_j, \sigma_j^2) \\
&\tau \sim G(a, b)
\end{align*}
I've been assuming $f: \R^k \to \R$, where $k$ is the dimension of the calibration parameter. 
For $k = 1$, and with $f(u) = u$, this is just the typical Normal Inverse-Gamma model and the posterior 
is well-known in closed form. In my understanding, in higher dimensions, an analytical solution is known 
for the Bayesian inverse problem where $f$ is linear and injective. However, if $f$ in our example is a scalar-valued
function than I don't see how to analytically derive the posterior. 








\end{document} 





