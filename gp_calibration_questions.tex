\documentclass[12pt]{article}
\RequirePackage[l2tabu, orthodox]{nag}
\usepackage[main=english]{babel}
\usepackage[rm={lining,tabular},sf={lining,tabular},tt={lining,tabular,monowidth}]{cfr-lm}
\usepackage{amsthm,amssymb,latexsym,gensymb,mathtools,mathrsfs}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{epstopdf,enumitem,microtype,dcolumn,booktabs,hyperref,url,fancyhdr}

% Plotting
\usepackage{pgfplots}
\usepackage{xinttools} % for the \xintFor***
\usepgfplotslibrary{fillbetween}
\pgfplotsset{compat=1.8}
\usepackage{tikz}

% Custom Commands
\newcommand*{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand*{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand*{\suchthat}{\,\mathrel{\big|}\,}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\Ker}{\mathrm{Ker}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Prob}{\mathbb{P}}
\DeclarePairedDelimiterX\innerp[2]{(}{)}{#1\delimsize\vert\mathopen{}#2}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\def\R{\mathbb{R}}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\newcommand*{\vertbar}{\rule[-1ex]{0.5pt}{2.5ex}} % For lines in matrix to represent columns
\newcommand*{\horzbar}{\rule[.5ex]{2.5ex}{0.5pt}} % For lines in matrix to represent rows

\setlist{topsep=1ex,parsep=1ex,itemsep=0ex}
\setlist[1]{leftmargin=\parindent}
\setlist[enumerate,1]{label=\arabic*.,ref=\arabic*}
\setlist[enumerate,2]{label=(\alph*),ref=(\alph*)}

% Specifically for paper formatting 
\renewcommand{\baselinestretch}{1.2} % Spaces manuscript for easy reading

% Formatting definitions, propositions, etc. 
\newtheorem{definition}{Definition}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{notation}{Notation}

\begin{document}

\begin{center}
\Large
Questions on GP Emulation/Calibration Methodology 
\end{center}

\section{Methodological Foundations}
I have been using Robert Gramacy's free online \href{https://bobby.gramacy.com/surrogates/}{textbook} on GPs and surrogate modeling as a primary reference on calibration of 
computer models. It is quite recent and nicely synthesizes many of the important papers on the subject. I do use my own notation here though, as I find that his can contain a somewhat
excessive number of sub and superscripts. I'll just walk through my understanding of the methodology, asking questions along the way. 

\subsection{Defining the Parameters of Interest}
The \textit{computer model} (i.e. \textit{simulator} or \textit{process-based model}, as referred to in the PEcAn papers) is deterministic and hence can be modeled as a function
$f: \mathcal{P} \to \R$, where $\mathcal{P} \subset \R^d$ is the parameter space of the computer model. For now I am assuming scalar output for simplicity, although I know that 
most of the PEcAn models have many outputs. The Kennedy and O'Hagan calibration framework (summarized in Gramacy's book) distinguishes between two types of parameters, so 
that 
\[\mathcal{P} = \mathcal{X} \times \mathcal{U}\]
where $\mathcal{X}$ is the space of \textit{observational parameters} and $\mathcal{U}$ the space of \textit{calibration parameters}. The former refers to those parameters that are 
observed in the real-world data and can also be included in the computer model. The calibration parameters cannot be observed in the real-world; this can mean that they have no 
real-world meaning (e.g. the mesh size in the computer model) or that they do exist in the real-world but cannot be directly observed. To my understanding, the observation parameters
are just like covariates in a typical regression, in the sense that the analysis will be conducted conditional on the values of the observation parameters. The calibration parameters are 
the true objects of interest; we seek to tune these parameters using field data as well as understand the uncertainty in the computer model as a function of the calibration parameters. 

\subsubsection{Questions}
\begin{itemize}
\item Is the distinction between observation and calibration parameters relevant for the PEcAn models? My understanding is that what you refer to as \textit{process-model parameters}
refers to calibration parameters. I would assume there is also some notion of observation parameters. \\
\textbf{Answer: } Yes, the distinction is relevant, but typically don't have well-constrained observable parameters. Looking at the code should help clear up how these are treated. 
\end{itemize}

\subsection{Statistical Model Relating Computer Simulation and Observational Data}
I have already defined the computer model $f$, and I will denote the outputs of this model as $y = f(x, u)$, for parameter values $(x, u) \in \mathcal{X} \times \mathcal{U} = \mathcal{P}$.
The real interest lies with the actual physical process that the computer model seeks to replicate. I will denote the value of this physical process at $x \in \mathcal{X}$ by $z^*(x)$. 
However, the field observations of this process - which I denote by $z(x)$ - are noisy. We thus assume a typical Gaussian noise model for these observations. 
\[z(x) = z^*(x) + \epsilon, \qquad \epsilon \overset{iid}{\sim} N(0, \tau^2)\]  
Kennedy and O'Hagan then relate the physical process to the computer model via 
\[z^*(x) = f(x, u) + b(x)\]
where $b(\cdot)$ is a discrepancy term or ``bias''. My understanding of the current PEcAn approach is to set $b \equiv 0$, which assumes that the computer model is faithful to the physical
process (when calibrated correctly). The ``$u$'' in the above expression can thus be interpreted as the ``true'' or ``correct''  value of the calibration parameters, though I don't think this 
point is especially necessary in a Bayesian framework. 

To summarize, the Kennedy and O'Hagan model is 
\[z(x) = z^*(x) + \epsilon = f(x, u) + b(x) + \epsilon, \qquad \epsilon \overset{iid}{\sim} N(0, \tau^2)\] 
and assume we have observational data $\{(x_i, z_i)\}_{i = 1}^{n}$. Let $z := (z_1, \dots, z_n)^T$. 

For the remainder of this document I will set the discrepancy to $0$, as I believe that's what is done in PEcAn, which gives the working model: 
\[z_i|x_i, u \overset{ind}{\sim} N(y_i^u, \tau^2), \qquad i = 1, \dots, n \]
where $y_i^u := f(x_i, u)$. 

\subsubsection{Questions}
\begin{itemize}
\item Am I correct in assuming that you are using the above model with discrepancy $b \equiv 0$? \\
\textbf{Answer: } Yes, this is correct. The future possibility of including a discrepancy term is mentioned in section 4.2 of Fer et al (2018). 
\end{itemize}

\subsection{``Brute Force'' Calibration}
You mention a brute force calibration strategy in your paper, and I just wanted to write that out below to make sure I'm on the right page. The current parameters of interest are
$u$ and $\tau$. We therefore have posterior (conditional on field observations), 
\[p(u, \tau|z) \propto p(z|u, \tau)p(u, \tau) = N_n(z|y^u, \tau^2 I_n)p(u, \tau)\]
for some assumed prior on $(u, \tau)$ and where $y^u := (y_1^u, \dots, y_n^u)$. Everything here is implicitly conditional on $x_1, \dots, x_n$ but I will suppress this to simplify notation. 
We can use MCMC to sample from this posterior. However, the key observation is that evaluating the likelihood $N_n(z|y^u, \tau^2 I_n)$ requires the computation of $y^u$, which 
involves running the entire simulation with the current calibration $u$. Therefore, the computer model must be run in full at every single step of MCMC, 
which is infeasible for computationally expensive simulations. 

\subsection{Emulator Methodology: ``Modular Approach''}
To deal with the computational issue in the brute force approach, we replace the computer model with a GP emulator/surrogate $\eta(\cdot) \sim \mathcal{GP}(\mu(\cdot), k(\cdot, \cdot))$. 
Among the various emulator calibration approaches, a simple one is the ``modular'' approach described in Gramacy 8.1.2 and originally in 
\href{https://projecteuclid.org/journals/bayesian-analysis/volume-4/issue-1/Modularization-in-Bayesian-analysis-with-emphasis-on-analysis-of-computer/10.1214/09-BA404.full}{Bayarri et al (2009)}. 
The calibration approach is as follows. 
\begin{enumerate}
\item Run the full model on $N$ design points (i.e. knots): $\{(x_1^m, u_1), \dots, (x_N^m, u_N)\}$, where the ``m'' superscript differentiates from the vector of observation parameters $x$
sampled in the field data. Presumably, the design points are chosen such that $\{x_1^m, \dots, x_N^m\} \subset \{x_1, \dots, x_n\}$; that is, the model is run at the observation parameter
values observed in the real-world, with the possibility of replicates. Continuing with the same notation as before, let $x^m$ and $u^m$ be N-dimensional vectors containing the 
$x_i^m$ and $u_i$, respectively. Also let $y^m = (y_1^{u_1}, \dots, y_N^{u_N})$ be the vector of simulator outputs evaluated at the $N$ design points. 
\item Fit the GP emulator \textit{only using the computer model data} $y^m$. Denote the fit emulator by $\eta$. 
\item Now essentially conduct the brute force approach described above, but with $\eta$ replacing $f$. That is, we sample from posterior
\[p(u, \tau|z, \eta) \propto p(z|u, \tau, \eta)p(u, \tau) = N_n(z|\eta(x, u), \tau^2 I_n)p(u, \tau)\]
Now in each step of MCMC we need only evaluate the mean of the GP $\eta(x, u)$ at the current proposed $u$, rather than the whole computer model $f$. 
\end{enumerate}
This approach is not the one proposed by Kennedy and O'Hagan, which is the ``fully Bayesian'' approach: they assume a GP prior for $f$, then consider the posterior over the GP parameters
in addition to the other parameters. Gramacy notes that the modular approach can help to avoid some of the pathologies sometimes present in the fully Bayesian approach. This is of course not
the approach used in PEcAn as I am still considering emulating model outputs directly here (instead of sufficient statistics), but I still find it helpful to review before considering the sufficient 
statistic approach. 

\subsection{Emulator Methodology: Emulating Likelihood}
The above section approached the problem of sampling from the posterior
\[p(u, \tau|z) \propto N_n(z|y^u, \tau^2 I_n)p(u, \tau)\]
by replacing the full simulator computations $y^u$ with a surrogate that approximates the full computations. Assuming the surrogate is much faster than the original simulation, then 
this can drastically speed likelihood computation. An alternative method is to essentially skip this intermediate step and directly emulate the likelihood itself. In this case, the function 
we seek to emulate is
\[\tilde{f}(x_i, z_i, u) := p(z_i|y_i^{u}, \tau) = N(z_i|y_i^{u}, \tau^2)\]\
To be able to  define this I believe we must consider the extended input space $\mathcal{X} \times \mathcal{Z} \times \mathcal{U}$, which has added the space of observed real-world
outputs as an extension to the parameter space $\mathcal{P}$. 
Taking things a step further, we can emulate a sufficient statistic that allows full calculation of the likelihood. That is, we can emulate
\[T(x_i, z_i, u) := (z_i - y_i^{u})^2\]\
Let $\hat{T}(\cdot, \cdot, \cdot)$ denote the fitted GP emulator. 
This approach again allows cheaper (approximate) likelihood computations. Indeed, the likelihood of observed data $z \in \R^n$ (implicitly conditioned on $x \in \R^n$) is given by  
\begin{align*}
p(z|u, \tau) = N_n(z|y^u, \tau^2) &\propto \tau^{-n} \exp\left\{-\frac{1}{\tau^2} \norm{z - y^u}^2_2 \right\} \\
					         &\approx \tau^{-n} \exp\left\{-\frac{1}{\tau^2} \sum_{i = 1}^{n} \hat{T}(x_i, z_i, u) \right\} \\
\end{align*}
Thus, the likelihood can be approximated using evaluations of the GP $\hat{T}$. 
Given this, here is my current understanding of the emulator methodology (for a single site). I'm not considering the experimental design or MCMC methodologies yet at this point, so 
I take them as given. 
\begin{itemize}
\item Choose design points $\{(x_1^m, z_1^m, u_1), \dots (x_N^m, z_N^m, u_N)\}$. 
\item Run full simulator at these $(x_i^m, u_i)$ points, producing the vector of simulator outputs $y^m \in \R^N$, as before. 
\item Fit GP emulator to the mapping $T(x_i, z_i, u) = (z_i - y_i^{u})^2$, producing fit emulator $\hat{T}(\cdot, \cdot, \cdot)$. 
\item Perform MCMC for the approximate posterior
\[p(u, \tau|x^m, z) \propto \tau^{-n} \exp\left\{-\frac{1}{\tau^2} \sum_{i = 1}^{n} \hat{T}(x_i, z_i, u) \right\}p(u, \tau)\]
\end{itemize}

\subsubsection{Questions}
\begin{itemize}
\item What choices are you making for the GP parameters (mean function $\mu$ and kernel/covariance function $k$)? \\
\textbf{Answer: } Using the defaults of the \textit{mlegp} R package. I believe the default kernel is the inverse squared exponential but I need to check. 
\item Am I correct in defining the sufficient statistic function that you are emulating? 
\textbf{Answer: } Yes. 
\end{itemize}

\subsection{MCMC Algorithm}
\textbf{TODO}


\section{Future Directions}
\subsection{GP Modeling Improvements} (Istem's work with her student this summer)
\begin{itemize}
\item Investigating radial/isotropic covariance assumption (i.e. using separable/antisotropic kernels) (see Gramacy 5.2.5).
\item Investigating stationarity assumption. 
	\begin{itemize}
	\item Heteroskedastic modeling (see Gramacy ch 10). 
	\end{itemize}
\end{itemize}

\subsection{Experimental Design Improvements}
\begin{itemize}
\item Lots of literature in sequential design/active learning/Bayesian optimization
\end{itemize}

\subsection{Speeding up GP calculations}
There is a lot of literature in this area as well, with all of the techniques essentially trying to cope with the expensive matrix decompositions required 
to calculate the inverse and determinant in the equations used for GP prediction and inference. The introduction to chapter 9 in Gramacy's book has a 
very nice list of some of these methods (pseudo-inputs, compactly supported kernels, local neighborhoods, etc.) 

\subsection{MCMC}
Here, we focus in on specific improvements on MCMC side of things, noting that there could certainly be interplay between the previous areas of possible improvement 
and improvements to the MCMC algorithm itself. 

As a first step, Jonathan has already mentioned the potential for adopting a gradient-based approach by trying out Hamiltonian Monte Carlo. 

There is certainly a wide body of literature on MCMC algorithms in settings with expensive likelihood calculations. I looked around a bit for literature in this vein but 
tailored specifically to parameter calibration for computer experiments and didn't find as much as I expected. Here are some papers I did find: 
\begin{itemize}
\item \href{https://docs.google.com/document/d/16C8Zjw3b5aVr-hwDpqHWf8gr0cP80vcCHxAlFY6zVkE/edit#heading=h.mw4tscs8lgdr}{Bayesian calibration of terrestrial ecosystem models: a study of advanced Markov chain Monte Carlo methods}
\item \href{https://www.tandfonline.com/doi/abs/10.1198/TECH.2010.09195}{Efficient MCMC Schemes for Computationally Expensive Posterior Distributions}
\item \href{https://www.sciencedirect.com/science/article/pii/S0304380021001708}{Sequential Monte-Carlo algorithms for Bayesian model calibration â€“ A review and method comparison} 
(SMC, but interesting comparison to MCMC in the calibration setting)
\end{itemize}



\subsection{Targeting Specific Needs in Terrestrial Carbon Monitoring}
I included this section as a longer-term goal of better understanding the current state of the terrestrial carbon monitoring field, and how a combination 
of improvements in statistical modeling and computational techniques might help address current limitations. 



\section{PEcAn Code}
\textbf{TODO}

\end{document} 





