\documentclass[12pt]{article}
\RequirePackage[l2tabu, orthodox]{nag}
\usepackage[main=english]{babel}
\usepackage[rm={lining,tabular},sf={lining,tabular},tt={lining,tabular,monowidth}]{cfr-lm}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{amsthm,amssymb,latexsym,gensymb,mathtools,mathrsfs}
\usepackage{epstopdf,enumitem,microtype,dcolumn,booktabs,hyperref,url,fancyhdr}
\usepackage{algorithm}
\usepackage{algpseudocode} % Note that this also loads algorithmicx
\usepackage{cleveref}
\usepackage{bbm}
\usepackage{caption, subcaption} % Captions and sub-figures. 
% \usepackage[demo]{graphicx}

% Plotting
\usepackage{pgfplots}
\usepackage{xinttools} % for the \xintFor***
\usepgfplotslibrary{fillbetween}
\pgfplotsset{compat=1.8}
\usepackage{tikz}

% Local custom commands. 
\include{latex_macros_general}
\include{latex_macros_calibrating_LSMs}
\newcommand{\bphi}{\boldsymbol{\phi}}

\setlist{topsep=1ex,parsep=1ex,itemsep=0ex}
\setlist[1]{leftmargin=\parindent}
\setlist[enumerate,1]{label=\arabic*.,ref=\arabic*}
\setlist[enumerate,2]{label=(\alph*),ref=(\alph*)}

% For embedding images
\graphicspath{{../output/gp_post_approx_paper/}}

% Specifically for paper formatting 
\renewcommand{\baselinestretch}{1.2} % Spaces manuscript for easy reading

% Formatting definitions, propositions, etc. 
\newtheorem{definition}{Definition}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{example}{Example}

% Title and author
\title{Parameter Calibration for Land Surface Models}
\author{Andrew Roberts}

\begin{document}
\maketitle

% The Structure of Land Surface Models
\section{The Structure of Land Surface Models}
Unlike the typical PDE models used in atmospheric and oceanic modeling, the standard toolkit in land surface modeling 
consists primarily of ODEs describing time evolution at a single location. In this introductory section, we omit discussion 
of the application of these models across space, but return to this important topic in a later section. The notation 
used throughout this section can thus be thought of as suppressing a fixed spatial index on most of the quantities considered. 

While LSMs can incorporate a wide 
variety of terrestrial processes, we will focus on models of the carbon cycle when providing concrete examples. 
LSMs model the carbon cycle as consisting of a set of states (i.e., \textit{pools}; soil, roots, above-ground vegetation, atmosphere, etc.),
along with processes that transfer carbon between these states (i.e., \textit{fluxes}; photosynthesis, respiration, etc.). 
We denote the state vector at time $\Time$ by 
\begin{align}
\state(\Time) \Def [\indexState[1]{\state}(\Time), \dots, \indexState[\dimState]{\state}(\Time)]^\top \in \R^{\dimState}.
\end{align}
Carbon fluxes between these states are then described by an ODE model of the form
\begin{align}
&\frac{d}{d\Time} \state_{\Par}(\Time) = \funcODE_{\Par}(\state_{\Par}(\Time), \forcing(\Time)), &&\state_{\Par}(\timeInitState) = \stateInit, \label{ode}
\end{align}
where $\funcODE_{\Par}(\cdot, \forcing): \R^{\dimState} \to \R^{\dimState}$ is typically a nonlinear function, and $\stateInit$ is the \textit{initial condition}, 
the value of the state at some initial time $\timeInitState$. Models of this form are referred to as \textit{pool-based models}, or more generically 
\textit{box models}, as they conceptualize the carbon cycle are consisting of a set of carbon pools that are exchanging carbon. 
The forward dynamics encoded by $\funcODE_{\Par}$ are dependent both on a set of 
\textit{parameters} $\Par \in \parSpace \subset \R^{\dimPar}$, as well as a \textit{forcing} (i.e., \textit{driving}) function $\forcing(\Time)$.
In general, the parameters $\Par$ are not physical constants; rather, they provide a means to empirically parameterize ecosystem 
processes, which may vary in space and time.   
We emphasize that the solution $\state_{\Par}(\Time)$ of the ODE is a function of the parameters $\Par$, driver $\forcing(\Time)$, and 
initial condition $\stateInit$, though only the first is made explicit in the notation as the parameters are the main quantity of interest in this document.
As described here, this model is point-based, in that it is designed to simulate carbon dynamics at a single spatial location. We note that 
there is a large diversity in LSMs, and other models may be defined to operate over regions (e.g., pixels). 

% The Forward Problem 
\subsection{The Forward Problem}
The \textit{forward problem} consists of recovering the solution $\state_{\Par}(\Time)$ over some time interval $[\timeInitState, \timeEnd]$ for given 
values of $\Par$, $\stateInit$, and $\forcing(\cdot)$. Given the nonlinearity of $\funcODE_{\Par}$, this solution is typically approximated 
numerically at a finite set of times $\{\indexTime{\Time}\}_{\timeIdx=\firstTimeIdxState}^{\lastTimeIdx} \subset [\timeInitState, \timeEnd]$. We 
let $\stateApprox_{\Par}(\indexTime{\Time})$ denote the approximation of $\state_{\Par}(\indexTime{\Time})$ for each 
$\timeIdx = \firstTimeIdxState, \dots, \lastTimeIdx$. Also, let $\indexTime{\timeStep} \Def \indexTime[\timeIdx+1]{\Time} - \indexTime{\Time}$ denote the 
time step used by the numerical solver at the $\timeIdx^{\text{th}}$ iteration. Standard ODE solvers are time-stepping algorithms of the 
form 
\footnote{The simplest example here is a forward Euler scheme, in which the update assumes the form 
$\stateApprox_{\Par}(\indexTime[\timeIdx+1]{\Time})
= \stateApprox_{\Par}(\indexTime{\Time}) + \indexTime{\timeStep} \funcODE(\stateApprox_{\Par}(\indexTime{\Time}), \forcing(\indexTime{\Time}))$.}
\begin{align}
&\stateApprox_{\Par}(\indexTime[\timeIdx+1]{\Time}) \Def \fwdOne_{\Par}(\stateApprox_{\Par}(\indexTime{\Time}), \forcing(\indexTime{\Time}); \indexTime{\timeStep}), 
&&\stateApprox_{\Par}(\indexTime[\firstTimeIdxState]{\Time}) = \stateInit, \label{ode_discrete}
\end{align}
for $\timeIdx = \firstTimeIdxState, \dots, \lastTimeIdx$. Setting a constant step size $\timeStep \equiv \indexTime{\timeStep}$ is a common choice, though 
algorithms that adapt the step size are also quite standard. In practice, the model drivers $\forcing(\indexTime{\Time})$
are typically given by a sequence of (noisy) observations of meteorological variables (e.g., photosynthetically-active radiation).
Going forward, we will treat the discrete dynamical system \ref{ode_discrete} as 
the true model and starting point for subsequent analysis, thus neglecting discretization error with respect to the true continuous-time solution. 



% The Inverse Problem
\subsection{The Inverse Problem}
Naturally, one must recognize that the predictions of the model \ref{ode_discrete} are subject to many sources of uncertainty. 
The simulated trajectory $\{\stateApprox_{\Par}(\indexTime{\Time})\}_{\timeIdx=\firstTimeIdxState}^{\lastTimeIdx}$ is a function of  
the chosen value of the parameters $\Par$, the model drivers  $\{\indexTime{\obs}\}_{\timeIdx=\firstTimeIdxState}^{\lastTimeIdx}$, the initial condition $\stateInit$, 
and the model processes encoded by $\fwdOne_{\Par}$ (irrespective of the parameter value). In the LSM context, 
all of these quantities contribute non-negligible uncertainty. 
In this document, we restrict ourselves to the consideration of parameter uncertainty. 

The values of LSM parameters are not known exactly, and moreover, cannot always be observed directly. In other words, offline estimation of $\Par$
using a separate data source is typically not possible for all of the parameters of interest. Historically, the choice of parameter settings 
was commonly performed on an ad hoc basis using expert judgement. 
Given that $\Par$ consists of empirical parameters 
(not physical constants) the concept of ``true'' parameter values is largely ill-defined. It is perhaps most conceptually useful to draw an 
analogy with empirically-determined parameters of a statistical model, in which ``true values'' is interpreted as best-fit in a regression sense.
Therefore, a natural solution is to learn the parameter values from data. For example, suppose we have access to noisy observations 
$\{\indexTime{\obs}\}_{\timeIdx=\firstTimeIdxObs}^{\lastTimeIdx}$ of the true states 
$\{\state_{\Par}(\indexTime{\Time})\}_{\timeIdx=\firstTimeIdxObs}^{\lastTimeIdx}$. 
We can then consider tuning (i.e., \textit{calibrating}), the value of $\Par$ such that $\{\state_{\Par}(\indexTime{\Time})\}_{\timeIdx=\firstTimeIdxObs}^{\lastTimeIdx}$
is ``close'' to $\{\indexTime{\obs}\}_{\timeIdx=\firstTimeIdxObs}^{\lastTimeIdx}$  in some well-defined sense. We will refer to the 
general problem of learning parameters from data as \textit{parameter calibration} or \textit{parameter estimation}. We make this 
problem precise in the following section, which casts parameter calibration within the generic framework of \textit{inverse problems}. Before 
doing so, we provide a concrete example of a basic vegetation model. 

% Toy Example: Very Simple Ecosystem Model 
\subsection{Toy Example: Very Simple Ecosystem Model} \label{vsem}
To illustrate the basic structure of LSMs, we consider the \textit{Very Simple Ecosystem Model (VSEM)}, a toy model implemented 
in the \textit{BayesianTools} R package for this express purpose. In this model, the state vector has dimension $\dimState=3$ and is given by 
\begin{align*}
\state(\Time) \Def [\stateV(\Time), \stateR(\Time), \stateS(\Time)]^\top \in \R^{3}, 
\end{align*}
where the states represent the above-ground vegetation, below-ground vegetation (roots), and soil organic matter 
carbon pools (\textrm{kg C/$m^2$}), respectively. The model driver $\forcing(\Time)$ is photosynthetically active radiation 
(PAR; \textrm{MJ/$m^2$/day}). The dynamics describing the carbon fluxes between these pools rely on a parameterized model 
of Net Primary Productivity (NPP; \textrm{kg C/$m^2$/day}), 
which is calculated as the Gross Primary Productivity (GPP) minus carbon released due to autotrophic respiration.
GPP quantifies the amount of carbon fixed by vegetation during photosynthesis.
The state equations describing the carbon dynamics are then given by
\begin{align}
\dstateV(\Time) &= \alphaV \NPP(\stateV(\Time), \forcing(\Time)) - \frac{\stateV(\Time)}{\tauV} \\
\dstateR(\Time) &= (1.0 - \alphaV) \NPP(\stateV(\Time), \forcing(\Time)) - \frac{\stateR(\Time)}{\tauR} \nonumber \\ 
\dstateS(\Time) &= \frac{\stateR(\Time)}{\tauR} + \frac{\stateV(\Time)}{\tauV} - \frac{\stateS(\Time)}{\tauS}, \nonumber
\end{align}
with $\NPP(\stateV(\Time), \forcing(\Time))$ modeled as
\begin{align}
\NPP(\stateV, \forcing) &= (1 - \fracRespiration) \GPP(\stateV, \forcing) \\
\GPP(\stateV, \forcing) &= \forcing \cdot \LUE \cdot \left[1 - \exp\left\{-\KEXT \cdot \LAI(\stateV) \right\} \right] \nonumber \\
\LAI(\stateV) &= \LAR \cdot \stateV. \nonumber
\end{align} 
This ODE is of the form \ref{ode}, with parameters
\begin{align}
\Par = \{\alphaV, \tauV, \tauR, \tauS, \LUE, \fracRespiration, \LAR\} \label{vsem_param}
\end{align}
Note that this model is over-parameterized, which can clearly be seen by writing 
\begin{align}
\NPP(\stateV, \forcing) &= \forcing \cdot \LUE \cdot (1 - \fracRespiration) \left[1 - \exp\left\{-\KEXT \cdot \LAR \cdot \stateV\right\} \right]. 
\end{align}
The parameters $\{\LUE, \fracRespiration\}$ are unidentifiable, as are $\{\LAR, \KEXT\}$. The following section will discuss methods 
to deal with this issue. 
The dynamics $\funcODE_{\Par}$ are nonlinear in $\state$ and autonomous, in that the 
time dependence appears only through $\state(\Time)$ and $\forcing(\Time)$. 

% Inverse problems 
\section{Inverse Problems}
In this section, we provide a brief introduction to inverse problems from a generic perspective. The following section 
interprets these ideas in the context of parameter calibration for LSMs. 

\subsection{Basic Setup}
We start by considering a \textit{forward model} 
\footnote{An alternative name for $\fwd$ is the \textit{parameter-to-observable map}. We favor the shorter term 
\textit{forward model}, but note that terminology can vary.}
$\fwd: \parSpace \subseteq \R^{\dimPar} \to \R^{\dimObs}$ describing some 
process of interest, parameterized by input parameters $\Par \in \parSpace$. In addition, suppose we have noisy 
observations $\obs \in \obsSpace \subset \R^{\dimObs}$ of the output signal that $\fwd(\Par)$ approximates. 
The canonical noise model is of the additive form 
\begin{align}
\obs &= \fwd(\Par) + \noise,
\end{align}
though other formulations are possible. 
We seek to invert the process to infer the parameter values that produced the data; loosely speaking, the 
\textit{inverse problem} consists of identifying $\Par$ such that $\fwd(\Par) \approx \obs$. 

\begin{example} \label{ex:lsm-inv-prob}
There are many ways we might define the forward model in the dynamical setting \ref{ode_discrete}, which we recall 
conceptualizes an LSM at a single spatial location. Commonly, the data 
$\obs \Def \{\indexTime{\obs}\}_{\timeIdx=\firstTimeIdxObs}^{\lastTimeIdx}$ in this context consists of noisy observations
of either carbon states or fluxes. Both cases typically imply that $\obs$ is viewed as some function of the underlying 
true states, which have been perturbed by noise. As a concrete example, the VSEM parameter inversion might be formulated
as 
\begin{align}
\obs &= \fwd(\Par) + \noise,
\end{align}
with $\Par$ given by \ref{vsem_param}. Some options for the definition of $\fwd: \parSpace \to \R^{\Ntime}$ include
\begin{align}
\fwd_{\indexTime{\Time}}(\Par) 
&\Def \LAI(\stateV(\indexTime{\Time})) 
= \LAR \cdot \stateV(\indexTime{\Time}) \label{lsm-fwd-state}
\end{align}
and
\begin{align}
\fwd_{\indexTime{\Time}}(\Par) 
&\Def \NPP(\stateV(\indexTime{\Time}), \forcing(\indexTime{\Time})) \label{lsm-fwd-flux} \\
&= \forcing(\indexTime{\Time}) \cdot \LUE \cdot (1 - \fracRespiration) \left[1 - \exp\left\{-\KEXT \cdot \LAR \cdot \stateV(\indexTime{\Time})\right\} \right], \nonumber
\end{align}
for $\timeIdx = \firstTimeIdxObs, \dots, \lastTimeIdx$. These two options are specific examples of pool-based and flux-based 
calibration, respectively. 
\footnote{It is also common to calibrate to multiple outputs, including combinations of pools and fluxes, simultaneously. Such multi-objective
formulations are discussed in \Cref{multi-objective}.}
Note that in both cases, the forward model is a composition of two maps: (1) the map from parameters to 
states (given by the forward ODE simulation); and (2) a map from states to observables (in this case LAI or NPP).   
Finally, we note that the initial conditions 
$\stateInit$ are typically subject to uncertainty; one approach to deal with this is to view the initial conditions
$\left\{\stateV(\timeInitState), \stateR(\timeInitState), \stateS(\timeInitState)\right\}$ as additional parameters to be estimated 
from data, thus incorporating them into the parameter set $\Par$. 
\end{example}

We emphasize that this setup is quite general; in particular, $\fwd$ can conceptualize a wide array 
of different problems. The forward model is an abstraction that represents the map from parameters to a quantity that
can be directly compared to the observations $\obs$. We focus on the setting where $\fwd$ is a deterministic, known 
function. It is often the case that the forward model has no analytical form; rather, $\fwd$ may be given by a complex 
computer simulation (e.g., an ODE solver). 
In the setting of interest, $\fwd$ typically has a variety of characteristics that make 
the inverse problem especially challenging. We will emphasize forward models that (1) are expensive to evaluate 
at inputs $\Par$; (2) have high-dimensional, structured output spaces (e.g., time series); and (3) are highly nonlinear. 
In addition, computing gradients of $\fwd$ may be difficult or impossible in many cases. Given these characteristics, 
it is desirable to consider algorithms that treat $\fwd$ as a ``black-box''; that is, algorithms that only require the 
ability to compute evaluations $\fwd(\Par)$, without assuming any additional structure. However, there are certainly 
cases where additional structure (e.g., derivative information, smoothness, periodicity) can be exploited. 

\subsection{Loss Minimization}
It is typically impossible (or undesirable) to seek an exact solution $\fwd(\Par) = \obs$. An alternative is to cast 
model inversion as an optimization problem of the form 
\begin{align}
\parEst \Def \text{argmin}_{\Par \in \parSpace} \loss(\Par), \label{opt}
\end{align}
where $\loss(\Par)$ is a loss function, providing some notion of discrepancy between $\fwd(\Par)$ and $\obs$.
A common choice is the Euclidean distance (i.e., mean squared error),
\begin{align}
\loss(\Par) = \norm{\obs - \fwd(\Par)}^2_2 \Def \sum_{\obsIdx=1}^{\dimObs} (\obs_{\obsIdx} - \fwd_{\obsIdx}(\Par))^2. \label{l2_loss}
\end{align}
This quadratic loss can be extended to weighted generalizations via 
\begin{align}
\loss(\Par) = \norm{\obs - \fwd(\Par)}^2_{\obsCov} \Def (\obs - \fwd(\Par))^\top \obsCov^{-1} (\obs - \fwd(\Par)), \label{l2_loss_weighted}
\end{align}
for some positive-definite matrix $\obsCov \in \R^{\dimObs \times \dimObs}$. Note that \ref{l2_loss_weighted} reduces to 
\ref{l2_loss} by setting $\obsCov$ to the identity matrix. 

In theory, any standard optimization algorithms may be employed to solve this optimization problem. However, 
the typical black-box structure of $\fwd$ poses a challenge. If gradients are available, then gradient descent or 
(quasi)-Newton methods are a possibility. However, evaluations of $\fwd(\Par)$ may be prohibitively expensive, 
requiring specialized optimization routines designed to minimize the required number of model evaluations. 
So-called \textit{black box}, or \textit{derivative free}, optimization algorithms are designed for the setting 
in which gradient evaluations are not available. 

Another issue to contend with is the potential for $\loss(\Par)$ to have many local minima. The phenomenon in which 
multiple values of $\Par$ explain the observed data equally well is generally referred to as \textit{non-identifiability}, 
or \textit{equifinality}. One approach to mitigate non-identifiability issues is to incorporate prior information regarding 
the value of $\Par$. This can be accomplished via the addition of a \textit{regularization} term $\regularizer(\Par)$ in 
the objective function, yielding 
\begin{align}
\parEst \Def \text{argmin}_{\Par \in \parSpace} \left\{\loss(\Par) + \regularizer(\Par) \right\}. \label{opt_reg}
\end{align}
A common choice of regularizer is $\regularizer(\Par) = \frac{1}{c^2} \norm{\Par - \priorMean}_2^2$, with $c > 0$ tuning 
the strength of the regularization. As before, this can be generalized to 
$\regularizer(\Par) = \norm{\Par - \priorMean}_{\priorCov}^2 = (\Par - \priorMean)^\top \priorCov^{-1} (\Par - \priorMean)$, 
with $\priorCov$ another positive definite matrix. 
Choosing both a quadratic loss and quadratic regularization yields the common formulation
\begin{align}
\parEst \Def \text{argmin}_{\Par \in \parSpace} \left\{\norm{\obs - \fwd(\Par)}^2_{\obsCov} + \norm{\Par - \priorMean}_{\priorCov}^2\right\}. \label{quadratic_opt}
\end{align}
The first term quantifies the model fit, while the second encourages agreement between $\Par$ and $\priorMean$.

\subsection{Maximum Likelihood}
We now consider a frequentist statistical approach to the solution of inverse problems. Instead of defining a loss function, the 
starting point is now to consider a \textit{likelihood function} $p(\obs | \Par)$ (viewed as a function of $\Par$, with $\obs$ fixed). 
For example, we might assume that the observations $\obs$ are given by Gaussian perturbations of the underlying signal 
$\fwd(\Par)$, for some ``true'' value of $\theta$: 
\begin{align}
\obs | \Par &\sim \Gaussian(\fwd(\Par), \obsCov). \label{gaussian-lik}
\end{align}
In this case, the likelihood function is then given by $p(\obs | \Par) = \Gaussian(\obs | \fwd(\Par), \obsCov)$, with 
$\Gaussian(\obs | \fwd(\Par), \obsCov)$ denoting the density of $\Gaussian(\fwd(\Par), \obsCov)$ evaluated 
at $\obs$. Note that \ref{gaussian-lik} can equivalently be written as the additive error model 
\begin{align}
\obs &= \fwd(\Par) + \noise, && \noise \sim \Gaussian(0, \obsCov). 
\end{align}
As was the case with the loss function, evaluating the likelihood at $\fwd$ requires the forward 
model evaluation $\fwd(\Par)$. We can then define the solution to the inverse problem as the value 
of $\Par$ that maximizes the likelihood of the data; that is, 
\begin{align}
\parEst \Def \text{argmax}_{\Par \in \parSpace} \ p(\obs | \Par). \label{mle}
\end{align}
Maximizing $p(\obs | \Par)$ is equivalent to minimizing $\loss(\Par) \Def -\log p(\obs | \Par)$, so we can view the 
negative log-likelihood as defining a loss function, thus drawing a connection with the non-statistical optimization 
framework discussed above. Indeed, if we consider the Gaussian observation model \ref{gaussian-lik}, the log-likelihood
assumes the form 
\begin{align}
\loss(\Par) = -\log p(\obs | \Par) = \cst + \frac{1}{2} \norm{\obs - \fwd(\Par)}_{\obsCov}^2, 
\end{align}
where $\cst$ is a constant that depends only on $\obsCov$, not $\Par$. 
This implies that the MLE solution in the Gaussian noise setting is equal to the minimizer of the loss function 
\ref{l2_loss_weighted}. The observation covariance $\obsCov$ is often not known in practice, and hence can 
also be jointly optimized along with $\Par$. A regularization term can also be included in the same way as in the 
loss minimization framework. 

\subsection{Bayesian Methods}
The frequentist MLE approach views the unknown parameter $\Par$ as a fixed quantity, which we seek to estimate.
The Bayesian approach instead views $\Par$ as a random variable, and proceeds by defining a joint probability 
distribution on $(\Par, \obs)$. This is typically specified by noting that the joint distribution can be 
factored as $p(\Par, \obs) = p(\obs | \Par)p(\Par)$. The first term is the likelihood, which we already considered in 
the previous section. The new requirement is thus to define the \textit{prior distribution} $p(\Par)$, which encodes
domain knowledge about $\Par$ prior to observing the data $\obs$. We can now define the solution to the Bayesian 
inverse problem to be the conditional distribution $p(\Par | \obs)$, known as the \textit{posterior distribution}. 
Applying Bayes' rule shows that the posterior density can be computed via 
\begin{align}
p(\Par | \obs) &= \frac{p(\obs | \Par) p(\Par)}{p(\obs)} \propto p(\obs | \Par) p(\Par). 
\end{align}
The denominator (known as the \textit{model evidence} or more generically the \textit{normalizing constant}) is 
not a function of $\Par$. Thus, so long as the data $\obs$ is viewed as fixed throughout the analysis, this constant 
is not a required input to most statistical inference algorithms. It is worth emphasizing that the solution to the Bayesian inverse problem 
is an entire distribution $p(\Par | \obs)$, whereas the loss minimization and MLE approaches instead result in point 
estimates of $\Par$. One can view the posterior $p(\Par | \obs)$ as assigning a weight to each possible value of 
$\Par$ that encodes both its fit to the data as well as its agreement with the prior.  
Therefore, the Bayesian approach is viewed as the gold standard for uncertainty quantification. In the Bayesian framework, 
the prior $p(\Par)$ acts as the regularizer, providing a probabilistic perspective on the regularization terms discussed previously. 

Within the loss minimization and MLE frameworks, the act of solving an inverse problem reduced to optimizing some 
objective function. In the Bayesian case, the key challenge is to extract useful information from the posterior distribution 
$p(\Par | \obs)$. In rare cases, the posterior assumes a well-understood analytical form (e.g., Gaussian). However, in 
general we must resort to computational methods. Extracting information from the posterior commonly implies
computing useful statistics (mean, median, mode, variance, quantiles, etc.) or drawing samples. 

We first consider computing the mode of the posterior distribution, which is known as the 
\textit{maximum a posteriori (MAP)} estimator. The MAP estimator solves the optimization problem 
\begin{align}
\parEst &\Def \text{argmax}_{\Par \in \parSpace} \ p(\Par | \obs), \nonumber \\
&= \text{argmin}_{\Par \in \parSpace} \left\{-\log p(\obs | \Par) - \log p(\Par) \right\} \label{map}
\end{align}
where we have taken the log, and have again re-written the maximization as a minimization by negating the objective function. 
We immediately see that \ref{map} is equivalent to the regularized MLE approach with $\regularizer(\Par) \Def -\log p(\Par)$ acting as the regularization term.
Choosing a uniform prior reduces to the classical (un-regularized) MLE setting. If we assume a Gaussian likelihood and prior, 
\begin{align*}
\obs | \Par &\sim \Gaussian(\fwd(\Par), \obsCov) \\
\Par &\sim \Gaussian(\priorMean, \priorCov), 
\end{align*} 
then the MAP optimization \ref{map} reduces to 
\begin{align}
\parEst \Def \text{argmin}_{\Par \in \parSpace} \left\{\norm{\obs - \fwd(\Par)}^2_{\obsCov} + \norm{\Par - \priorMean}_{\priorCov}^2\right\}, 
\end{align}
which is precisely equal to \ref{quadratic_opt}. We have thus established a connection between regularized loss minimization, regularized 
MLE, and MAP estimation. 

While the MAP estimate provides a nice interpretation of regularization, it again reduces to a single point estimate of $\Par$. The true 
power of Bayesian methods lies in the information provided by the entire posterior distribution. It is beyond the scope of these notes to 
summarize the wide array of techniques for interrogating the posterior, but we note that the most common method involves drawing 
samples from $p(\Par | \obs)$. These samples can then be used to estimate moments (mean, variance, etc.), generate histograms, 
and produce other posterior summaries. Markov chain Monte Carlo (MCMC) methods is a powerful class of algorithms for producing 
samples from arbitrary probability distributions. These algorithms are iterative, and simply require the ability to evaluate the 
unnormalized posterior density $p(\obs | \Par) p(\Par)$ given any value of $\Par$. Similar to standard iterative optimization algorithms, 
MCMC therefore requires one evaluation of the forward model $\fwd(\Par)$ for each iteration of the algorithm. It is not uncommon for 
MCMC to require $\BigO(10^5)$ or more iterations, which can render these algorithms infeasible for expensive forward models. 

% Parameter calibration 
\section{Parameter Calibration for LSMs}
We now return to the discussion of LSMs, mapping the generic inverse problem concepts introduced above to the problem 
of calibrating LSM parameters. We recall from \Cref{ex:lsm-inv-prob} that the observation $\obs$ in this setting typically 
consists of a noisy time series $\{\obs_{\indexTime[\firstTimeIdxObs]{\Time}}, \dots, \obs_{\indexTime[\lastTimeIdx]{\Time}}\}$
of observable carbon pool or flux related quantities. These time series may contain widely varying degrees of missing data, 
stemming from the fact that the temporal resolution of the data may differ significantly from the time step used by the ODE 
solver. For example, the ODE may be solved numerically using a sub-daily 
time step, while the time resolution of the calibration data products can vary from sub-daily (e.g., eddy covariance flux observations) 
to annual (e.g., field measurements of soil and leaf litter). It is common to calibrate LSM parameters using multiple data sources of 
varying temporal resolution, though we defer such discussions of multi-objective calibration to \Cref{multi-objective}. 
We therefore start by considering a forward model of the form $\fwd: \parSpace \to \R^{\Ntime}$ mapping from parameters 
to observables, with \ref{lsm-fwd-state} and \ref{lsm-fwd-flux} providing concrete examples of calibrating to pool and flux 
data, respectively. The following sections dive deeper into various challenges associated with LSM parameter calibration. 

\subsection{Spatial Considerations}
Up until this point, we have been considering parameter calibration for an LSM at a single spatial location. In practice, these 
models are run at many different spatial locations in order to produce regional, continental, or global forecasts of the carbon 
cycle. We emphasize that this is distinct from the typical models employed in ocean and atmosphere modeling, which are PDEs
and hence explicitly describe evolution in time and space. By contrast, LSMs are ODEs describing time dependence, that 
are then simulated independently at a set of spatial locations. For example, LSMs may be run at a set of locations 
aligning with ground-based data (e.g., the NEON or AMERIFLUX networks) or on a pixel-by-pixel basis if calibrating with 
respect to satellite observations (e.g., as is done within the CARDAMOM framework). We therefore start by making explicit 
the dependence of these models on space. We modify the single-location formulation \ref{ode} as
\begin{align}
&\frac{d}{d\Time} \indexSpace{\state}(\Time) = \funcODE_{\indexSpace{\Par}}(\indexSpace{\state}(\Time), \indexSpace{\forcing}(\Time)), &&\indexSpace{\state}(\timeInitState) = \stateInitSpace, \label{ode-space}
\end{align}
where $\spaceIdx$ is some two-dimensional spatial coordinate (e.g., latitude/longitude or a two-dimensional Euclidean projection). Both the 
initial condition $\stateInitSpace$ and model driver $\indexSpace{\forcing}(\Time)$ vary in space; in numerically solving 
the ODE these quantities are typically provided by data sources with explicit spatial identification. Consequently, the discretized state trajectories 
$\{\indexSpace{\state}(\indexTime{\Time})\}_{\timeIdx=\firstTimeIdxState}^{\lastTimeIdx}$ will also have spatial dependence. Crucially, 
the parameters $\indexSpace{\Par}$ are also expected to vary in space. For example, plant traits will naturally vary across different ecosystems. 
To account for such differences, a standard approach is to group plants into broader classes known as \textit{plant functional types (PFTs)}. 
When such approaches are used, we let $\Npft$ denote the number of PFTs. 
For example, a single plant trait may contribute $\Npft$ different parameters to $\Par$, one parameter per PFT. Importantly, the use of PFTs is 
only a coarse surrogate for spatial variation. Parameters may exhibit significant spatial variation even within the same PFT. To be clear on notation, 
the parameter set $\Par$ is already assumed to contain PFT-specific parameters; therefore, $\indexSpace{\Par}$ identifies the potential for 
spatial variation even after specifying PFT classifications. PFT considerations are discussed further in 
\Cref{param-structure}. We also note that not all parameters in LSMs correspond to plant traits, and these 
additional parameters are typically also subject to spatial variability. 

\subsubsection{Change-of-Support}
\subsubsection{Impure Pixels}
\subsubsection{Spatially-Varying Parameter Calibration}


\subsection{Parameter Structure} \label{param-structure}
PFTs, identifiability/equifinality, non-linear dependence.

\subsubsection{Parameter Dimension Reduction}
Parameter fixing, potential for more sophisticated methods, scaling factors for PFTs

\subsection{Multi-Objective Calibration} \label{multi-objective}

\subsection{Learning Initial Conditions}

\subsection{Connections with State Estimation}

% Surrogate Modeling 
\section{Surrogate Modeling}

\end{document} 




