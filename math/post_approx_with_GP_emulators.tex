\documentclass[12pt]{article}
\RequirePackage[l2tabu, orthodox]{nag}
\usepackage[main=english]{babel}
\usepackage[rm={lining,tabular},sf={lining,tabular},tt={lining,tabular,monowidth}]{cfr-lm}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{amsthm,amssymb,latexsym,gensymb,mathtools,mathrsfs}
\usepackage{epstopdf,enumitem,microtype,dcolumn,booktabs,hyperref,url,fancyhdr}
\usepackage{algorithm}
\usepackage{algpseudocode} % Note that this also loads algorithmicx
\usepackage{cleveref}
\usepackage{bbm}

% Plotting
\usepackage{pgfplots}
\usepackage{xinttools} % for the \xintFor***
\usepgfplotslibrary{fillbetween}
\pgfplotsset{compat=1.8}
\usepackage{tikz}

% Local custom commands. 
\include{latex_macros_general}
\include{latex_macros_gp_inv_prob}
\newcommand{\bphi}{\boldsymbol{\phi}}

\setlist{topsep=1ex,parsep=1ex,itemsep=0ex}
\setlist[1]{leftmargin=\parindent}
\setlist[enumerate,1]{label=\arabic*.,ref=\arabic*}
\setlist[enumerate,2]{label=(\alph*),ref=(\alph*)}

% For embedding images
\graphicspath{ {./images/} }

% Specifically for paper formatting 
\renewcommand{\baselinestretch}{1.2} % Spaces manuscript for easy reading

% Formatting definitions, propositions, etc. 
\newtheorem{definition}{Definition}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{corollary}{Corollary}

% Title and author
\title{Gaussian Process Surrogates for Bayesian Inverse Problems: Posterior Approximation and Sequential Design}
\author{Andrew Roberts}

\begin{document}

\maketitle

% Use citep and citet
% Write w_1 and w_2 as functions of u and u-tilde 
% Email Amy 

% Introduction 
\section{Introduction}

% Setting and Background 
\section{Setting and Background}

\subsection{Bayesian Inverse Problems}
We consider a \textit{forward model} $\fwd: \R^{\dimPar} \to \R^{\dimObs}$ describing some system of interest, parameterized by 
input parameters $\Par \in \parSpace \subset \R^{\dimPar}$. In addition, we suppose that we have noisy observations 
$\obs \in \obsSpace \subset \R^{\dimObs}$ of the output signal which $\fwd(\Par)$ seeks to approximate. The 
\textit{inverse problem} concerns learning the parameter values $\Par$ such that $\fwd(\Par) \approx \obs$; i.e., \textit{calibrating}
the model so that it agrees with reality. The statistical approach to this problem assumes that the link between model outputs and 
observations is governed by a probability distribution on  $\obs | \Par$. We assume that this distribution admits a density 
with corresponding log-likelihood 
\begin{align}
\llik: \parSpace \to \R, \label{log_likelihood}
\end{align}
such that $p(\obs | \Par) = \Exp{\llik(\Par)}$ 
\footnote{For succinctness, the notation $\llik(\Par)$ suppresses dependence on the data $\obs$, 
as we assume the data is fixed throughout the analysis.}
\footnote{We are currently focusing on the forward model parameter $\Par$, treating other likelihood 
parameters (e.g., observation covariance) as fixed. We will relax this assumption 
in section \ref{section_lik_par}.}.

The Bayesian approach completes this specification with a prior distribution 
on $\Par$. Letting $\priorDens(\Par)$ denote the density of this distribution, the Bayesian solution of the inverse problem is given 
by the posterior distribution $\postDensNorm(\Par) \Def p(\Par | \obs)$. The posterior density is computed by Bayes' rule 
\begin{align}
\postDensNorm(\Par) = \frac{1}{\normCst}\postDens(\Par) \Def \frac{1}{\normCst} \priorDens(\Par) \Exp{\llik(\Par)}, \label{post_dens}
\end{align}
with normalizing constant given by 
\begin{align}
\normCst &= \int \priorDens(\Par) \Exp{\llik(\Par)} d\Par. \label{norm_cst}
\end{align}
We emphasize that throughout this paper $\postDens(\Par)$ denotes the \textit{unnormalized} 
posterior density. When relevant, we will make explicit the likelihood dependence on the forward 
model by writing $\llik(\Par; \fwd)$. In addition to considering 
generic likelihoods, we will give special attention to the additive Gaussian noise model
\begin{align}
\obs &= \fwd(\Par) + \noise \label{inv_prob_Gaussian} \\
\noise &\sim \Gaussian(0, \likPar) \nonumber \\
\Par &\sim \priorDens \nonumber, 
\end{align}
with corresponding log-likelihood 
\begin{align}
\llik(\Par; \fwd) &= \log\Gaussian(\obs| \fwd(\Par), \likPar) 
= -\frac{1}{2} \log\det (2\pi \likPar) - \frac{1}{2} (\obs - \fwd(\Par))^\top \likPar^{-1} (y - \fwd(\Par)). \label{llik_Gaussian}
\end{align}

In general, the nonlinearity of $\fwd$ precludes a closed-form characterization of the posterior. In this case, the 
standard approach is to instead draw samples from the distribution using a Markov chain Monte Carlo (MCMC) 
algorithm. Such algorithms are iterative in nature, often requiring $\BigO(10^6)$ iterations, with each 
iteration involving an evaluation of the unnormalized posterior density $\postDens$. In the inverse problem 
context, this computational requirement is often prohibitive when the forward model evaluations $\fwd(\Par)$
incur significant computational cost. This becomes clear upon observing that each log-likelihood evaluation 
$\llik(\Par)$ (e.g., \ref{llik_Gaussian} in the additive Gaussian case) requires the corresponding forward 
model evaluation $\fwd(\Par)$. Motivated by this problem, a large body of work has focused on deriving 
cheap approximations to either $\fwd(\Par)$ or $\llik(\Par)$ (note that approximating the former induces 
an approximation of the latter). We refer to such approximations interchangeably 
as \textit{surrogates} or \textit{emulators}. The focus of the present work is on utilizing \textit{Gaussian processes} 
to derive emulators for the forward model or log-likelihood. 

\subsection{Gaussian Processes}
In this section, we provide a brief review of Gaussian processes; for in-depth treatments, we refer readers to 
\cite{gramacy2020surrogates, StuartTeck2, gpML}. A \textit{Gaussian process} (GP) is a probability 
distribution over a set of functions $\func: \parSpace \to \R$, defined by the property that 
the random vector $[\func(\Par_1), \dots, \func(\Par_\Ndesign)]^\top$ has
a joint Gaussian distribution for any set of inputs $\parMat \Def \{\Par_1, \dots, \Par_\Ndesign\} \subset \parSpace$. 
A GP is defined by its \textit{mean function} $\gpMeanPrior: \parSpace \to \R$ and positive-definite \textit{kernel} 
(i.e., \textit{covariance function}) $\gpKerPrior: \parSpace \times \parSpace \to \R$ and is thus denoted by 
$\GP(\gpMeanPrior, \gpKerPrior)$. Throughout this paper, we will vectorize notation as follows. Letting 
$\tilde{\parMat} \Def [\tilde{\Par}_1, \dots, \tilde{\Par}_M]  \subset \parSpace$ be a second set of inputs, we define 
$\func(\parMat) \Def [\func(\Par_1), \dots, \func(\Par_\Ndesign)]^\top \in \R^{\Ndesign}$ (and similarly for $\gpMeanPrior(\parMat)$), 
$\gpKerPrior(\parMat, \tilde{\parMat}) \Def \{\gpKerPrior(\Par_{\idxDesign}, \tilde{\Par}_m)\}_{1 \leq \idxDesign, m \leq \Ndesign} \in \R^{\Ndesign \times M}$, 
and $\gpKerPrior(\parMat) \Def \gpKerPrior(\parMat, \parMat) \in \R^{\Ndesign \times \Ndesign}$. Given this notation, 
the defining property of the GP can be written as 
\begin{align}
\func(\parMat) &\sim \Gaussian(\gpMeanPrior(\parMat), \gpKerPrior(\parMat)).
\end{align}
Now suppose that we have observed the (noiseless) function evaluations $\func(\designIn)$ at a set of \textit{design points} $\designIn$. 
It is well-known \cite{gpML} that the conditional distribution $\func|\func(\designIn)$ is also a GP
\begin{align}
\funcEm[\Ndesign] \Def \func|\func(\designIn) \sim \GP(\gpMean[\Ndesign], \gpKer[\Ndesign]), \label{generic_gp_conditional}
\end{align}
with mean and kernel given by 
\begin{align}
\gpMean[\Ndesign](\parMat) &= \gpMeanPrior(\parMat) + \gpKerPrior(\parMat, \designIn) \gpKerPrior(\designIn)^{-1} [\func(\designIn) - \gpMeanPrior(\designIn)] \label{kriging_eqns} \\
\gpKer[\Ndesign](\parMat) &= \gpKerPrior(\parMat) - \gpKerPrior(\parMat, \designIn) \gpKerPrior(\designIn)^{-1} \gpKerPrior(\designIn, \parMat). \nonumber
\end{align}
The predictive mean can be seen to interpolate the design points (i.e., $\gpMean[\Ndesign](\designIn) = \func(\designIn)$), with 
the predictive variance vanishing at these inputs (i.e., $\gpKer[\Ndesign](\designIn) = 0$). To break these interpolation properties,
a constant can be added to the diagonal of the kernel matrix so that $(\gpKerPrior(\designIn) + \nuggetSD^2 I)^{-1}$ replaces 
$\gpKer(\designIn)^{-1}$ in \ref{kriging_eqns}. In the present context, the maps $\fwd$ and $\llik$ are assumed to be deterministic functions 
of $\Par$ and hence the interpolation property is desirable. Nevertheless, a small, positive fixed constant $\nuggetSD^2$ is still 
typically included for numerical stability. 

Our default choice of covariance function is the popular \textit{exponentiated quadratic} 
(also called \textit{squared exponential} or \textit{Gaussian}) kernel, which is given by 
\begin{align}
\gpKerPrior(\Par, \tilde{\Par}) &= 
\margSD^2 \Exp{- \sum_{\idxParDim=1}^{\dimPar} \left(\frac{\Par_{\idxParDim} - \tilde{\Par}_{\idxParDim}}{\lengthscale_{\idxParDim}} \right)^2}, 
\end{align}
defined by the choice of positive hyperparameters $\margSD, \lengthscale_{1}, \dots, \lengthscale_{\dimPar}$. This is an example
of a stationary, or shift-invariant, kernel since $\gpKer(\Par, \tilde{\Par})$ only depends on its arguments through their difference 
$\Par - \tilde{\Par}$. To complete the GP prior specification, we typically take the mean 
function to be a constant or quadratic. We opt for the common empirical Bayes approach of fixing all mean and kernel hyperparameters 
at their maximum likelihood estimates. 

Finally, we consider the extension to multi-output functions $\func: \parSpace \to \R^{\dimObs}$. There is a large literature on multi-output 
kernels (\todo: cite), but we focus here on the simplest case in which each output is modeled separately as an independent GP. 
In this setting, the notation $\gpKerPrior(\Par)$ is now interpreted as the $\dimObs \times \dimObs$ diagonal matrix collecting the variance of each 
independent GP. Similarly, for a set of inputs $\parMat \in \R^{M \times \dimPar}$, $\gpKerPrior(\parMat)$ is now the 
$M\dimObs \times M\dimObs$ block diagonal matrix constructed from the predictive covariances of each independent GP. 

\subsection{Surrogate Construction}
We now consider modeling either $\fwd$ or $\llik$ with a GP, referring to these cases as \textit{forward model emulation} and 
\textit{log-likelihood} emulation, respectively. As mentioned previously, computationally costly forward models render 
standard inference algorithms intractable for solving Bayesian inverse problems due to the large number of model 
evaluations that must be conducted iteratively. The main idea of model emulation is to front-load the forward model 
runs, which can often be computed in parallel, to obtain a design 
$\{(\Par_{\idxDesign}, \fwd(\Par_{\idxDesign}))\}_{\idxDesign=1}^{\Ndesign}$ or 
$\{(\Par_{\idxDesign}, \llik(\Par_{\idxDesign}))\}_{\idxDesign=1}^{\Ndesign}$, which can then be used to train a model that 
approximates $\fwd$ or $\llik$. 

\subsubsection{Forward Model Emulation}
If the output dimension $\dimObs$ is reasonably small, then a common approach is to specify an independent 
multi-output GP prior directly for the forward model, $\fwd \sim \GP(\emMeanPrior{\fwd}, \emKerPrior{\fwd})$. After 
conditioning on the design $\{(\Par_{\idxDesign}, \fwd(\Par_{\idxDesign}))\}_{\idxDesign=1}^{\Ndesign}$, 
the predictive distribution $\fwdEm[\Ndesign] \sim \GP(\emMean[\Ndesign]{\fwd}, \emKer[\Ndesign]{\fwd})$ 
represents the emulator for $\fwd$. We emphasize that $\fwdEm[\Ndesign]$ is a \textit{stochastic} surrogate; 
the model output predictions at inputs $\parMat$ are given by the entire distribution 
$\fwdEm[\Ndesign](\parMat) \sim \Gaussian(\emMean[\Ndesign]{\fwd}(\parMat), \emKer[\Ndesign]{\fwd}(\parMat))$. 
Thus, the mean $\emMean[\Ndesign]{\fwd}(\parMat)$ provides a point estimate while the covariance 
$\emKer[\Ndesign]{\fwd}(\parMat)$ summarizes the uncertainty in the estimate. This is useful for the purpose
of uncertainty quantification, in that the additional uncertainty introduced as a consequence of the surrogate 
approximation can be propagated to the approximate posterior distribution. We note that plugging the random 
function $\fwdEm[\Ndesign]$ in place of $\fwd$ in the log-likelihood induces an approximation of the log-likelihood. 
In the case of the additive Gaussian likelihood \ref{llik_Gaussian} this approximation takes the form 
\begin{align}
\llikEmFwd(\Par) \Def \llik(\Par; \fwdEm) =  \log\Gaussian(\obs| \fwdEm[\Ndesign](\Par), \likPar). \label{induced_llik_emulator}
\end{align}
We emphasize two facts: (i.) the induced log-likelihood approximation $\llikEmFwd[\Ndesign](\Par)$ is also 
stochastic, but no longer Gaussian; and (ii.) no surrogate modeling is performed directly for $\llik$; instead, 
the approximation $\llikEmFwd[\Ndesign](\Par) = \llik(\Par; \fwdEm[\Ndesign])$ is derived by ``plugging in'' the 
forward model emulator. This represents a modular surrogate-modeling approach \cite{modularization}. 
Emulating expensive models using GPs is one of the central questions 
studied in the computer experiments literature; see \cite{gramacy2020surrogates} and 
\cite{design_analysis_computer_experiments} for detailed treatments. Challenges arise when the output
dimension $\dimObs$ is large or the outputs are highly interdependent, rendering the independent GP 
approach infeasible or ill-advised. For example, the output may correspond to the numerical solution of 
a differential equation and thus may take the form of a time series or spatiotemporal grid. 
A great deal of literature has addressed these challenges, including approaches tailored to forward models
with dynamical structure 
(e.g., \cite{GP_dynamic_emulation, Bayesian_emulation_dynamic, Liu_West_dynamic_emulation, dynamic_nonlinear_simulators_GP}).
A popular alternative is to first approximately express the high-dimensional output as a linear combination of basis vectors 
\begin{align}
\fwd(\Par) &= \sum_{\idxBasis=1}^{\dimBasis} \basisWeight_{\idxBasis}(\Par) \basisVec_{\idxBasis} + \noise_{\basisVec},
\end{align}
and then model the coefficients $\basisWeight_{\idxBasis}(\Par)$ using independent GPs. The basis vectors 
are typically found via a singular value decomposition, though other bases have been explored 
(\cite{HigdonBasis, emulate_functional_output, functionValuedModels, PODemulation}). The surrogate constructed in 
this fashion is simply a linear combination of independent GP emulators, and hence most of the results on forward model 
emulation in this paper are easily extended to this setting. See the appendix for details (\todo). 

\subsubsection{Log-Likelihood Emulation}
We observe in \ref{post_dens} that the forward model evaluations appear in the posterior density only through the 
likelihood. Thus, for the purposes of posterior inference, it is sufficient to directly approximate $\llik(\Par)$ without 
obtaining an approximation of the forward model. It is typically preferable to focus on approximating the logarithm 
of the likelihood, which usually results in a smoother response surface that is more amenable to GP emulation. 
The emulator construction proceeds as before; a GP prior $\llik \sim \GP(\emMeanPrior{\llik}, \emKerPrior{\llik})$ 
is specified and the predictive 
distribution $\llikEm[\Ndesign] \sim \GP(\emMean[\Ndesign]{\llik}, \emKer[\Ndesign]{\llik})$ obtained by 
conditioning on the design $\{(\Par_{\idxDesign}, \llik(\Par_{\idxDesign}))\}_{\idxDesign=1}^{\Ndesign}$. 
The use of GPs to approximate log-likelihoods has been widely used in approximate Bayesian inference 
(\cite{VehtariParallelGP, Kandasamy_2017, llikRBF, trainDynamics, quantileApprox, wang2018adaptive, landslideCalibration})
and Bayesian quadrature (\cite{BayesQuadrature, BayesQuadRatios}). In the context of inverse problems with 
high-dimensional output spaces, log-likelihood emulation has the clear benefit of reducing to a univariate GP setting. 
On the other hand, it has several downsides. Even on the log scale, log-likelihoods can be fast-varying and exhibit large 
dynamic range, proving troublesome for stationary GP priors \cite{wang2018adaptive}. A second potential weakness 
stems from the fact that log-likelihoods often have known bound constraints, which may not be respected by a GP 
prior or predictive distribution. Recent work has dealt with this problem by incorporating constraints into the 
GP hyperparameter optimization \cite{quantileApprox}. A third challenge follows from the fact that, in many applications, 
the likelihood parameters (e.g., $\likPar$) are typically not known and must also be learned from data. An obvious 
solution is to extend the input space of the emulator to include the likelihood parameters, though the resulting response surface 
may prove a more potent challenge for GP emulators. This approach is explored in \cite{llikRBF}. Alternatively, specific likelihood 
choices may admit a sufficient statistic which can be emulated independently of the likelihood parameters. This approach is employed 
in \cite{FerEmulation}, which focuses on the Gaussian likelihood \ref{llik_Gaussian} in the specific case $\likPar = \sigma^2 I$. In this 
case, a surrogate can be constructed to approximate the sufficient statistic $\norm{\obs - \fwd(\Par)}_2^2$, 
which does not depend on the likelihood parameter $\sigma^2$. 
Finally, we note that it may be difficult to incorporate prior domain knowledge 
into the GP prior for the log-likelihood, whereas in certain settings it is easier to do so when constructing a forward model 
surrogate \cite{GP_PDE_priors}. The paper \ref{VehtariParallelGP} finds that a GP with a quadratic mean function tends to 
work well for log-likelihood emulation, perhaps reflecting the fact that many log-likelihoods tend to be well-approximated by 
a quadratic at a coarse scale, with the GP kernel correcting for finer-scale variations. 
Despite all of these challenges, we emphasize that the reduction to a scalar-valued output space is a significant benefit which 
may outweigh the downsides of log-likelihood emulation in many applications. 

% Posterior Approximation
\section{Posterior Approximation}
The forward model and log-likelihood emulation approaches result in the random log-likelihood approximations 
$\llikEmFwd[\Ndesign]$ and $\llikEm[\Ndesign]$, respectively. The latter results in a Gaussian predictor 
$\llikEm[\Ndesign](\parMat) \sim \Gaussian(\emMean[\Ndesign]{\llik}(\parMat), \emKer[\Ndesign]{\llik}(\parMat))$ 
at any parameter set $\parMat$. On the other hand, the distribution of $\llikEmFwd[\Ndesign](\parMat)$ is 
typically non-Gaussian and will depend on the specific likelihood under consideration.
Plugging these log-likelihood surrogates into the posterior density expression \ref{post_dens} yields

\begin{align}
\fwdEmRdm[\Ndesign]{\postDensNorm}(\Par) 
&\Def \frac{1}{\fwdEmRdm[\Ndesign]{\normCst}} \fwdEmRdmDens[\Ndesign](\Par) 
\Def  \frac{1}{\fwdEmRdm[\Ndesign]{\normCst}} \priorDens(\Par) \Exp{\llikEmFwd[\Ndesign](\Par)} \label{post_dens_random_fwd} \\
\llikEmRdm[\Ndesign]{\postDensNorm}(\Par) 
&\Def \frac{1}{\llikEmRdm[\Ndesign]{\normCst}} \llikEmRdmDens[\Ndesign](\Par) 
\Def \frac{1}{\llikEmRdm[\Ndesign]{\normCst}} \priorDens(\Par) \Exp{\llikEm[\Ndesign](\Par)} \label{post_dens_random_llik},
\end{align}
with normalizing constants given by 
\begin{align}
\fwdEmRdm[\Ndesign]{\normCst}
&= \int \priorDens(\Par) \Exp{\llikEmFwd[\Ndesign](\Par)} d\Par \label{norm_cst_random_fwd} \\
\llikEmRdm[\Ndesign]{\normCst}
&= \int \priorDens(\Par) \Exp{\llikEm[\Ndesign](\Par)} d\Par \label{norm_cst_random_llik}. 
\end{align}
The unnormalized posterior densities $\fwdEmRdmDens[\Ndesign]$ and $\llikEmRdmDens[\Ndesign]$ are stochastic; 
each sample trajectory from the underlying GP induces a different posterior. 
The papers \cite{StuartTeck1, StuartTeck2} refer to this as the \textit{sample-based}
approximation of the posterior. Given that it is not obvious how to perform inference using these random objects, one typically 
proceeds by choosing a single, deterministic posterior approximation that somehow summarizes the uncertainty in the 
random posterior measure. Popular choices for such deterministic approximations are described below. Going forward, 
for succinctness we will only define the unnormalized posterior densities, with the associated normalizing constants  
implicitly defined. 

\subsection{Approximating the Unnormalized Posterior Density}
Arguably the simplest approach to constructing a deterministic approximation is to reduce the random emulator 
to a deterministic one by considering only the GP predictive mean. 
The plug-in mean estimators yield the unnormalized posterior density approximations 
\begin{align}
\fwdEmMeanDens[\Ndesign](\Par) &\Def \priorDens(\Par) \Exp{\llik(\Par; \emMean[\Ndesign]{\fwd}(\Par))}
 \label{post_dens_mean_fwd} \\
\llikEmMeanDens[\Ndesign](\Par) &\Def \priorDens(\Par) \Exp{\emMean[\Ndesign]{\llik}(\Par)} 
\label{post_dens_mean_llik}.
\end{align}
These approximations fail to account for the uncertainty introduced by the surrogate model, which can lead 
to overconfidence in downstream posterior inference. These mean-based approximations are analyzed 
theoretically in \cite{StuartTeck1}, with error bounds provided in Hellinger distance. In \ref{VehtariParallelGP}, 
the approximation \ref{post_dens_mean_llik} is considered and is instead referred to as the 
\textit{median approximation}, owing to the fact that the median formula for a log-normal random variable 
gives $\text{median}(\Exp{\llikEm[\Ndesign](\Par)}) = \Exp{\emMean[\Ndesign]{\fwd}(\Par)}$. In other words, 
the median of the likelihood emulator yields the same estimator as plugging in the mean of the 
\textit{log}-likelihood emulator.
The approximation $\llikEmMeanDens[\Ndesign](\Par)$ can be shown to be optimal in a certain  
$L^1$ sense \cite{VehtariParallelGP, StuartTeck2}. 

One approach to propagate the 
emulator uncertainty is to marginalize the likelihood with respect to the emulator. Doing so yields the approximations 
\begin{align}
\fwdEmMargDens[\Ndesign](\Par) &\Def \priorDens(\Par) \E_{\fwdEm}\left[\Exp{\llikEmFwd[\Ndesign](\Par)} \right] \label{post_dens_marg_fwd} \\
\llikEmMargDens[\Ndesign](\Par) &\Def \priorDens(\Par) \E_{\llikEm}\left[\Exp{\llikEm[\Ndesign](\Par)} \right], \label{post_dens_marg_llik}
\end{align}
with the expectation subscripts indicating integration with respect to the GP predictive distributions 
$\Gaussian(\emMean[\Ndesign]{\fwd}(\Par), \emKer[\Ndesign]{\fwd}(\Par))$ and
$\Gaussian(\emMean[\Ndesign]{\llik}(\Par), \emKer[\Ndesign]{\llik}(\Par))$, respectively. Note that these expectations are 
computed independently for each $\Par$; the GP predictive covariance structure is not considered. 
We refer to these posteriors as \textit{marginal approximations} following the terminology used in
 \cite{StuartTeck1, StuartTeck2}, which analyze the error of these approximations in the Gaussian likelihood setting. 

Marginal approximations can be justified in various ways. \todo

We now consider performing inference for the approximations \ref{post_dens_marg_fwd} and \ref{post_dens_marg_llik}. 
In the log-likelihood emulation setting, observe that the induced likelihood approximation is log-normally distributed
\begin{align}
\Exp{\llikEm[\Ndesign](\Par)} &\sim \LN(\emMean[\Ndesign]{\llik}(\Par), \emKer[\Ndesign]{\llik}(\Par)), 
\end{align}
which implies that \ref{post_dens_marg_llik} simplifies to 
\begin{align}
\llikEmMargDens[\Ndesign](\Par) 
&\Def \priorDens(\Par) \Exp{\emMean[\Ndesign]{\llik}(\Par) + \frac{1}{2} \emKer[\Ndesign]{\llik}(\Par)} 
\label{post_dens_marg_llik_simplified} \\
&= \llikEmMeanDens[\Ndesign](\Par) \Exp{\frac{1}{2} \emKer[\Ndesign]{\llik}(\Par)}. 
\label{post_dens_marg_var_inflation}
\end{align}
The final expression \ref{post_dens_marg_var_inflation} can be interpreted as a form of posterior inflation 
in regions where the emulator is uncertain. When $\emKer[\Ndesign]{\llik}(\Par) = 0$ the marginal approximation 
reduces to the mean approximation. While the unnormalized posterior density \ref{post_dens_marg_var_inflation}
can be plugged into off-the-shelf sampling algorithms, the uncertainty inflation term can cause highly multi-modal 
distributions which prove difficult for standard MCMC schemes. (\todo: add plot) This difficulty is noted in the 
supplement of \cite{VehtariParallelGP}. 

The expectation in the forward model emulation case 
\ref{post_dens_marg_fwd} is generally intractable, though pseudo-marginal MCMC methods offer a potential 
avenue for inference \cite{pseudoMarginalMCMC}. In the special case of a Gaussian likelihood \ref{llik_Gaussian}, 
the expectation admits the closed-form expression
\begin{align}
\fwdEmMargDens[\Ndesign](\Par) 
&= \priorDens(\Par) \ \E_{\fwdEm}\left[\Gaussian(\obs | \fwdEm[\Ndesign](\Par), \likPar) \right]
\nonumber \\
&= \priorDens(\Par) \ \Gaussian(\obs | \emMean[\Ndesign]{\fwd}(\Par), \likPar + \emKer[\Ndesign]{\fwd}(\Par)),
\label{post_dens_marg_fwd_Gaussian}
\end{align}
which follows immediately from the fact that the expectation is of the form of a convolution 
of two Gaussian densities (see appendix; \todo). The marginal approximation \ref{post_dens_marg_fwd_Gaussian}
can be interpreted as the posterior distribution of the modified Bayesian inverse problem 
\begin{align}
\obs &= \emMean[\Ndesign]{\fwd}(\Par) + v(\Par) + \noise \label{inv_prob_Gaussian_marginal_approx} \\
v(\Par) &\sim \Gaussian(0, \emKer[\Ndesign]{\fwd}(\Par)) \nonumber \\
\noise &\sim \Gaussian(0, \likPar) \nonumber \\
\Par &\sim \priorDens \nonumber, 
\end{align}
where $v(\Par)$ and $\noise$ are independent. It is interesting to note that the GP uncertainty in 
\ref{post_dens_marg_fwd_Gaussian} manifests in the form of additive variance inflation, distinct 
from the multiplicative uncertainty inflation in \ref{post_dens_marg_var_inflation} \cite{GP_PDE_priors}. 
The Gaussian posterior approximation \ref{post_dens_marg_fwd_Gaussian} has been considered in 
\cite{StuartTeck2, hydrologicalModel, GP_PDE_priors, GP_PDE_priors, CES, idealizedGCM, weightedIVAR}. 

Finally we note that generalizations of $\text{median}(\Exp{\llikEm[\Ndesign](\Par)})$ have been considered, 
resulting in the $\quantileProb$-quantile, $\quantileProb \in (0,1)$ approximations 
\begin{align}
\fwdEmQDens[\Ndesign](\Par) &\Def \priorDens(\Par) \quantile \left[\Exp{\llikEmFwd[\Ndesign](\Par)} \right] \label{post_dens_quantile_fwd} \\
\llikEmQDens[\Ndesign](\Par) &\Def \priorDens(\Par) \quantile \left[\Exp{\llikEm[\Ndesign](\Par)} \right]. \label{post_dens_quantile_llik}
\end{align}
The log-likelihood emulation case \ref{post_dens_quantile_llik} simplifies to 
\begin{align}
\llikEmQDens[\Ndesign](\Par) 
&= \priorDens(\Par) \Exp{\emMean[\Ndesign]{\llik}(\Par) + \GaussianCDF^{-1}(\quantileProb) \sqrt{\emKer[\Ndesign]{\llik}(\Par)}} 
\label{post_dens_quantile_llik_simplified} \\
&= \llikEmMeanDens[\Ndesign](\Par) \Exp{\GaussianCDF^{-1}(\quantileProb) \sqrt{\emKer[\Ndesign]{\llik}(\Par)}},
\label{post_dens_quantile_llik_var_inflation}
\end{align}
with $\GaussianCDF$ denoting the standard Gaussian distribution function. The approximate posterior 
\ref{post_dens_quantile_llik_simplified} is considered in \cite{VehtariParallelGP, quantileApprox}. Larger choices 
of $\alpha$ correspond to more conservative approximations in the sense of inducing a larger degree of posterior 
inflation in regions where the GP is uncertain. As compared with $\llikEmMeanDens[\Ndesign](\Par)$
\ref{post_dens_marg_var_inflation}, we notice that the quantile approximation \ref{post_dens_quantile_llik_var_inflation}
induces a ``softer'' form of uncertainty inflation, with the inflation factor scaling with the exponentiated GP standard 
deviation instead of the variance. 

\subsection{Approximate MCMC}
We now interpret the approximate posteriors introduced in the previous section as inducing different 
approximations of the Metropolis-Hastings (MH) acceptance probability. This perspective motivates 
an alternative approach to GP-induced posterior approximation; instead of first deriving an 
approximation of the unnormalized posterior density, we consider the approximation to the MCMC 
scheme as the starting point. We consider a class of MCMC algorithms which produce 
noisy approximations to the MH acceptance probability, and thus fall into the noisy MCMC 
framework which has been considered in the literature 
\cite{noisyMCMC, stabilityNoisyMH, noisyMCSurvey, pseudoMarginalMCMC}.  

\subsubsection{Deterministic Metropolis-Hastings Approximations}
We recall that the MH algorithm is defined by a proposal kernel, with density that we denote by 
$\propDens(\Par, \cdot)$. If the Markov chain is in the current state $\Par \in \parSpace$ then 
the next state is defined by sampling a proposal $\propPar \sim \propDens(\Par, \cdot)$ which is 
accepted with probability $\accProbMH(\Par, \propPar)$, defined by 

\begin{align}
&\accProbMH(\Par, \propPar) = 
\min\left\{1, \frac{\priorDens(\propPar) \Exp{\llik(\propPar)} \propDens(\propPar, \Par)}{\priorDens(\Par) \Exp{\llik(\Par)} \propDens(\Par, \propPar)} \right\}.
\label{MH_acc_prob_exact}
\end{align}
If accepted, the next state is defined to be $\propPar$, else it is set to the current state $\Par$. 
This procedure is summarized in \Cref{alg:MH}. For notational convenience in deriving approximations 
of the acceptance probability, we define 
\begin{align}
&\accProbRatio(\Par, \propPar) 
= \frac{\priorDens(\propPar) \Exp{\llik(\propPar)} \propDens(\propPar, \Par)}{\priorDens(\Par) \Exp{\llik(\Par)} \propDens(\Par, \propPar)}, 
&&\likRatio(\Par, \propPar) = \frac{\Exp{\llik(\propPar)}}{\Exp{\llik(\Par)}}.
\label{MH_acc_prob_exact}
\end{align}
and refer to $\likRatio(\Par, \propPar)$, $\accProbRatio(\Par, \propPar)$ and $\accProbMH(\Par, \propPar)$ 
as the likelihood ratio, acceptance ratio, and acceptance probability, respectively. 

\begin{algorithm}
    \caption{Metropolis-Hastings}
    \label{alg:MH}
    \begin{algorithmic}[1] % The number tells where the line numbering should start
    \Function{MH}{$\indexMCMC[0]{\Par}, \NMCMC$}     
        \For{$\mcmcIndex \gets1,\NMCMC$} 
        		\State $\propPar \sim \propDens(\indexMCMC{\Par}, \cdot)$ \Comment{Proposal}
		\State $\accProbMH(\indexMCMC{\Par}, \propPar) \gets \min\left\{1, \frac{\priorDens(\propPar)\Exp{\llik(\propPar)}
				\propDens(\propPar, \indexMCMC{\Par})}{\priorDens(\indexMCMC{\Par}) \Exp{\llik(\indexMCMC{\Par})}  \propDens(\indexMCMC{\Par}, \propPar)} \right\}$
		\State $b \sim \text{Bernoulli}(\accProbMH(\indexMCMC{\Par}, \propPar))$
		\If{$b = 1$}
			\State $\indexMCMC[\mcmcIndex+1]{\Par} \gets \propPar$ 
		\Else
			\State $\indexMCMC[\mcmcIndex+1]{\Par} \gets \indexMCMC{\Par}$
		\EndIf
	\EndFor
	\EndFunction
    \end{algorithmic}
\end{algorithm}

We can define approximate surrogate-based MCMC algorithms by utilizing $\llikEm[\Ndesign]$ 
or $\llikEmFwd[\Ndesign]$ to approximate $\accProbMH(\Par, \propPar)$, either directly or by plugging 
in approximations of $\accProbRatio(\Par, \propPar)$ or $\likRatio(\Par, \propPar)$.  
For brevity, we focus on the log-likelihood emulation setting here, as the definitions are similar in the forward 
model emulation case. 
We thus consider the random variables $\Em[\Ndesign]{\likRatio}(\Par, \propPar)$,
$\Em[\Ndesign]{\accProbRatio}(\Par, \propPar)$, and 
$\Em[\Ndesign]{\accProbMH}(\Par, \propPar)$ defined by inserting the emulated log-likelihoods 
$\llikEm[\Ndesign](\propPar)$ and $\llikEm[\Ndesign](\Par)$ in place of their exact counterparts
in \ref{MH_acc_prob_exact}. The mean, marginal, and quantile approximate posteriors introduced in the previous 
section correspond to the following deterministic, plug-in estimates of $\Em[\Ndesign]{\likRatio}(\Par, \propPar)$:
\begin{align}
\llikEmMean[\Ndesign]{\likRatio}(\Par, \propPar) 
\coloneqq \Exp{\emMean[\Ndesign]{\llik}(\propPar) - \emMean[\Ndesign]{\llik}(\Par)} \\
\llikEmMarg[\Ndesign]{\likRatio}(\Par, \propPar) 
\Def \llikEmMean[\Ndesign]{\likRatio}(\Par, \propPar) 
\Exp{\frac{1}{2}\left[\emKer[\Ndesign]{\llik}(\propPar) - \emKer[\Ndesign]{\llik}(\Par)\right]} \\
\llikEmQ[\Ndesign]{\likRatio}(\Par, \propPar) 
\Def \llikEmMean[\Ndesign]{\likRatio}(\Par, \propPar) 
\Exp{\GaussianCDF^{-1}(\quantileProb)  \left[\sqrt{\emKer[\Ndesign]{\llik}(\propPar)} - \sqrt{\emKer[\Ndesign]{\llik}(\Par)}\right]}.
\end{align}
Notice that the marginal and quantile approximations inflate the probability of acceptance when the 
uncertainty at the proposed state $\propPar$ is greater than at the current state $\Par$.
All three of these approximations are derived by independently approximating the numerator and denominator 
in the ratio $\Exp{\llik(\propPar)} / \Exp{\llik(\Par)}$ and then dividing the two approximations.
However, the GP emulator $\llikEm[\Ndesign]$ is a random function, meaning that 
$\llikEm[\Ndesign](\propPar)$ and $\llikEm[\Ndesign](\Par)$ are typically correlated, the covariance
being given by $\emKer[\Ndesign]{\llik}(\propPar, \Par)$. It is therefore natural to consider incorporating 
this information into approximations of the likelihood ratio. We define a second marginal approximation 
by marginalizing $\Em[\Ndesign]{\likRatio}(\Par, \propPar)$ with respect to the \textit{joint} 
distribution of $[\llikEm[\Ndesign](\propPar), \llikEm[\Ndesign](\Par)]$, which gives
\begin{align}
\llikEmJointMarg[\Ndesign]{\likRatio}(\Par, \propPar) 
&\Def \E_{\llikEm[\Ndesign]} \left[\Em[\Ndesign]{\likRatio}(\Par, \propPar) \right] \\
&= \llikEmMean[\Ndesign]{\likRatio}(\Par, \propPar)  \Exp{\frac{1}{2} \Var\left[\llikEm[\Ndesign](\propPar) - \llikEm[\Ndesign](\Par) \right]} \\
&=  \llikEmMean[\Ndesign]{\likRatio}(\Par, \propPar)  
\Exp{\frac{1}{2}\left[\emKer[\Ndesign]{\llik}(\propPar) + \emKer[\Ndesign]{\llik}(\Par) - 2\emKer[\Ndesign]{\llik}(\propPar, \Par) \right]}. 
\label{lik_ratio_joint_marg}
\end{align} 
The joint marginal approximation inflates the acceptance probability when the GP is uncertain about the 
\textit{difference} in log-likelihood values at the locations $\propPar$ and $\Par$. For example, it is possible
that the marginal uncertainty at each location is large, but a large covariance between the locations 
might imply little uncertainty in the difference $\llikEm[\Ndesign](\propPar) - \llikEm[\Ndesign](\Par)$. 

The marginal approximation can be taken a step further by integrating $\llikEm$ out of 
$\Em[\Ndesign]{\accProbRatio}(\Par, \propPar)$ or $\Em[\Ndesign]{\accProbMH}(\Par, \propPar)$ instead of 
$\Em[\Ndesign]{\likRatio}(\Par, \propPar)$. The former case 
\begin{align}
\llikEmJointMarg[\Ndesign]{\accProbRatio}(\Par, \propPar) 
&= \frac{\priorDens(\propPar) \propDens(\propPar, \Par)}{\priorDens(\Par) \propDens(\Par, \propPar)} 
\llikEmJointMarg[\Ndesign]{\likRatio}(\Par, \propPar)
\label{acc_ratio_joint_marg}
\end{align}
follows immediately from \ref{lik_ratio_joint_marg}, 
given that $\accProbRatio(\Par, \propPar)$ differs from $\likRatio(\Par, \propPar)$ only by a multiplicative 
constant (with respect to the emulator). Thus, the approximations 
$\llikEmJointMarg[\Ndesign]{\accProbRatio}(\Par, \propPar)$ and 
$\llikEmJointMarg[\Ndesign]{\likRatio}(\Par, \propPar)$ induce identical inference algorithms. 
Marginalizing the acceptance probability directly proves more interesting, yielding
\begin{align}
\llikEmJointMarg[\Ndesign]{\accProbMH}(\Par, \propPar) 
&\Def \E_{\llikEm[\Ndesign]} \left[\min\left\{1, \Em[\Ndesign]{\accProbRatio}(\Par, \propPar)\right\} \right] \\
&= w_1 + w_2 \llikEmJointMarg[\Ndesign]{\accProbRatio}(\Par, \propPar),
\label{acc_prob_joint_marg}
\end{align} 
where 
\begin{align*}
w_1 &= \Prob(\Em[\Ndesign]{\accProbRatio}(\Par, \propPar) \geq 1) \\
w_2 &= \Prob\left(\Em[\Ndesign]{\accProbRatio}(\Par, \propPar) \leq \Exp{-\Var\left[\llikEm[\Ndesign](\propPar) - \llikEm[\Ndesign](\Par) \right]}\right).
\end{align*}
This is derived in \Cref{prop:joint-marg-accept-prob} in the appendix. While the plug-in acceptance probability 
approximation based on $\llikEmJointMarg[\Ndesign]{\accProbRatio}(\Par, \propPar)$ yields 
$\min\left\{1, \llikEmJointMarg[\Ndesign]{\accProbRatio}(\Par, \propPar) \right\}$, 
$\llikEmJointMarg[\Ndesign]{\accProbMH}(\Par, \propPar)$ instead takes the form of a linear combination of 
$1$ and $\llikEmJointMarg[\Ndesign]{\accProbRatio}(\Par, \propPar)$, with weights given by the probabilities $w_1$, $w_2$.


\subsubsection{Noisy Metropolis-Hastings Approximations}
The previous section considered deterministic approximations to the MH acceptance probability. Here we turn 
to random approximations, thus entering the realm of so-called ``noisy'' MCMC algorithms 
\cite{noisyMCMC, stabilityNoisyMH, noisyMCSurvey, pseudoMarginalMCMC}. The general framework,
summarized in \Cref{alg:noisy-MH}, is characterized by the choice of distribution $\llikSampDist_{\Par, \propPar}$
from which to sample log-likelihood values $\llikSamp$ and $\llikSampProp$ which replace the exact 
log-likelihood evaluations $\llik(\Par)$ and $\llik(\propPar)$ when computing the MH acceptance probability. 
For brevity we will continue to specialize the definitions to the log-likelihood emulation setting, but the forward model 
emulation case is nearly identical, with $\llikEmFwd$ replacing $\llikEm$. 

\begin{algorithm}
    \caption{Noisy Metropolis-Hastings}
    \label{alg:noisy-MH}
    \begin{algorithmic}[1] % The number tells where the line numbering should start
    \Function{noisyMH}{$\indexMCMC[0]{\Par}, \NMCMC, \propDens, \llikSampDist$}     
        \For{$\mcmcIndex \gets1,\NMCMC$} 
        		\State $\propPar \sim \propDens(\indexMCMC{\Par}, \cdot)$ 
		\State $(\llikSamp, \llikSampProp) \sim \llikSampDist_{\indexMCMC{\Par}, \propPar}$  \Comment{Sample log-likelihood values}
		\State $\accProbMH(\indexMCMC{\Par}, \propPar) \gets \min\left\{1, \frac{\priorDens(\propPar)e^{\llikSampProp} 
				\propDens(\propPar, \indexMCMC{\Par})}{\priorDens(\indexMCMC{\Par}) e^{\llikSamp}  \propDens(\indexMCMC{\Par}, \propPar)} \right\}$
		\State $b \sim \text{Bernoulli}(\accProbMH(\indexMCMC{\Par}, \propPar))$
		\If{$b = 1$}
			\State $\indexMCMC[\mcmcIndex+1]{\Par} \gets \propPar$ 
		\Else
			\State $\indexMCMC[\mcmcIndex+1]{\Par} \gets \indexMCMC{\Par}$
		\EndIf
	\EndFor
	\EndFunction
    \end{algorithmic}
\end{algorithm}

We define the following noisy MCMC algorithms based on different choices for $\llikSampDist_{\Par, \propPar}$.
Below we consider three such choices. In each case, we show that the transition kernel defining the Markov chain 
of the respective algorithm coincides with that of one of the deterministic approximations considered in the previous 
section. Thus, the noisy MCMC algorithms provide alternative inference algorithms for sampling from the previously 
considered approximate posteriors. Moreover, in the forward model emulation setting, they also generalize to 
situations in which the deterministic approximations yield intractable integrals. For example, these algorithms are 
not restricted to the setting of a Gaussian likelihood.  
We use the notation $\parMat \Def \{\Par, \propPar\}$ in the below definitions. 

\paragraph{Joint Monte Carlo Within Metropolis-Hastings}
We first consider setting $\llikSampDist_{\Par, \propPar}$ to be the joint distribution of 
$[\llikEm[\Ndesign](\Par), \llikEm[\Ndesign](\propPar)]$: 
\begin{align}
\llikSampDist_{\Par, \propPar} = \Gaussian(\emMean[\Ndesign]{\llik}(\parMat), \emKer[\Ndesign]{\llik}(\parMat)).
\label{llik_dist_MCWMH-joint}
\end{align}
We call the resulting algorithm \textit{Joint Monte Carlo within Metropolis-Hastings (MCWMH-joint)}. 
Like the deterministic joint marginal approximations \ref{lik_ratio_joint_marg} and \ref{acc_prob_joint_marg}, 
the \textit{MCWMH-joint} algorithm takes into account the GP covariance structure. Note that since the log-likelihood
values are sampled independently across iterations, then the \textit{MCWMH-joint} does indeed define a valid 
Markov chain. The transition kernel of this Markov chain is given by 
\begin{align}
\llikEmJointMCWMH[\Ndesign]{\MarkovKernel}(\Par, A) 
&= \int_{A} \propDens(\Par, \propPar) \llikEmJointMarg[\Ndesign]{\accProbMH}(\Par, \propPar) d\propPar 
+ \delta_{\Par}(A) \left[1 - \int_{\parSpace} \propDens(\Par, \propPar) \llikEmJointMarg[\Ndesign]{\accProbMH}(\Par, \propPar) d\propPar \right],
\label{MCWMH-joint-kernel}
\end{align}
which is derived in the appendix, \Cref{transition_kernel_derivations}. This is precisely the transition kernel of the Markov chain 
defined by the deterministic joint marginal approximation of the acceptance probability \ref{acc_prob_joint_marg}. 

\paragraph{Independent Monte Carlo Within Metropolis-Hastings}
An alternative is to sample the log-likelihood values independently, without considering the GP covariance; i.e., 
\begin{align}
\llikSampDist_{\Par, \propPar} 
= \Gaussian\left(\emMean[\Ndesign]{\llik}(\parMat), \text{diag}\left\{\emKer[\Ndesign]{\llik}(\Par), \emKer[\Ndesign]{\llik}(\propPar)\right\}\right).
\label{llik_dist_MCWMH-ind}
\end{align}
We refer to the resulting algorithm as \textit{Independent Monte Carlo Within Metropolis-Hastings (MCWMH-ind)}. Analogously to the 
joint case, the transition kernel of this algorithm is given by \ref{MCWMH-joint-kernel} but with the marginal acceptance probabilities computed
with respect to $\Gaussian\left(\emMean[\Ndesign]{\llik}(\parMat), \text{diag}\left\{\emKer[\Ndesign]{\llik}(\Par), \emKer[\Ndesign]{\llik}(\propPar)\right\}\right)$
in place of the full joint distribution. The \textit{MCWMH-ind} algorithm is utilized to calibrate expensive ecosystem models 
in \cite{FerEmulation}. 

\paragraph{Pseudo-Marginal Metropolis-Hastings}
Note that the two previous algorithms produce new log-likelihood samples at both the current $\Par$ and proposed 
$\propPar$ locations at each iteration of the algorithm. An alternative is to draw a new sample only at the 
proposed location, while recycling the sample at the current location from the previous iteration. Formally, this procedure
implies the product measure 
\begin{align}
\llikSampDist_{\Par, \propPar} 
= \delta_{\llikSamp} \otimes  
\Gaussian(\emMean[\Ndesign]{\llik}(\propPar), \emKer[\Ndesign]{\llik}(\propPar)), \label{llik_dist_pseudo_marg}
\end{align}
which precisely falls into the pseudo-marginal MCMC framework \cite{pseudoMarginalMCMC}. We thus term 
this algorithm \textit{Pseudo-Marginal Metropolis-Hastings (MH-pseudo-marg)}. Given a log-likelihood sample 
$\llikSamp$, by definition the unnormalized posterior approximation $\priorDens(\Par) \Exp{\llikSamp}$ is 
an unbiased estimator of 
$\llikEmMargDens[\Ndesign](\Par) = \E_{\llikEm[\Ndesign]}\left[\priorDens(\Par) \Exp{\llikEm[\Ndesign](\Par)} \right]$. 
This implies that the \textit{MH-pseudo-marg} algorithm defines a Markov chain with target distribution 
$\llikEmMargDens[\Ndesign]$, which was already defined in \ref{post_dens_marg_llik}. 


% Sequential Design
\section{Sequential Design}
In applications, the GP emulator is often constructed sequentially. In this section, we consider augmenting the 
initial design $\designIn[\Ndesign]$ with new input points that are chosen to yield maximal improvement in the 
GP-induced posterior approximation. This can be thought of as a question of experimental design for the 
solution of a Bayesian inverse problem. We begin by reviewing the relevant background on GP-based 
sequential design, and then proceed to discuss design methods that are tailored to the goal of 
posterior approximation.  

\subsection{Background: Sequential Design for Gaussian Processes}

\subsubsection{The Sequential Design Loop}
Sequential design algorithms for GPs have been studied extensively across several fields. We introduce this topic 
for a generic GP predictive distribution $\funcEm[\Ndesign] \Def \func|\func(\designIn)$, as defined in 
\ref{generic_gp_conditional}. To be clear, this notation indicates that the GP prior 
$\func \sim \GP(\gpMeanPrior, \gpKerPrior)$ has already been conditioned on the \textit{initial design}
$\design[\Ndesign] \Def \{\designIn, \func(\designIn)\}$. We now consider the question of identifying a new set of $\Nbatch$
inputs $\designBatchIn$ to augment the initial design. We denote the new design by 
$\design[\Naugment] \Def \{\designIn[\Naugment], \func(\designIn[\Naugment])\}$, where 
 $\designIn[\Naugment] \Def \designIn \cup \designBatchIn$. 
Conditioning on the augmented design results in the updated GP
\begin{align}
\funcEm[\Naugment] \Def \func|\design[\Naugment] = \funcEm[\Ndesign] | \func(\designIn[\Nbatch])   \sim \GP(\gpMean[\Naugment], \gpKer[\Naugment]),
\end{align}
where the predictive mean $\gpMean[\Naugment]$ and covariance $\gpKer[\Naugment]$ functions are given by \ref{kriging_eqns}
with $\{\designIn, \func(\designIn)\}$ replaced by $\{\designIn[\Naugment], \func(\designIn[\Naugment])\}$. This update 
can be computed in $\BigO(\Nbatch^3)$ operations, a significant improvement over the naive $\BigO([\Ndesign + \Nbatch]^3)$
implementation (\todo: add appendix on fast GP updates). Naturally, the new batch $\designBatchIn$ should be chosen to 
yield maximal improvement in the GP model $\funcEm[\Naugment]$ with respect to the downstream task that is to be performed. 
This idea naturally leads to an optimization problem of the form 
\begin{align}
\designBatchIn = \argmin_{\designBatchIn^\prime \subset \parSpace} \acq[\Ndesign](\designBatchIn^\prime), \label{acq_func_opt}
\end{align}
where $\acq[\Ndesign]: \parSpace^{\Nbatch} \to \R$ is an \textit{acquisition function} (i.e., \textit{design criterion}) that 
encodes the quality of a batch of design points in terms of the task at hand. Iterating this procedure $\Nrounds$ times yields the so-called 
\textit{sequential design} (a.k.a., \textit{active learning}) loop, summarized in \Cref{alg:seq_des_loop}.  

\begin{algorithm}
    \caption{Gaussian Process Sequential Design Loop}
    \label{alg:seq_des_loop}
    \begin{algorithmic}[1] % The number tells where the line numbering should start
    \Function{seqDesign}{$\design[\Ndesign], \Nrounds$}     
    	\State $\design[] \gets \design[\Ndesign]$ \Comment{Initial Design}
	\State $\hat{f} \gets f|\design[] \sim \GP(\gpMean[\Ndesign], \gpKer[\Ndesign])$
        \For{$\designIndex \gets 1,\Nrounds$} \Comment{Sequential Design Loop}
        		\State $\designBatchIn \gets \argmin_{\designBatchIn^\prime \subset \parSpace} \acq[\Ndesign](\designBatchIn^\prime)$ 
		\State $\design[] \gets \design[] \cup \{\designBatchIn, \func(\designBatchIn)\}$
		\State $\hat{f} \gets \hat{f} | \design[]$
	\EndFor
	\EndFunction
    \end{algorithmic}
\end{algorithm}

One could of course consider 
varying the batch size $\Nbatch$ across the $\Nrounds$ iterations, but to simplify notation we keep $\Nbatch$ constant. We note that the 
special case $\Nbatch = 1$ corresponds to the pure sequential design setting, in which the function $\func$ is evaluated at each acquired 
point before considering the subsequent acquisition. The \textit{batch} sequential design setting $\Nbatch > 1$ is more challenging,  
owing to the fact that $\func$ is only evaluated after an entire batch $\designBatchIn$ has been acquired, meaning that inputs must 
be selected without observing the outputs of the other points in the batch. The applications motivating this work render the 
pure sequential design setting completely infeasible, as a single function evaluation $f(\Par)$ may require thousands of CPU hours. 
However, computing resources often allow $\BigO(10^2)$ function evaluations to be performed in parallel. Such constraints imply a 
``large batch, few iterations'' setting which poses a greater challenge than the one-at-a-time pure sequential design that is 
often considered in the literature.

\subsubsection{Acquisition Functions}
We now summarize some particular choices of acquisition function $\acq[\Ndesign]$ which are popular in practice. These will form 
the basis for extensions that target the specific goal of GP-induced posterior approximation. We begin by emphasizing that the 
choice of acquisition function should reflect how the GP is being used in downstream tasks. Perhaps the most common goal is that 
of optimization. The field of \textit{Bayesian optimization} is dedicated to this question, which has produced a wide variety
acquisition functions that are tailored to the task of optimizing expensive black-box functions $\func$; 
see \cite{reviewBayesOpt, gramacy2020surrogates} for in-depth reviews.  
The related field of \textit{Bayesian quadrature} \cite{BayesQuadrature, BayesQuadrature, BayesQuadRatios}
instead focuses on the goal of approximating integrals with expensive integrands. The computer experiments community 
focuses on various tasks, including contour estimation and reliability analysis \cite{contourEstimation, cole2021entropybased}. 
Perhaps the most common goal considered in the sequential design of computer experiments is to fit a response surface
(see \cite{gramacy2020surrogates, SanterCompExp, design_analysis_computer_experiments} for reviews), 
which typically leads to design procedures 
that seek to ``evenly'' fill the input space $\parSpace$ in some sense.  
In such settings, a reasonable approach is to choose points in locations where the GP is most uncertain. 
Defining ``uncertainty'' in terms 
of variance or entropy yields the \textit{maximum variance (max-var)} and \textit{maximum entropy (max-ent)} acquisition functions, 
respectively, 
\begin{align}
&\labelAcq[\Ndesign]{\maxvarLabel}(\Par) \Def -\Var[\funcEm[\Ndesign](\Par)] , 
&&\labelAcq[\Ndesign]{\maxentLabel}(\designBatchIn) \Def -\Ent[\funcEm[\Ndesign](\designBatchIn)], \label{acq_max_var_ent}
\end{align} 
where $\Ent[Z]$ denotes the entropy of a random variable $Z$. The variance and entropy are negated to align with our convention 
of minimizing acquisition functions. The $\maxvarLabel$ criterion does not have an obvious generalization to the batch setting, 
so is defined to only accept a single input $\Par$. A noted weakness of these criteria is their locality; 
they target the single point (sets) with maximal uncertainty, ignoring the effect of the acquisition on the remainder of the design space. 
This often favors points at the boundaries of $\parSpace$, behavior that is typically not desirable \cite{gramacy2020surrogates}. 
To address this weakness, an alternative approach is to minimize the \textit{conditional} uncertainty 
($\Var[\funcEm[\Ndesign](\Par) | \designBatchIn]$ or $\Ent[\funcEm[\Ndesign](\Par) | \designBatchIn]$), \textit{averaged} over 
the input space $\parSpace$. The resulting \textit{integrated uncertainty criteria}
\begin{align}
\labelAcq[\Ndesign]{\intvarLabel}(\designBatchIn)
&= \int_{\parSpace} \Var[\funcEm[\Ndesign](\Par) | \designBatchIn] \weightDens(\Par) d\Par, \label{acq_int_var} \\ 
\labelAcq[\Ndesign]{\intentLabel}(\designBatchIn)
&= \int_{\parSpace} \Ent[\funcEm[\Ndesign](\Par) | \designBatchIn] \weightDens(\Par) d\Par, \label{acq_int_ent}
\end{align}
are naturally defined in the batch setting and account for the global effect of acquiring new design points. Here $\weightDens$ is 
a probability density on $\parSpace$, allowing for the expectation over $\parSpace$ to weighted. 
A nice summary and analysis of integrated variance criteria of the form \ref{acq_int_var} is given 
in \cite{Mercer_kernels_IVAR}, with an emphasis is on one-shot, rather than sequential, design. 
Since $\Var[\funcEm[\Ndesign](\Par) | \designBatchIn] = \gpKer[\Naugment](\Par)$ and 
$\Ent[\funcEm[\Ndesign](\Par) | \designBatchIn] = \frac{1}{2} \log(2\pi e \gpKer[\Naugment](\Par))$, the integrands of 
\ref{acq_int_var} and \ref{acq_int_ent} are available in closed-form. However, outside of special choices for $\parSpace$ and 
$\weightDens$ (see, e.g., \cite{Binois_2018}), the integrals are typically replaced with Monte Carlo approximations. It is worth noting that 
these acquisition functions must be computed without observing the function evaluations $\func(\designBatchIn)$, which is 
not an issue for any of the above acquisitions given that $\gpKer[\Naugment](\Par)$ is independent of $\func(\designBatchIn)$ 
(see \ref{kriging_eqns}). However, the predictive mean $\gpMean[\Naugment](\Par)$ does depend on $\func(\designBatchIn)$, 
a fact which will become relevant in the subsequent section. We therefore state a lemma below which summarizes the 
dependence of the predictive mean on $\func(\designBatchIn)$, under the assumption that the latter is distributed according 
to the current GP $\funcEm[\Ndesign]$. This result is also provided in \cite{VehtariParallelGP}. 
\begin{lemma} \label{lemma:pred-mean-dist}
Let $(\designBatchIn, \designBatchFunc)$ be a new batch of $\Nbatch$ design points, with 
$\designBatchFunc \sim \Gaussian(\gpMean[\Ndesign](\designBatchIn), \gpKer[\Ndesign](\designBatchIn))$. 
Let  
\begin{align}
\gpMean[\Naugment](\parMat | \designBatchFunc) 
\Def \E[\funcEm[\Ndesign](\parMat) | (\designBatchIn, \designBatchFunc)] \label{conditional-mean-notation}
\end{align}
denote the predictive mean at inputs $\parMat$ after conditioning $\funcEm[\Ndesign]$ on 
$(\designBatchIn, \designBatchFunc)$. Then, the predictive mean $\gpMean[\Naugment](\parMat)$
(viewed as a function of the random variable $\designBatchFunc$), satisfies 
\begin{align}
\gpMean[\Naugment](\parMat | \designBatchFunc)
\sim \Gaussian\left(\gpMean[\Naugment](\parMat | \gpMean[\Ndesign](\designBatchIn)), 
                               \gpKer[\Ndesign](\parMat) - \gpKer[\Naugment](\parMat) \right) \label{pred-mean-dist},
\end{align}
\end{lemma}
We see that the mean (with respect to $\designBatchFunc$) of $\gpMean[\Naugment](\parMat | \designBatchFunc)$ is equal to the mean of
the conditional 
$\funcEm[\Ndesign](\parMat) | (\designBatchIn, \gpMean[\Ndesign](\designBatchIn))$, which treats the current 
predictive mean $\gpMean[\Ndesign](\designBatchIn)$ as if it were the true response; i.e., the GP $\funcEm[\Ndesign]$
has been conditioned on the ``pseudo-data'' $(\designBatchIn, \gpMean[\Ndesign](\designBatchIn))$. 
The variance of 
$\gpMean[\Naugment](\parMat | \designBatchFunc)$ is equal to the change in GP variance at the inputs $\parMat$ due to the act 
of conditioning on the batch $\designBatchIn$. Therefore, the uncertainty in $\gpMean[\Naugment](\parMat | \designBatchFunc)$ 
is large when the design points $\designBatchIn$ are more ``influential.''

\subsection{Goal-Oriented Acquisition Functions: Posterior Approximation}
The previous section introduced several acquisition functions that seek to improve GP predictions by targeting
design point batches that cause large reductions of uncertainty in the GP predictive distribution. While such criteria may be appropriate 
when good predictions are required across all of $\parSpace$, this may not align well with the goal of constructing a GP emulator 
which induces a quality posterior approximation. In the Bayesian inverse problem setting, it has repeatedly been noted that the 
posterior is often highly concentrated relative to the prior. Intuitively, one would therefore expect that the GP emulator need only 
achieve a high predictive accuracy in the region with high posterior mass, and avoid wasteful acquisition of design points in 
regions with negligible mass. This intuition is placed on more rigorous ground in \cite{StuartTeck2}, in which bounds on the 
posterior error are given in terms of the likelihood approximation error in a $L^2_{\postDens}(\parSpace)$ sense. The authors 
stated motivation was to provide rigorous justification for the use of ensemble Kalman methods as a cheap experimental 
design method for the training of GP emulators \cite{CES, idealizedGCM}. These so-called ensemble Kalman inversion methods 
often require $\BigO(10^3)$ sequential forward model evaluations, a significant improvement over MCMC but still intractable for 
forward models of significant computational expense, such as those that motivated the present work. 

We instead focus on design methods that fit within the sequential design framework summarized in \Cref{alg:seq_des_loop}, and 
are tailored to the goal of posterior approximation. The main idea is to define acquisition functions based on 
the unnormalized posterior density emulator ($\fwdEmRdm[\Ndesign]{\postDens}(\Par)$ or $\llikEmRdmDens[\Ndesign](\Par)$), 
rather than the underlying GPs themselves. The problem of GP-based sequential design tailored to posterior approximation 
has gained recent attention, and is addressed in 
\cite{SinsbeckNowak, Surer2023sequential, VehtariParallelGP, briol2017sampling, ranjan2016inverse, 
	landslideCalibration, KandasamyActiveLearning2015, Kandasamy_2017, wang2018adaptive,   
	weightedIVAR, quantileApprox, hydrologicalModel, briol2017sampling}. 
[\todo: need to look at the method used in \cite{quantileApprox, hydrologicalModel} more closely.]
As an initial example, we consider tailoring the $\maxvarLabel$ and $\maxentLabel$ to the posterior approximation goal in 
the log-likelihood emulation setting. Their natural analogs in this setting are 
\begin{align}
&\labelAcq[\Ndesign]{\maxexpvarLabel}(\Par) \Def -\Var[\llikEmRdmDens[\Ndesign](\Par)] , 
&&\labelAcq[\Ndesign]{\maxexpentLabel}(\Par) \Def -\Ent[\llikEmRdmDens[\Ndesign](\Par)]. \label{acq_max_exp_var_ent}
\end{align} 
The ``exp'' in the naming convention highlights that these criteria are being applied to the exponentiated GP 
$\Exp{\llikEm(\Par)}$ (ignoring the scaling by the prior). Thus, many of the acquisition functions considered here can be 
generically viewed as the analogs of existing GP acquisition functions for log-normal processes (LNPs). Since the 
variance and entropy of a log-normal random variable are well-known, the acquisitions \ref{acq_max_exp_var_ent}
are easily computable in closed-form. The $\maxexpvarLabel$ acquisition is considered in 
\cite{, KandasamyActiveLearning2015, Kandasamy_2017}, 
while $\maxexpentLabel$ is utilized in \cite{wang2018adaptive, landslideCalibration}. 
[\todo: discuss generalization to batch setting]. 

\subsubsection{Expected Integrated Variance}
We now focus on adapting the integrated variance criterion \ref{acq_int_var} to LNPs. 
In the log-likelihood emulation 
setting, we will be able to derive a closed-form integrand, as was the case in \ref{acq_int_var}. In the special case 
of a Gaussian likelihood, the analogous criterion in the forward model emulation setting also admits a closed-form 
integrand. 

\paragraph{Log-Likelihood Emulation}
We begin with the log-likelihood emulation case. Recall that the 
$\intvarLabel$ criterion relied on the GP property that the predictive variance $\emKer[\Naugment]{\llik}(\Par)$
does not depend on the response $\llik(\designBatchIn)$. This is no longer the case for 
$\Var[\llikEmRdmDens[\Naugment](\Par)]$, given that this quantity depends on $\emMean[\Naugment]{\llik}(\Par)$, 
which in turn depends on $\llik(\designBatchIn)$ (see \ref{kriging_eqns}). 
However, under the current GP model we have that 
$\llik(\designBatchIn) \sim \Gaussian(\emMean[\Ndesign]{\llik}(\designBatchIn), \emKer[\Ndesign]{\llik}(\designBatchIn))$.
The dependence of $\emMean[\Naugment]{\llik}(\Par)$ on $\llik(\designBatchIn)$ is summarized in the below 
result, which is also given in \cite{VehtariParallelGP}.
The above lemma allows us to 
average over the uncertainty in $\Var[\llikEmRdmDens[\Naugment](\Par)]$ stemming from the random 
quantity $\designBatchLlik$. 

\begin{lemma} \label{lemma:evar}
Under the same assumptions as \Cref{lemma:pred-mean-dist}, it follows that 
\begin{align}
\E_{\designBatchLlik} \Var[\llikEmRdmDens[\Naugment](\Par)]
&= \Var[\llikEmRdmDens[\Ndesign](\Par) | \emMean[\Ndesign]{\llik}(\designBatchIn)]
	\varInflation(\Par; \designBatchIn), \label{evar-expression}
\end{align}
where 
\begin{align}
\varInflation(\Par; \designBatchIn)
&= \Exp{2\left(\emKer[\Ndesign]{\llik}(\parMat) - \emKer[\Naugment]{\llik}(\parMat)\right)}. \label{var_inflation_factor}
\end{align}
\end{lemma}
The interpretation of \Cref{lemma:evar} is similar to that for \Cref{lemma:pred-mean-dist}. 
The first term in \ref{evar-expression} is the variance of the log-normal random variable
 $\llikEmRdmDens[\Ndesign](\Par)$ conditioned on the ``pseudo-data''
 $(\designBatchIn, \emMean[\Ndesign]{\llik}(\designBatchIn))$. The second term 
 $\varInflation(\Par; \designBatchIn)$, which we refer to as the \textit{variance inflation factor}, 
 accounts for the uncertainty due to $\designBatchLlik$. We note that 
$\varInflation(\Par; \designBatchIn) \geq 1$ and the variance inflation is larger when the 
input batch $\designBatchIn$ is more influential. If $\designBatchIn$ is a subset of the 
current design $\designIn[\Ndesign]$, then $\varInflation(\Par; \designBatchIn) = 1$ (no 
variance inflation is applied). 

With these preliminary results established, we define the $\intExpVarLabel$ criterion
\begin{align}
\labelAcq[\Ndesign]{\intExpVarLabel}(\designBatchIn) \Def
\int_{\parSpace} \E_{\designBatchLlik} \Var[\llikEmRdmDens[\Naugment](\Par)] \weightDens(\Par) d\Par, 
\end{align}
emphasizing the the expectation is still with respect to 
$\designBatchLlik \sim \Gaussian(\emMean[\Ndesign]{\llik}(\designBatchIn), \emKer[\Ndesign]{\llik}(\designBatchIn))$. 
Applying \Cref{lemma:evar} immediately gives the following simplification. 
\begin{lemma}
The $\intExpVarLabel$ acquisition function is equal to 
\begin{align}
\labelAcq[\Ndesign]{\intExpVarLabel}(\designBatchIn) =
\int_{\parSpace} \Var[\llikEmRdmDens[\Ndesign](\Par) | \emMean[\Ndesign]{\llik}(\designBatchIn)]
	\varInflation(\Par; \designBatchIn) \weightDens(\Par) d\Par, 
\end{align}
where the variance inflation factor $\varInflation(\Par; \designBatchIn)$ is defined in \ref{var_inflation_factor}. 
\end{lemma}
This criterion is considered in \cite{VehtariParallelGP} in training a log-likelihood emulator to be used for 
approximate Bayesian computation. 

\paragraph{Forward Model Emulation}
The acquisition function is defined identically in the forward model emulation setting, 
\begin{align}
\labelAcq[\Ndesign]{\fwdintExpVarLabel}(\designBatchIn) \Def
\int_{\parSpace} \E_{\designBatchLlik} \Var[\fwdEmRdm[\Naugment]{\postDens}(\Par)] \weightDens(\Par) d\Par. \label{fwd-int-exp-var}
\end{align}
To our knowledge, 
the paper \cite{SinsbeckNowak} is the first to consider a design criterion of the form \ref{fwd-int-exp-var}. 
While in general the tractability of this criterion depends on the specific likelihood, \cite{Surer2023sequential} 
demonstrates that the integrand admits a closed-form expression when the likelihood is Gaussian. 
To derive this result, we begin with an analog to \Cref{lemma:pred-mean-dist}, which describes the 
dependence of $\emMean[\Naugment]{\fwd}(\parMat)$ on $\fwd(\designBatchIn)$, under the assumption 
that $\fwd(\designBatchIn)$ is distributed according to the current GP. 
\begin{lemma} \label{lemma:fwd-pred-mean-dist}
Let $(\designBatchIn, \designBatchFwd)$ be a new batch of $\Nbatch$ design points, with 
$\designBatchFwd \sim \Gaussian(\emMean[\Ndesign]{\fwd}(\designBatchIn), \emKer[\Ndesign]{\fwd}(\designBatchIn))$. 
Then for an arbitrary set of inputs $\parMat$, the predictive mean $\emMean[\Naugment]{\fwd}(\parMat)$
(viewed as a function of the random variable $\designBatchFwd$), satisfies 
\begin{align}
\emMean[\Naugment]{\fwd}(\parMat) 
\sim \Gaussian\left(\E[\llikEmFwd[\Ndesign](\parMat) | \emMean[\Ndesign]{\fwd}(\designBatchIn)], 
                               \emKer[\Ndesign]{\fwd}(\parMat) - \emKer[\Naugment]{\fwd}(\parMat)] \right) \label{fwd-pred-mean-dist}
\end{align}
\end{lemma}




% Learning Likelihood Parameters
\section{Learning Likelihood Parameters} \label{section_lik_par}

% Appendix 
\section{Appendix}

\subsection{Marginal Acceptance Probability}
In this section we derive an expression for 
\begin{align}
\llikEmJointMarg[\Ndesign]{\accProbMH}(\Par, \propPar) 
&= \E_{\llikEm[\Ndesign]} \left[\min\left\{1, \Em[\Ndesign]{\accProbRatio}(\Par, \propPar)\right\} \right], 
\end{align}
as considered in \ref{acc_prob_joint_marg}. We start by noting 
\begin{align}
\Em[\Ndesign]{\accProbRatio}(\Par, \propPar)
&\sim \LN\left(\log C + \emMean[\Ndesign]{\llik}(\propPar) - \emMean[\Ndesign]{\llik}(\Par), \Var\left[\llikEm[\Ndesign](\propPar) - \llikEm[\Ndesign](\Par) \right] \right),
\label{acc_ratio_LN}
\end{align}
where 
\begin{align*}
C \Def \frac{\priorDens(\propPar) \propDens(\propPar, \Par)}{\priorDens(\Par) \propDens(\Par, \propPar)}.
\end{align*}
Thus, the required computation reduces to computing the expectation of $\min\left\{1, Y \right\}$, where $Y$ is a log-normally distributed 
random variable. 

\begin{lemma} \label{lemma:exp_max_one_LN}
Let $Y \sim \LN(m, s^2)$. Then, 
\begin{align}
\E\left[\min\left\{1, Y \right\} \right] &= \GaussianCDF\left(\frac{m}{s} \right) + \GaussianCDF\left(-\frac{m + s^2}{s} \right) e^{m + \frac{1}{2}s^2}
\end{align}
\end{lemma}

\begin{proof}
We have 
\begin{align}
\E\left[\min\left\{1, Y \right\} \right]
&= \int_{0}^{\infty} \min\{1, y\} \LN(y | m, s^2) dy \nonumber \\
&= \int_{-\infty}^{\infty} \min\{1, e^x\} \Gaussian(x | m, s^2) dx \nonumber \\
&= \int_{0}^{\infty} \Gaussian(x | m, s^2) dx +  \int_{-\infty}^{0} e^x \Gaussian(x | m, s^2) dx \nonumber \\
&= \GaussianCDF(m/s) + \int_{-\infty}^{0} e^x \Gaussian(x | m, s^2) dx. \nonumber \\
&= \GaussianCDF(m/s) + \int_{-\infty}^{0} \frac{1}{\sqrt{2\pi s^2}} e^x \Exp{-\frac{1}{2s^2} (x - m)^2} dx.  
\label{second_integral} 
\end{align}
For the integral in \ref{second_integral} we combine the exponential terms and complete the square. 
This yields 
\begin{align*}
&\int_{-\infty}^{0} \frac{1}{\sqrt{2\pi s^2}} e^x \Exp{-\frac{1}{2s^2} (x - m)^2} dx \\
&= \Exp{-\frac{m^2}{2s^2}} \int_{-\infty}^{0} \frac{1}{\sqrt{2\pi s^2}} \Exp{-\frac{1}{2} \left[\frac{x^2}{2} - 2\left(\frac{m}{s^2} + 1\right)x  \right]} dx \\
&= \Exp{-\frac{m^2}{2s^2} + \frac{[m + s^2]^2}{2s^2}} \int_{-\infty}^{0} \frac{1}{\sqrt{2\pi s^2}} \Exp{-\frac{1}{2s^2}(x - [m+s^2])^2} dx \\
&= \Exp{m + \frac{1}{2}s^2} \GaussianCDF\left(-\frac{m+s^2}{s}\right).
\end{align*}
Plugging this back into \ref{second_integral} completes the proof. 
\end{proof}

We now apply \Cref{lemma:exp_max_one_LN} to obtain the expression for the marginal acceptance probability. 
\begin{prop} \label{prop:joint-marg-accept-prob}
\begin{align*}
\llikEmJointMarg[\Ndesign]{\accProbMH}(\Par, \propPar) 
&= \E_{\llikEm[\Ndesign]} \left[\min\left\{1, \Em[\Ndesign]{\accProbRatio}(\Par, \propPar)\right\} \right] 
= w_1 + w_2 \llikEmJointMarg[\Ndesign]{\accProbRatio}(\Par, \propPar), 
\end{align*}
where 
\begin{align*}
w_1 &= \Prob(\Em[\Ndesign]{\accProbRatio}(\Par, \propPar) \geq 1) \\
w_2 &= \Prob\left(\Em[\Ndesign]{\accProbRatio}(\Par, \propPar) \leq \Exp{-\Var\left[\llikEm[\Ndesign](\propPar) - \llikEm[\Ndesign](\Par) \right]}\right).
\end{align*}
\end{prop}

\begin{proof}
We recall from \ref{acc_ratio_LN} that $\Em[\Ndesign]{\accProbRatio}(\Par, \propPar) \sim \LN(m, s^2)$, with 
\begin{align*}
m &= \log C + \emMean[\Ndesign]{\llik}(\propPar) - \emMean[\Ndesign]{\llik}(\Par) \\
s^2 &= \Var\left[\llikEm[\Ndesign](\propPar) - \llikEm[\Ndesign](\Par) \right]. 
\end{align*}
We thus have 
\begin{align*}
e^{m + \frac{1}{2}s^2} = \E_{\llikEm}\left[\Em[\Ndesign]{\accProbRatio}(\Par, \propPar) \right] 
=  \llikEmJointMarg[\Ndesign]{\accProbRatio}(\Par, \propPar). 
\end{align*}
It remains to verify the expressions for the weights $w_1$ and $w_2$. Letting $Z \sim \Gaussian(0,1)$, 
we apply \Cref{lemma:exp_max_one_LN} to obtain
\begin{align*}
w_1 &= \GaussianCDF(m/s) = \Prob(Z \leq m/s) = \Prob(\Exp{m+sZ} \geq 1) = \Prob(\Em[\Ndesign]{\accProbRatio}(\Par, \propPar) \geq 1) \\
w_2 &= \GaussianCDF(-(m+s^2)/s) = \Prob(\Exp{m + sZ} \leq e^{-s}) = \Prob(\Em[\Ndesign]{\accProbRatio}(\Par, \propPar) \leq e^{-s}).
\end{align*}
\end{proof}

\subsection{Transition Kernels of Approximate MCMC Algorithms} \label{transition_kernel_derivations}
We derive the expression for the \textit{MCWMH-joint} transition 
kernel $\llikEmJointMCWMH[\Ndesign]{\MarkovKernel}$ given in \ref{MCWMH-joint-kernel}. 
Let $\Par \in \parSpace$ and $A \subset \parSpace$ a Borel set. 
We recall that the kernel for a standard MH algorithm is given by 
\begin{align}
\MarkovKernel(\Par, A)
&= \int_{A} \propDens(\Par, \propPar) \accProbMH(\Par, \propPar) d\propPar
+ [1 - \avgAccProbMH(\Par)] \delta_{\Par}(A) \label{MH-kernel}, 
\end{align}
where $\avgAccProbMH(\Par)$ is the overall acceptance probability, averaged over all proposals,
\begin{align}
\avgAccProbMH(\Par)
&= \int_{\parSpace} \propDens(\Par, \propPar) \accProbMH(\Par, \propPar) d\propPar. 
\end{align}
Denote $\parMat \Def \{\Par, \propPar\}$. 
The \textit{MCWMH-joint} algorithm replaces the exact log-likelihood evaluations 
$\llik(\parMat) \Def [\llik(\Par), \llik(\propPar)]^\top$ used to define $\propDens(\Par, \propPar)$ by 
sampled approximate values 
\begin{align}
\llikSamp_{\parMat} \Def [\llikSamp, \llikSampProp]^\top \sim \Gaussian(\emMean[\Ndesign]{\llik}(\parMat), \emKer[\Ndesign]{\llik}(\parMat)). 
\end{align}
We let $\Em[\Ndesign]{\accProbMH}(\Par, \propPar | \llikSamp_{\parMat})$ denote the approximate acceptance probability defined using 
the sampled values $\llikSamp_{\parMat}$. The probability of accepting a state in the set $A$, conditional on the sample $\llikSamp_{\parMat}$, 
is thus 
\begin{align}
\Prob(\indexMCMC[\mcmcIndex+1]{\Par} \in A | \Par, \llikSamp_{\parMat})
&=  \int_{A} \propDens(\Par, \propPar) \Em[\Ndesign]{\accProbMH}(\Par, \propPar | \llikSamp_{\parMat}) d\propPar. 
\end{align}
The unconditional probability follows from the law of total probability:
\begin{align}
\Prob(\indexMCMC[\mcmcIndex+1]{\Par} \in A | \Par)
&= \int_{\R^2} \Prob(\indexMCMC[\mcmcIndex+1]{\Par} \in A | \Par, \llikSamp_{\parMat}) \Gaussian(\llikSamp_{\parMat} | \emMean[\Ndesign]{\llik}(\parMat), \emKer[\Ndesign]{\llik}(\parMat)) d\llikSamp_{\parMat} \\
&= \int_{\R^2} \int_{A} \propDens(\Par, \propPar) \Em[\Ndesign]{\accProbMH}(\Par, \propPar | \llikSamp_{\parMat})  
\Gaussian(\llikSamp_{\parMat} | \emMean[\Ndesign]{\llik}(\parMat), \emKer[\Ndesign]{\llik}(\parMat)) d\propPar \ d\llikSamp_{\parMat} \\
&= \int_{A} \propDens(\Par, \propPar) \int_{\R^2}  \Em[\Ndesign]{\accProbMH}(\Par, \propPar | \llikSamp_{\parMat})  
\Gaussian(\llikSamp_{\parMat} | \emMean[\Ndesign]{\llik}(\parMat), \emKer[\Ndesign]{\llik}(\parMat)) d\llikSamp_{\parMat} \ d\propPar \label{flip_integral_order} \\
&= \int_{A} \propDens(\Par, \propPar) \llikEmJointMarg[\Ndesign]{\accProbMH}(\Par, \propPar)  d\propPar
\end{align}
where \ref{flip_integral_order} follows from Tonelli's theorem, and using the definition of $\llikEmJointMarg[\Ndesign]{\accProbMH}(\Par, \propPar)$
in \ref{acc_prob_joint_marg}. Setting $A \Def \parSpace$ in the above integral yields the overall acceptance probability 
\begin{align}
\llikEmJointMarg[\Ndesign]{\avgAccProbMH}(\Par) \Def \int_{\parSpace} \propDens(\Par, \propPar) \llikEmJointMarg[\Ndesign]{\accProbMH}(\Par, \propPar)  d\propPar. 
\end{align}
These quantities thus give 
\begin{align}
\llikEmJointMCWMH[\Ndesign]{\MarkovKernel}(\Par, A)
&= \int_{A} \propDens(\Par, \propPar) \llikEmJointMarg[\Ndesign]{\accProbMH}(\Par, \propPar) d\propPar
+ [1 - \llikEmJointMarg[\Ndesign]{\avgAccProbMH}(\Par)] \delta_{\Par}(A) \label{MH-kernel}, 
\end{align}
which follows from same derivations for the standard MH kernel \ref{MH-kernel} with the marginal acceptance probabilities substituted for
the original ones. The transition kernel for the \textit{MCWMH-ind} algorithm follows immediately by marginalizing with respect 
to $\Gaussian(\emMean[\Ndesign]{\llik}\left(\parMat), \text{diag}\left\{\emKer[\Ndesign]{\llik}(\Par), \emKer[\Ndesign]{\llik}(\propPar) \right\}\right)$
in place of the full joint distribution. 

% Questions and TODOs
\section{Questions and TODOs}
\subsection{Notation}
\begin{enumerate}
\item How to clean up notation for all of the different approximations being considered here? 
\end{enumerate}

\subsection{Need to add}
\begin{enumerate}
\item Summary of results from noisy MCMC literature. 
\end{enumerate}

\subsection{Things to consider trying}
\begin{enumerate}
\item Perhaps give some context by discussing connections to Bayesian optimization and to log-likelihood approximation used in 
simulation-based inference. 
\item Hyperparameter marginalization: need to look into opportunities for closed-form hyperparameter marginalization during 
sequential design phase. 
\item Think about comparing the two forms of variance inflation in the frequency domain. 
\end{enumerate}


\bibliography{post_approx_with_GP_emulators} 
\bibliographystyle{ieeetr}

\end{document}







