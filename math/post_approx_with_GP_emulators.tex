\documentclass[12pt]{article}
\RequirePackage[l2tabu, orthodox]{nag}
\usepackage[main=english]{babel}
\usepackage[rm={lining,tabular},sf={lining,tabular},tt={lining,tabular,monowidth}]{cfr-lm}
\usepackage{amsthm,amssymb,latexsym,gensymb,mathtools,mathrsfs}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{epstopdf,enumitem,microtype,dcolumn,booktabs,hyperref,url,fancyhdr}
\usepackage{algorithmic}
\usepackage[ruled,vlined,commentsnumbered,titlenotnumbered]{algorithm2e}
\usepackage{bbm}

% Plotting
\usepackage{pgfplots}
\usepackage{xinttools} % for the \xintFor***
\usepgfplotslibrary{fillbetween}
\pgfplotsset{compat=1.8}
\usepackage{tikz}

% Local custom commands. 
\include{latex_macros_general}
\include{latex_macros_gp_inv_prob}
\newcommand{\bphi}{\boldsymbol{\phi}}

\setlist{topsep=1ex,parsep=1ex,itemsep=0ex}
\setlist[1]{leftmargin=\parindent}
\setlist[enumerate,1]{label=\arabic*.,ref=\arabic*}
\setlist[enumerate,2]{label=(\alph*),ref=(\alph*)}

% For embedding images
\graphicspath{ {./images/} }

% Specifically for paper formatting 
\renewcommand{\baselinestretch}{1.2} % Spaces manuscript for easy reading

% Formatting definitions, propositions, etc. 
\newtheorem{definition}{Definition}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{corollary}{Corollary}

% Title and author
\title{Gaussian Process Surrogates for Bayesian Inverse Problems: Posterior Approximation and Sequential Design}
\author{Andrew Roberts}

\begin{document}

\maketitle


% Introduction 
\section{Introduction}

% Setting and Background 
\section{Setting and Background}

\subsection{Bayesian Inverse Problems}
We consider a \textit{forward model} $\fwd: \R^{\dimPar} \to \R^{\dimObs}$ describing some system of interest, parameterized by 
input parameters $\Par \in \parSpace \subset \R^{\dimPar}$. In addition, we suppose that we have noisy observations 
$\obs \in \obsSpace \subset \R^{\dimObs}$ of the output signal which $\fwd(\Par)$ seeks to approximate. The 
\textit{inverse problem} concerns learning the parameter values $\Par$ such that $\fwd(\Par) \approx \obs$; i.e., \textit{calibrating}
the model so that it agrees with reality. The statistical approach to this problem assumes that the link between model outputs and 
observations is governed by a probability distribution on  $\obs | \Par$. We assume that this distribution admits a density 
with corresponding log-likelihood 
\begin{align}
\llik: \parSpace \to \R, \label{log_likelihood}
\end{align}
such that $p(\obs | \Par) = \Exp{\llik(\Par)}$ 
\footnote{For succinctness, the notation $\llik(\Par)$ suppresses dependence on the data $\obs$, 
as we assume the data is fixed throughout the analysis.}
\footnote{We are currently focusing on the forward model parameter $\Par$, treating other likelihood 
parameters (e.g., observation covariance) as fixed. We will relax this assumption 
in section \ref{section_lik_par}.}.

The Bayesian approach completes this specification with a prior distribution 
on $\Par$. Letting $\priorDens(\Par)$ denote the density of this distribution, the Bayesian solution of the inverse problem is given 
by the posterior distribution $\postDensNorm(\Par) := p(\Par | \obs)$. The posterior density is computed by Bayes' rule 
\begin{align}
\postDensNorm(\Par) = \frac{1}{\normCst}\postDens(\Par) := \frac{1}{\normCst} \priorDens(\Par) \Exp{\llik(\Par)}, \label{post_dens}
\end{align}
with normalizing constant given by 
\begin{align}
\normCst &= \int \priorDens(\Par) \Exp{\llik(\Par)} d\Par. \label{norm_cst}
\end{align}
We emphasize that throughout this paper $\postDens(\Par)$ denotes the \textit{unnormalized} 
posterior density. When relevant, we will make explicit the likelihood dependence on the forward 
model by writing $\llik(\Par; \fwd)$. In addition to considering 
generic likelihoods, we will give special attention to the additive Gaussian noise model
\begin{align}
\obs &= \fwd(\Par) + \noise \label{inv_prob_Gaussian} \\
\noise &\sim \Gaussian(0, \likPar) \nonumber \\
\Par &\sim \priorDens \nonumber, 
\end{align}
with corresponding log-likelihood 
\begin{align}
\llik(\Par; \fwd) &= \log\Gaussian(\obs| \fwd(\Par), \likPar) 
= -\frac{1}{2} \log\det (2\pi \likPar) - \frac{1}{2} (\obs - \fwd(\Par))^\top \likPar^{-1} (y - \fwd(\Par)). \label{llik_Gaussian}
\end{align}

In general, the nonlinearity of $\fwd$ precludes a closed-form characterization of the posterior. In this case, the 
standard approach is to instead draw samples from the distribution using a Markov chain Monte Carlo (MCMC) 
algorithm. Such algorithms are iterative in nature, often requiring $\BigO(10^6)$ iterations, with each 
iteration involving an evaluation of the unnormalized posterior density $\postDens$. In the inverse problem 
context, this computational requirement is often prohibitive when the forward model evaluations $\fwd(\Par)$
incur significant computational cost. This becomes clear upon observing that each log-likelihood evaluation 
$\llik(\Par)$ (e.g., \ref{llik_Gaussian} in the additive Gaussian case) requires the corresponding forward 
model evaluation $\fwd(\Par)$. Motivated by this problem, a large body of work has focused on deriving 
cheap approximations to either $\fwd(\Par)$ or $\llik(\Par)$ (note that approximating the former induces 
an approximation of the latter). We refer to such approximations interchangeably 
as \textit{surrogates} or \textit{emulators}. The focus of the present work is on utilizing \textit{Gaussian processes} 
to derive emulators for the forward model or log-likelihood. 

\subsection{Gaussian Processes}
In this section, we provide a brief review of Gaussian processes; for in-depth treatments, we refer readers to 
\cite{gramacy2020surrogates, StuartTeck2, gpML}. A \textit{Gaussian process} (GP) is a probability 
distribution over a set of functions $\func: \parSpace \to \R$, defined by the property that 
the random vector $[\func(\Par_1), \dots, \func(\Par_\Ndesign)]^\top$ has
a joint Gaussian distribution for any set of inputs $\parMat := \{\Par_1, \dots, \Par_\Ndesign\} \subset \parSpace$. 
A GP is defined by its \textit{mean function} $\gpMeanPrior: \parSpace \to \R$ and positive-definite \textit{kernel} 
(i.e., \textit{covariance function}) $\gpKerPrior: \parSpace \times \parSpace \to \R$ and is thus denoted by 
$\GP(\gpMeanPrior, \gpKerPrior)$. Throughout this paper, we will vectorize notation as follows. Letting 
$\tilde{\parMat} := [\tilde{\Par}_1, \dots, \tilde{\Par}_M]  \subset \parSpace$ be a second set of inputs, we define 
$\func(\parMat) := [\func(\Par_1), \dots, \func(\Par_\Ndesign)]^\top \in \R^{\Ndesign}$ (and similarly for $\gpMeanPrior(\parMat)$), 
$\gpKerPrior(\parMat, \tilde{\parMat}) := \{\gpKerPrior(\Par_{\idxDesign}, \tilde{\Par}_m)\}_{1 \leq \idxDesign, m \leq \Ndesign} \in \R^{\Ndesign \times M}$, 
and $\gpKerPrior(\parMat) := \gpKerPrior(\parMat, \parMat) \in \R^{\Ndesign \times \Ndesign}$. Given this notation, 
the defining property of the GP can be written as 
\begin{align}
\func(\parMat) &\sim \Gaussian(\gpMeanPrior(\parMat), \gpKerPrior(\parMat)).
\end{align}
Now suppose that we have observed the (noiseless) function evaluations $\func(\designIn)$ at a set of \textit{design points} $\designIn$. 
It is well-known \cite{gpML} that the conditional distribution $\func|\func(\designIn)$ is also a GP
\begin{align}
\funcEm[\Ndesign] := \func|\func(\designIn) \sim \GP(\gpMean[\Ndesign], \gpKer[\Ndesign]),
\end{align}
with mean and kernel given by 
\begin{align}
\gpMean[\Ndesign](\parMat) &= \gpMeanPrior(\parMat) + \gpKerPrior(\parMat, \designIn) \gpKerPrior(\designIn)^{-1} [\func(\designIn) - \gpMeanPrior(\designIn)] \label{kriging_eqns} \\
\gpKer[\Ndesign](\parMat) &= \gpKerPrior(\parMat) - \gpKerPrior(\parMat, \designIn) \gpKerPrior(\designIn)^{-1} \gpKerPrior(\designIn, \parMat). \nonumber
\end{align}
The predictive mean can be seen to interpolate the design points (i.e., $\gpMean[\Ndesign](\designIn) = \func(\designIn)$), with 
the predictive variance vanishing at these inputs (i.e., $\gpKer[\Ndesign](\designIn) = 0$). To break these interpolation properties,
a constant can be added to the diagonal of the kernel matrix so that $(\gpKerPrior(\designIn) + \nuggetSD^2 I)^{-1}$ replaces 
$\gpKer(\designIn)^{-1}$ in \ref{kriging_eqns}. In the present context, the maps $\fwd$ and $\llik$ are assumed to be deterministic functions 
of $\Par$ and hence the interpolation property is desirable. Nevertheless, a small, positive fixed constant $\nuggetSD^2$ is still 
typically included for numerical stability. 

Our default choice of covariance function is the popular \textit{exponentiated quadratic} 
(also called \textit{squared exponential} or \textit{Gaussian}) kernel, which is given by 
\begin{align}
\gpKerPrior(\Par, \tilde{\Par}) &= 
\margSD^2 \Exp{- \sum_{\idxParDim=1}^{\dimPar} \left(\frac{\Par_{\idxParDim} - \tilde{\Par}_{\idxParDim}}{\lengthscale_{\idxParDim}} \right)^2}, 
\end{align}
defined by the choice of positive hyperparameters $\margSD, \lengthscale_{1}, \dots, \lengthscale_{\dimPar}$. This is an example
of a stationary, or shift-invariant, kernel since $\gpKer(\Par, \tilde{\Par})$ only depends on its arguments through their difference 
$\Par - \tilde{\Par}$. To complete the GP prior specification, we typically take the mean 
function to be a constant or quadratic. We opt for the common empirical Bayes approach of fixing all mean and kernel hyperparameters 
at their maximum likelihood estimates. 

Finally, we consider the extension to multi-output functions $\func: \parSpace \to \R^{\dimObs}$. There is a large literature on multi-output 
kernels (\todo: cite), but we focus here on the simplest case in which each output is modeled separately as an independent GP. 
In this setting, the notation $\gpKerPrior(\Par)$ is now interpreted as the $\dimObs \times \dimObs$ diagonal matrix collecting the variance of each 
independent GP. Similarly, for a set of inputs $\parMat \in \R^{M \times \dimPar}$, $\gpKerPrior(\parMat)$ is now the 
$M\dimObs \times M\dimObs$ block diagonal matrix constructed from the predictive covariances of each independent GP. 

\subsection{Surrogate Construction}
We now consider modeling either $\fwd$ or $\llik$ with a GP, referring to these cases as \textit{forward model emulation} and 
\textit{log-likelihood} emulation, respectively. As mentioned previously, computationally costly forward models render 
standard inference algorithms intractable for solving Bayesian inverse problems due to the large number of model 
evaluations that must be conducted iteratively. The main idea of model emulation is to front-load the forward model 
runs, which can often be computed in parallel, to obtain a design 
$\{(\Par_{\idxDesign}, \fwd(\Par_{\idxDesign}))\}_{\idxDesign=1}^{\Ndesign}$ or 
$\{(\Par_{\idxDesign}, \llik(\Par_{\idxDesign}))\}_{\idxDesign=1}^{\Ndesign}$, which can then be used to train a model that 
approximates $\fwd$ or $\llik$. 

\subsubsection{Forward Model Emulation}
If the output dimension $\dimObs$ is reasonably small, then a common approach is to specify an independent 
multi-output GP prior directly for the forward model, $\fwd \sim \GP(\emMeanPrior{\fwd}, \emKerPrior{\fwd})$. After 
conditioning on the design $\{(\Par_{\idxDesign}, \fwd(\Par_{\idxDesign}))\}_{\idxDesign=1}^{\Ndesign}$, 
the predictive distribution $\fwdEm[\Ndesign] \sim \GP(\emMean[\Ndesign]{\fwd}, \emKer[\Ndesign]{\fwd})$ 
represents the emulator for $\fwd$. We emphasize that $\fwdEm[\Ndesign]$ is a \textit{stochastic} surrogate; 
the model output predictions at inputs $\parMat$ are given by the entire distribution 
$\fwdEm[\Ndesign](\parMat) \sim \Gaussian(\emMean[\Ndesign]{\fwd}(\parMat), \emKer[\Ndesign]{\fwd}(\parMat))$. 
Thus, the mean $\emMean[\Ndesign]{\fwd}(\parMat)$ provides a point estimate while the covariance 
$\emKer[\Ndesign]{\fwd}(\parMat)$ summarizes the uncertainty in the estimate. This is useful for the purpose
of uncertainty quantification, in that the additional uncertainty introduced as a consequence of the surrogate 
approximation can be propagated to the approximate posterior distribution. We note that plugging the random 
function $\fwdEm[\Ndesign]$ in place of $\fwd$ in the log-likelihood induces an approximation of the log-likelihood. 
In the case of the additive Gaussian likelihood \ref{llik_Gaussian} this approximation takes the form 
\begin{align}
\llikEmFwd(\Par) := \llik(\Par; \fwdEm) =  \log\Gaussian(\obs| \fwdEm[\Ndesign](\Par), \likPar). \label{induced_llik_emulator}
\end{align}
We emphasize two facts: (i.) the induced log-likelihood approximation $\llikEmFwd[\Ndesign](\Par)$ is also 
stochastic, but no longer Gaussian; and (ii.) no surrogate modeling is performed directly for $\llik$; instead, 
the approximation $\llikEmFwd[\Ndesign](\Par) = \llik(\Par; \fwdEm[\Ndesign])$ is derived by ``plugging in'' the 
forward model emulator. This represents a modular surrogate-modeling approach \cite{modularization}. 
Emulating expensive models using GPs is one of the central questions 
studied in the computer experiments literature; see \cite{gramacy2020surrogates} and 
\cite{design_analysis_computer_experiments} for detailed treatments. Challenges arise when the output
dimension $\dimObs$ is large or the outputs are highly interdependent, rendering the independent GP 
approach infeasible or ill-advised. For example, the output may correspond to the numerical solution of 
a differential equation and thus may take the form of a time series or spatiotemporal grid. 
A great deal of literature has addressed these challenges, including approaches tailored to forward models
with dynamical structure 
(e.g., \cite{GP_dynamic_emulation, Bayesian_emulation_dynamic, Liu_West_dynamic_emulation, dynamic_nonlinear_simulators_GP}).
A popular alternative is to first approximately express the high-dimensional output as a linear combination of basis vectors 
\begin{align}
\fwd(\Par) &= \sum_{\idxBasis=1}^{\dimBasis} \basisWeight_{\idxBasis}(\Par) \basisVec_{\idxBasis} + \noise_{\basisVec},
\end{align}
and then model the coefficients $\basisWeight_{\idxBasis}(\Par)$ using independent GPs. The basis vectors 
are typically found via a singular value decomposition, though other bases have been explored 
(\cite{HigdonBasis, emulate_functional_output, functionValuedModels, PODemulation}). The surrogate constructed in 
this fashion is simply a linear combination of independent GP emulators, and hence most of the results on forward model 
emulation in this paper are easily extended to this setting. See the appendix for details (\todo). 

\subsubsection{Log-Likelihood Emulation}
We observe in \ref{post_dens} that the forward model evaluations appear in the posterior density only through the 
likelihood. Thus, for the purposes of posterior inference, it is sufficient to directly approximate $\llik(\Par)$ without 
obtaining an approximation of the forward model. It is typically preferable to focus on approximating the logarithm 
of the likelihood, which usually results in a smoother response surface that is more amenable to GP emulation. 
The emulator construction proceeds as before; a GP prior $\llik \sim \GP(\emMeanPrior{\llik}, \emKerPrior{\llik})$ 
is specified and the predictive 
distribution $\llikEm[\Ndesign] \sim \GP(\emMean[\Ndesign]{\llik}, \emKer[\Ndesign]{\llik})$ obtained by 
conditioning on the design $\{(\Par_{\idxDesign}, \llik(\Par_{\idxDesign}))\}_{\idxDesign=1}^{\Ndesign}$. 
The use of GPs to approximate log-likelihoods has been widely used in approximate Bayesian inference 
(\cite{VehtariParallelGP, Kandasamy_2017, llikRBF, trainDynamics, quantileApprox, wang2018adaptive, landslideCalibration})
and Bayesian quadrature (\cite{BayesQuadrature, BayesQuadRatios}). In the context of inverse problems with 
high-dimensional output spaces, log-likelihood emulation has the clear benefit of reducing to a univariate GP setting. 
On the other hand, it has several downsides. Even on the log scale, log-likelihoods can be fast-varying and exhibit large 
dynamic range, proving troublesome for stationary GP priors \cite{wang2018adaptive}. A second potential weakness 
stems from the fact that log-likelihoods often have known bound constraints, which may not be respected by a GP 
prior or predictive distribution. Recent work has dealt with this problem by incorporating constraints into the 
GP hyperparameter optimization \cite{quantileApprox}. Finally, it may be difficult to incorporate prior domain knowledge 
into the GP prior for the log-likelihood, as compared to forward model emulation \cite{GP_PDE_priors}. 
Nevertheless, the benefits of reducing to a scalar output space may outweigh these downsides in many applications. 

% Posterior Approximation
\section{Posterior Approximation}
The forward model and log-likelihood emulation approaches result in the random log-likelihood approximations 
$\llikEmFwd[\Ndesign]$ and $\llikEm[\Ndesign]$, respectively. The latter results in a Gaussian predictor 
$\llikEm[\Ndesign](\parMat) \sim \Gaussian(\emMean[\Ndesign]{\llik}(\parMat), \emKer[\Ndesign]{\llik}(\parMat))$ 
at any parameter set $\parMat$. On the other hand, the distribution of $\llikEmFwd[\Ndesign](\parMat)$ is 
typically non-Gaussian and will depend on the specific likelihood under consideration. We use the notation  
$\llikEmGeneric[\Ndesign]$ as a stand-in for either $\llikEmFwd[\Ndesign]$ or $\llikEm[\Ndesign]$ when 
stating results that apply to both settings. 

Plugging these log-likelihood surrogates into the posterior density expression \ref{post_dens} yields 

\begin{align}
\fwdEmRdm[\Ndesign]{\postDensNorm}(\Par) 
&:= \frac{1}{\fwdEmRdm[\Ndesign]{\normCst}} \fwdEmRdmDens[\Ndesign](\Par) 
:=  \frac{1}{\fwdEmRdm[\Ndesign]{\normCst}} \priorDens(\Par) \Exp{\llikEmFwd[\Ndesign](\Par)} \label{post_dens_random_fwd} \\
\llikEmRdm[\Ndesign]{\postDensNorm}(\Par) 
&:= \frac{1}{\llikEmRdm[\Ndesign]{\normCst}} \llikEmRdmDens[\Ndesign](\Par) 
:=  \frac{1}{\llikEmRdm[\Ndesign]{\normCst}} \priorDens(\Par) \Exp{\llikEm[\Ndesign](\Par)} \label{post_dens_random_llik},
\end{align}
with normalizing constants given by 
\begin{align}
\fwdEmRdm[\Ndesign]{\normCst}
&= \int \priorDens(\Par) \Exp{\llikEmFwd[\Ndesign](\Par)} d\Par \label{norm_cst_random_fwd} \\
\llikEmRdm[\Ndesign]{\normCst}
&= \int \priorDens(\Par) \Exp{\llikEm[\Ndesign](\Par)} d\Par \label{norm_cst_random_llik}. 
\end{align}
The unnormalized posterior densities $\fwdEmRdmDens[\Ndesign]$ and $\llikEmRdmDens[\Ndesign]$ are stochastic; 
each sample trajectory from the underlying GP induces a different posterior. 
The papers \cite{StuartTeck1, StuartTeck2} refer to this as the \textit{sample-based}
approximation of the posterior. Given that it is not obvious how to perform inference using these random objects, one typically 
proceeds by choosing a single, deterministic posterior approximation that somehow summarizes the uncertainty in the 
random posterior measure. Popular choices for such deterministic approximations are described below. Going forward, 
for succinctness we will only define the unnormalized posterior densities, the associated normalized being 
implicitly defined. 

\subsection{Deterministic Approximations}
Arguably the simplest approach to constructing a deterministic approximation is to reduce the random emulator 
to a deterministic one by considering only the GP predictive mean. 
The plug-in mean estimators yield posterior approximations with unnormalized densities 
\begin{align}
\fwdEmMeanDens[\Ndesign](\Par) &:= \priorDens(\Par) \Exp{\llik(\Par; \emMean[\Ndesign]{\fwd}(\Par))}
 \label{post_dens_mean_fwd} \\
\llikEmMeanDens[\Ndesign](\Par) &:= \priorDens(\Par) \Exp{\emMean[\Ndesign]{\llik}(\Par)} 
\label{post_dens_mean_llik}.
\end{align}
These approximations fail to account for the uncertainty introduced by the surrogate model, which can lead 
to overconfidence in downstream posterior inference. These mean-based approximations are analyzed 
theoretically in \cite{StuartTeck1}, with error bounds provided in Hellinger distance. In \ref{VehtariParallelGP}, 
the approximation \ref{post_dens_mean_llik} is considered and is instead referred to as the 
\textit{median approximation}, owing to the fact that the mean and median of Gaussian distributions coincide. 
The approximation $\llikEmMeanDens[\Ndesign](\Par)$ can be shown to be optimal in a certain  
$L^1$ sense \cite{VehtariParallelGP, StuartTeck2}. 

One approach to propagate the 
emulator uncertainty is to marginalize the likelihood with respect to the emulator. Doing so yields the approximations 
\begin{align}
\fwdEmMargDens[\Ndesign](\Par) &:= \priorDens(\Par) \E_{\fwdEm}\left[\Exp{\llikEmFwd[\Ndesign](\Par)} \right] \label{post_dens_marg_fwd} \\
\llikEmMargDens[\Ndesign](\Par) &:= \priorDens(\Par) \E_{\llikEm}\left[\Exp{\llikEm[\Ndesign](\Par)} \right], \label{post_dens_marg_llik}
\end{align}
with the expectation subscripts indicating integration with respect to the GP predictive distributions 
$\Gaussian(\emMean[\Ndesign]{\fwd}(\Par), \emKer[\Ndesign]{\fwd}(\Par))$ and
$\Gaussian(\emMean[\Ndesign]{\llik}(\Par), \emKer[\Ndesign]{\llik}(\Par))$, respectively. Note that these expectations are 
computed independently for each $\Par$; the GP predictive covariance structure is not considered. 
We refer to these posteriors as \textit{marginal approximations} following the terminology used in
 \cite{StuartTeck1, StuartTeck2}, which analyze the error of these approximations in the Gaussian likelihood setting. 

Marginal approximations can be justified in various ways. \todo

We now consider performing inference for the approximations \ref{post_dens_marg_fwd} and \ref{post_dens_marg_llik}. 
In the log-likelihood emulation setting, observe that the induced likelihood approximation is log-normally distributed
\begin{align}
\Exp{\llikEm[\Ndesign](\Par)} &\sim \LN(\emMean[\Ndesign]{\llik}(\Par), \emKer[\Ndesign]{\llik}(\Par)), 
\end{align}
which implies that \ref{post_dens_marg_llik} simplifies to 
\begin{align}
\llikEmMargDens[\Ndesign](\Par) 
&:= \priorDens(\Par) \Exp{\emMean[\Ndesign]{\llik}(\Par) + \frac{1}{2} \emKer[\Ndesign]{\llik}(\Par)} 
\label{post_dens_marg_llik_simplified} \\
&= \llikEmMeanDens[\Ndesign](\Par) \Exp{\frac{1}{2} \emKer[\Ndesign]{\llik}(\Par)}. 
\label{post_dens_marg_var_inflation}
\end{align}
The final expression \ref{post_dens_marg_var_inflation} can be interpreted as a form of posterior inflation 
in regions where the emulator is uncertain. When $\emKer[\Ndesign]{\llik}(\Par) = 0$ the marginal approximation 
reduces to the mean approximation. While the unnormalized posterior density \ref{post_dens_marg_var_inflation}
can be plugged into off-the-shelf sampling algorithms, the uncertainty inflation term can cause highly multi-modal 
distributions which prove difficult for standard MCMC schemes. (\todo: add plot) This difficulty is noted in the 
supplement of \cite{VehtariParallelGP}.


\subsection{Deterministic Approximations}
\subsection{Noisy MCMC}

% Sequential Design
\section{Sequential Design}
\subsection{Background}
\subsection{Integrated Expected Variance}

% Learning Likelihood Parameters
\section{Learning Likelihood Parameters} \label{section_lik_par}

% Appendix 
\section{Appendix}

% Questions and TODOs
\section{Questions and TODOs}
\subsection{Notation}
\begin{enumerate}
\item Cleanest way to handle notation for llik emulation vs. forward model emulation? 
\item Should I define $\postDens$ to be unnormalized, or introduce separate notation for the unnormalized density. 
\item Should I define everything in terms of the potential (i.e. negative llik) instead of the llik? 
\item Think about how to define the domain/co-domain of the forward model; should this differ from the par/observation space? 
\item Perhaps give some context by discuss connections to Bayesian optimization and to log-likelihood approximation used in 
simulation-based inference. 
\item Hyperparameter marginalization: need to look into opportunities for closed-form hyperparameter marginalization during 
sequential design phase. 
\item Notation questions: $\llik^{\fwd}_{\Ndesign}(u)$ or $\llik(u; \fwd_{N})$? Other? Define marginal approximation separately 
for the two or can I use one notation that encompasses both? 
\end{enumerate}



\bibliography{post_approx_with_GP_emulators} 
\bibliographystyle{ieeetr}

\end{document}







