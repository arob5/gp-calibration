\documentclass[12pt]{article}
\RequirePackage[l2tabu, orthodox]{nag}
\usepackage[main=english]{babel}
\usepackage[rm={lining,tabular},sf={lining,tabular},tt={lining,tabular,monowidth}]{cfr-lm}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{amsthm,amssymb,latexsym,gensymb,mathtools,mathrsfs}
\usepackage{epstopdf,enumitem,microtype,dcolumn,booktabs,hyperref,url,fancyhdr}
\usepackage{algorithm}
\usepackage{algpseudocode} % Note that this also loads algorithmicx
\usepackage{cleveref}
\usepackage{bbm}
\usepackage{caption, subcaption} % Captions and sub-figures. 
% \usepackage[demo]{graphicx}

% Plotting
\usepackage{pgfplots}
\usepackage{xinttools} % for the \xintFor***
\usepgfplotslibrary{fillbetween}
\pgfplotsset{compat=1.8}
\usepackage{tikz}

% Local custom commands. 
\include{latex_macros_general}
\include{latex_macros_gp_inv_prob}
\newcommand{\bphi}{\boldsymbol{\phi}}

\setlist{topsep=1ex,parsep=1ex,itemsep=0ex}
\setlist[1]{leftmargin=\parindent}
\setlist[enumerate,1]{label=\arabic*.,ref=\arabic*}
\setlist[enumerate,2]{label=(\alph*),ref=(\alph*)}

% For embedding images
\graphicspath{{../output/gp_post_approx_paper/}}

% Specifically for paper formatting 
\renewcommand{\baselinestretch}{1.2} % Spaces manuscript for easy reading

% Formatting definitions, propositions, etc. 
\newtheorem{definition}{Definition}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{corollary}{Corollary}

% Title and author
\title{Gaussian Process Surrogates for Bayesian Inverse Problems: Posterior Approximation and Sequential Design}
\author{Andrew Roberts}

\begin{document}

\maketitle

% Use citep and citet
% Write w_1 and w_2 as functions of u and u-tilde 
% Email Amy 

% Introduction 
\section{Introduction}

% Setting and Background 
\section{Setting and Background}

\subsection{Bayesian Inverse Problems}
We consider a \textit{forward model} $\fwd: \R^{\dimPar} \to \R^{\dimObs}$ describing some system of interest, parameterized by 
input parameters $\Par \in \parSpace \subset \R^{\dimPar}$. In addition, we suppose that we have noisy observations 
$\obs \in \obsSpace \subset \R^{\dimObs}$ of the output signal which $\fwd(\Par)$ seeks to approximate. The 
\textit{inverse problem} concerns learning the parameter values $\Par$ such that $\fwd(\Par) \approx \obs$; i.e., \textit{calibrating}
the model so that it agrees with reality. The statistical approach to this problem assumes that the link between model outputs and 
observations is governed by a probability distribution on  $\obs | \Par$. We assume that this distribution admits a density 
with corresponding log-likelihood 
\begin{align}
\llik: \parSpace \to \R, \label{log_likelihood}
\end{align}
such that $p(\obs | \Par) = \Exp{\llik(\Par)}$ 
\footnote{For succinctness, the notation $\llik(\Par)$ suppresses dependence on the data $\obs$, 
as we assume the data is fixed throughout the analysis.}
\footnote{We are currently focusing on the forward model parameter $\Par$, treating other likelihood 
parameters (e.g., observation covariance) as fixed. We will relax this assumption 
in section \ref{section_lik_par}.}.

The Bayesian approach completes this specification with a prior distribution 
on $\Par$. Letting $\priorDens(\Par)$ denote the density of this distribution, the Bayesian solution of the inverse problem is given 
by the posterior distribution $\postDensNorm(\Par) \Def p(\Par | \obs)$. The posterior density is computed by Bayes' rule 
\begin{align}
\postDensNorm(\Par) = \frac{1}{\normCst}\postDens(\Par) \Def \frac{1}{\normCst} \priorDens(\Par) \Exp{\llik(\Par)}, \label{post_dens}
\end{align}
with normalizing constant given by 
\begin{align}
\normCst &= \int \priorDens(\Par) \Exp{\llik(\Par)} d\Par. \label{norm_cst}
\end{align}
We emphasize that throughout this paper $\postDens(\Par)$ denotes the \textit{unnormalized} 
posterior density. When relevant, we will make explicit the dependence on the forward 
model by writing $\llik(\Par; \fwd)$ and $\postDens(\Par; \fwd)$. In addition to considering 
generic likelihoods, we will give special attention to the additive Gaussian noise model
\begin{align}
\obs &= \fwd(\Par) + \noise \label{inv_prob_Gaussian} \\
\noise &\sim \Gaussian(0, \likPar) \nonumber \\
\Par &\sim \priorDens \nonumber, 
\end{align}
with corresponding log-likelihood 
\begin{align}
\llik(\Par; \fwd) &= \log\Gaussian(\obs| \fwd(\Par), \likPar) 
= -\frac{1}{2} \log\det (2\pi \likPar) - \frac{1}{2} (\obs - \fwd(\Par))^\top \likPar^{-1} (y - \fwd(\Par)). \label{llik_Gaussian}
\end{align}

In general, the nonlinearity of $\fwd$ precludes a closed-form characterization of the posterior. In this case, the 
standard approach is to instead draw samples from the distribution using a Markov chain Monte Carlo (MCMC) 
algorithm. Such algorithms are iterative in nature, often requiring $\BigO(10^6)$ iterations, with each 
iteration involving an evaluation of the unnormalized posterior density $\postDens(\Par)$. In the inverse problem 
context, this computational requirement is often prohibitive when the forward model evaluations $\fwd(\Par)$
incur significant computational cost. This becomes clear upon observing that each log-likelihood evaluation 
$\llik(\Par)$ (e.g., \ref{llik_Gaussian} in the additive Gaussian case) requires the corresponding forward 
model evaluation $\fwd(\Par)$. Motivated by this problem, a large body of work has focused on deriving 
cheap approximations to either $\fwd(\Par)$ or $\llik(\Par)$ (note that approximating the former induces 
an approximation of the latter). We refer to such approximations interchangeably 
as \textit{surrogates} or \textit{emulators}. The focus of the present work is on utilizing \textit{Gaussian processes} 
to derive emulators for the forward model or log-likelihood. 

\subsection{Gaussian Processes}
In this section, we provide a brief review of Gaussian processes; for in-depth treatments, we refer readers to 
\cite{gramacy2020surrogates, StuartTeck2, gpML}. A \textit{Gaussian process} (GP) is a probability 
distribution over a set of functions $\funcPrior: \parSpace \to \R$, defined by the property that 
the random vector $[\funcPrior(\Par_1), \dots, \funcPrior(\Par_\Ndesign)]^\top$ has
a joint Gaussian distribution for any set of inputs $\parMat \Def \{\Par_1, \dots, \Par_\Ndesign\} \subset \parSpace$. 
A GP is defined by its \textit{mean function} $\gpMeanPrior: \parSpace \to \R$ and positive-definite \textit{kernel} 
(i.e., \textit{covariance function}) $\gpKerPrior: \parSpace \times \parSpace \to \R$ and is thus denoted by 
$\GP(\gpMeanPrior, \gpKerPrior)$. Throughout this paper, we will vectorize notation as follows. Letting 
$\tilde{\parMat} \Def [\tilde{\Par}_1, \dots, \tilde{\Par}_M]  \subset \parSpace$ be a second set of inputs, we define 
$\funcPrior(\parMat) \Def [\funcPrior(\Par_1), \dots, \funcPrior(\Par_\Ndesign)]^\top \in \R^{\Ndesign}$, 
\footnote{We similarly use this vectorized notation for other functions; e.g., $\gpMeanPrior(\parMat)$, $\fwd(\parMat)$, and $\llik(\parMat)$}
$\gpKerPrior(\parMat, \tilde{\parMat}) \Def \{\gpKerPrior(\Par_{\idxDesign}, \tilde{\Par}_m)\}_{\substack{1 \leq \idxDesign \leq \Ndesign \\ 1 \leq m \leq M}} \in \R^{\Ndesign \times M}$, 
and $\gpKerPrior(\parMat) \Def \gpKerPrior(\parMat, \parMat) \in \R^{\Ndesign \times \Ndesign}$. Given this notation, 
the defining property of the GP can be written as 
\begin{align}
\funcPrior(\parMat) &\sim \Gaussian(\gpMeanPrior(\parMat), \gpKerPrior(\parMat)).
\end{align}
For our purposes, the GP $\funcPrior$ will be a prior distribution for a \textit{deterministic} function $\func: \parSpace \to \R$. 
However, the underlying ideas explored here can be extended to the setting where the evaluations of $\func$ 
are corrupted by noise. 
Now suppose that, at a set of \textit{design points} $\designIn$, we have computed the noiseless function values
$\func(\designIn)$. Viewing these values as a realization of the random variable $\funcPrior(\designIn)$, we can 
consider the conditional (i.e., \textit{predictive}) distribution $\funcPrior|[\funcPrior(\designIn)=\func(\designIn)]$. It is well-known
\cite{gpML} that this conditional distribution is also a GP
\begin{align}
\funcEm[\Ndesign] \Def \funcPrior|[\funcPrior(\designIn)=\func(\designIn)]\sim \GP(\gpMean[\Ndesign], \gpKer[\Ndesign]), \label{generic_gp_conditional}
\end{align}
with mean and kernel given by
\footnote{The GP conditional mean and covariance equations apply more generally when conditioning on 
$\{\designIn[\Ndesign], \funcVal[\Ndesign]\}$, regardless of whether $\funcVal[\Ndesign]$ is interpreted as 
a set of noiseless function evaluations or not. However, in our setting we reserve the notation $\gpMean[\Ndesign]$
for the case when $\funcVal[\Ndesign] = \func(\designIn[\Ndesign])$; i.e., conditioning on the exact function 
evaluations. Note that $\gpKer[\Ndesign]$ is independent of $\funcVal[\Ndesign]$ so this distinction is only 
relevant for the mean.} 
\begin{align}
\gpMean[\Ndesign](\parMat) &= \gpMeanPrior(\parMat) + \gpKerPrior(\parMat, \designIn) \gpKerPrior(\designIn)^{-1} [\func(\designIn) - \gpMeanPrior(\designIn)] \label{kriging_eqns} \\
\gpKer[\Ndesign](\parMat) &= \gpKerPrior(\parMat) - \gpKerPrior(\parMat, \designIn) \gpKerPrior(\designIn)^{-1} \gpKerPrior(\designIn, \parMat). \nonumber
\end{align}
The predictive mean can be seen to interpolate the design points (i.e., $\gpMean[\Ndesign](\designIn) = \func(\designIn[\Ndesign])$), with 
the predictive variance vanishing at these inputs (i.e., $\gpKer[\Ndesign](\designIn) = 0$). To break these interpolation properties,
a constant can be added to the diagonal of the kernel matrix so that $(\gpKerPrior(\designIn) + \nuggetSD^2 I)^{-1}$ replaces 
$\gpKerPrior(\designIn)^{-1}$ in \ref{kriging_eqns}. In the present context, the maps $\fwd$ and $\llik$ are assumed to be deterministic functions 
of $\Par$ and hence the interpolation property is desirable. Nevertheless, a small, positive fixed constant $\nuggetSD^2$ is still 
typically included for numerical stability. 

Our default choice of covariance function is the popular \textit{exponentiated quadratic} 
(also called \textit{squared exponential} or \textit{Gaussian}) kernel, which is given by 
\begin{align}
\gpKerPrior(\Par, \tilde{\Par}) &= 
\margSD^2 \Exp{- \sum_{\idxParDim=1}^{\dimPar} \left(\frac{\Par_{\idxParDim} - \tilde{\Par}_{\idxParDim}}{\lengthscale_{\idxParDim}} \right)^2}, 
\end{align}
defined by the choice of positive hyperparameters $\margSD, \lengthscale_{1}, \dots, \lengthscale_{\dimPar}$. This is an example
of a stationary, or shift-invariant, kernel since $\gpKerPrior(\Par, \tilde{\Par})$ only depends on its arguments through their difference 
$\Par - \tilde{\Par}$. To complete the GP prior specification, we typically take the mean 
function to be a constant or quadratic. We opt for the common empirical Bayes approach of fixing all mean and kernel hyperparameters 
at their maximum (marginal) likelihood estimates.

Finally, we consider the extension to multi-output functions $\funcPrior: \parSpace \to \R^{\dimObs}$. There is a large literature on multi-output 
kernels (\todo: cite), but we focus here on the simplest case in which each output is modeled separately as an independent GP. 
In this setting, the notation $\gpKerPrior(\Par)$ is now interpreted as the $\dimObs \times \dimObs$ diagonal matrix collecting the variance of each 
independent GP. Similarly, for a set of inputs $\parMat \in \R^{M \times \dimPar}$, $\gpKerPrior(\parMat)$ is now the 
$M\dimObs \times M\dimObs$ block diagonal matrix constructed from the predictive covariances of each independent GP. 

\subsection{Surrogate Construction}
We now consider modeling either $\fwd$ or $\llik$ with a GP, referring to these cases as \textit{forward model emulation} and 
\textit{log-likelihood} emulation, respectively. As mentioned previously, computationally costly forward models render 
standard inference algorithms intractable for solving Bayesian inverse problems due to the large number of model 
evaluations that must be conducted iteratively. The main idea of model emulation is to front-load the forward model 
runs, which can often be computed in parallel, to obtain a design $\{\designIn, \fwd(\designIn)\}$ or 
$\{\designIn, \llik(\designIn)\}$ which can then be used to train a model that 
approximates $\fwd$ or $\llik$.

\subsubsection{Forward Model Emulation}
If the output dimension $\dimObs$ is reasonably small, then a common approach is to specify an independent 
multi-output GP prior directly for the forward model, $\fwdPrior \sim \GP(\emMeanPrior{\fwd}, \emKerPrior{\fwd})$. 
Conditioning on this design yields the predictive distribution 
$\fwdEm[\Ndesign]  \Def \fwdPrior|[\fwdPrior(\designIn) = \fwd(\designIn)] \sim \GP(\emMean[\Ndesign]{\fwd}, \emKer[\Ndesign]{\fwd})$, 
which constitutes the emulator for $\fwd. $
We emphasize that $\fwdEm[\Ndesign]$ is a \textit{stochastic} surrogate; 
the model predictions at inputs $\parMat$ are given by the entire distribution 
$\fwdEm[\Ndesign](\parMat) \sim \Gaussian(\emMean[\Ndesign]{\fwd}(\parMat), \emKer[\Ndesign]{\fwd}(\parMat))$. 
Thus, the mean $\emMean[\Ndesign]{\fwd}(\parMat)$ provides a point estimate while the covariance 
$\emKer[\Ndesign]{\fwd}(\parMat)$ summarizes the uncertainty in the estimate. This is useful for the purpose
of uncertainty quantification in that the additional uncertainty introduced as a consequence of the surrogate 
approximation can be propagated to the approximate posterior distribution. We note that plugging the random 
function $\fwdEm[\Ndesign]$ in place of $\fwd$ in the log-likelihood induces an approximation of the log-likelihood,
which we denote by $\llikEmFwd$. 
In the case of the additive Gaussian likelihood \ref{llik_Gaussian} this approximation takes the form 
\begin{align}
\llikEmFwd(\Par) \Def \llik(\Par; \fwdEm) =  \log\Gaussian(\obs| \fwdEm[\Ndesign](\Par), \likPar). \label{induced_llik_emulator}
\end{align}
We emphasize two facts: (i.) the induced log-likelihood approximation $\llikEmFwd[\Ndesign](\Par)$ is also 
stochastic, but no longer Gaussian in general; and (ii.) no surrogate modeling is performed directly for $\llik$; instead, 
the approximation $\llikEmFwd[\Ndesign](\Par) = \llik(\Par; \fwdEm[\Ndesign])$ is derived by plugging the 
conditioned forward model emulator into the log-likelihood function, 
constituting a modular surrogate-modeling approach \cite{modularization}. While the induced stochastic approximation 
to the likelihood (and posterior density) cannot in general be described explicitly, the Gaussian likelihood 
example \ref{induced_llik_emulator} represents an important exception. The mean and variance of the induced
likelihood approximation are given in the following proposition. 

\begin{prop} \label{fwd_em_lik_emulator_moments_Gaussian}
Given a GP forward model emulator $\fwdEm[\Ndesign] \sim \GP(\emMean[\Ndesign]{\fwd}, \emKer[\Ndesign]{\fwd})$, 
the plug-in likelihood estimator $\Exp{\llik(\Par; \fwdEm)} = \Gaussian(\obs| \fwdEm[\Ndesign](\Par), \likPar)$ has 
mean and variance given by 
\begin{align}
\E_{\fwdEm}\left[\Exp{\llik(\Par; \fwdEm)} \right] 
&= \Gaussian(\obs | \emMean[\Ndesign]{\fwd}(\Par), \likPar + \emKer[\Ndesign]{\fwd}(\Par)) \\
\Var_{\fwdEm}\left[\Exp{\llik(\Par; \fwdEm)} \right]
&= \frac{\Gaussian\left(\obs | \emMean[\Ndesign]{\fwd}(\Par), \frac{1}{2}\likPar + \emKer[\Ndesign]{\fwd}(\Par)  \right)}{\det(2\pi \likPar)^{1/2}}
- \frac{\Gaussian\left(\obs | \emMean[\Ndesign]{\fwd}(\Par), \frac{1}{2}\left[\likPar + \emKer[\Ndesign]{\fwd}(\Par)\right]  \right)}{\det(2\pi [\likPar + \emKer[\Ndesign]{\fwd}(\Par)])^{1/2}}
\end{align}
\end{prop}
This follows as a special case of \Cref{prop:Gaussian_marginal_moments}, and its implications will be explored in subsequent sections. 

We note that the emulation of expensive models is one of the central questions studied in the 
computer experiments literature, with GPs arguably representing the most popular tool 
for the job; see \cite{gramacy2020surrogates} and 
\cite{design_analysis_computer_experiments} for detailed treatments.
The forward model emulation strategy detailed above becomes problematic when the output 
dimension $\dimObs$ is large or the outputs are highly interdependent, rendering the independent GP 
approach infeasible or ill-advised.
For example, the forward model output may correspond to the numerical solution of 
a differential equation and thus may take the form of a time series or spatiotemporal grid. 
A great deal of literature has addressed these challenges, including approaches tailored to forward models
with dynamical structure 
(e.g., \cite{GP_dynamic_emulation, Bayesian_emulation_dynamic, Liu_West_dynamic_emulation, dynamic_nonlinear_simulators_GP}).
A popular alternative is to first approximately express the high-dimensional output as a linear combination of basis vectors 
\begin{align}
\fwd(\Par) &= \sum_{\idxBasis=1}^{\dimBasis} \basisWeight_{\idxBasis}(\Par) \basisVec_{\idxBasis} + \noise_{\basisVec},
\end{align}
and then model the coefficients $\basisWeight_{\idxBasis}(\Par)$ using independent GPs. The basis vectors 
are typically found via a singular value decomposition, though other bases have been explored 
(\cite{HigdonBasis, emulate_functional_output, functionValuedModels, PODemulation}). The surrogate constructed in 
this fashion is simply a linear combination of independent GP emulators, and hence most of the results on forward model 
emulation in this paper are easily extended to this setting. See the appendix for details (\todo). 

\subsubsection{Log-Likelihood Emulation}
We observe in \ref{post_dens} that the forward model evaluations appear in the posterior density only through the 
likelihood. Thus, for the purposes of posterior inference, it is sufficient to directly approximate $\llik(\Par)$ without 
obtaining an approximation of the forward model. It is typically preferable to focus on approximating the logarithm 
of the likelihood, which usually results in a smoother response surface more amenable to GP emulation [\todo: cite]. 
The emulator construction proceeds as before; a GP prior $\llikPrior \sim \GP(\emMeanPrior{\llik}, \emKerPrior{\llik})$ 
is specified and the predictive distribution 
\begin{align*}
\llikEm[\Ndesign] \Def \llikPrior | [\llikPrior(\designIn) = \llik(\designIn)] \sim \GP(\emMean[\Ndesign]{\llik}, \emKer[\Ndesign]{\llik})
\end{align*}
is obtained by conditioning on the design $\{\designIn, \llik(\designIn)\}$. 
The distribution of $\llikEm[\Ndesign]$ is Gaussian, unlike
the log-likelihood surrogate $\llikEmFwd[\Ndesign]$ in the forward model emulation setting. 
The use of GPs to approximate log-likelihoods has been widely used in approximate Bayesian inference 
(\cite{VehtariParallelGP, Kandasamy_2017, llikRBF, trainDynamics, quantileApprox, wang2018adaptive, landslideCalibration})
and Bayesian quadrature (\cite{BayesQuadrature, BayesQuadRatios}). In the context of inverse problems with 
high-dimensional output spaces, log-likelihood emulation has the clear benefit of reducing to a univariate GP setting. 
On the other hand, it has several downsides. Even on the log scale, log-likelihoods can be fast-varying and exhibit large 
dynamic range, proving troublesome for stationary GP priors \cite{wang2018adaptive}. A second potential weakness 
stems from the fact that log-likelihoods often have known bound constraints, which may not be respected by a GP 
prior or predictive distribution. Recent work has dealt with this problem by incorporating constraints into the 
GP hyperparameter optimization \cite{quantileApprox}. A third challenge follows from the fact that, in many applications, 
the likelihood parameters (e.g., $\likPar$) are typically not known and must also be learned from data. An obvious 
solution is to extend the input space of the emulator to include the likelihood parameters, though the resulting response surface 
may prove more challenging to emulate; this approach is explored in \cite{llikRBF}. Alternatively, specific likelihood 
choices may admit a sufficient statistic which can be emulated independently of the likelihood parameters. This method is employed 
in \cite{FerEmulation}, which focuses on the Gaussian likelihood \ref{llik_Gaussian} in the specific case $\likPar = \sigma^2 I$. In this 
case, a surrogate can be constructed to approximate the sufficient statistic $\norm{\obs - \fwd(\Par)}_2^2$, 
which does not depend on the likelihood parameter $\sigma^2$. 
Finally, we note that it may be difficult to incorporate prior domain knowledge 
into the GP prior for the log-likelihood, whereas in certain settings it is easier to do so when constructing a forward model 
surrogate \cite{GP_PDE_priors}. The paper \ref{VehtariParallelGP} finds that a GP with a quadratic mean function tends to 
work well for log-likelihood emulation, perhaps reflecting the fact that many log-likelihoods tend to be well-approximated by 
a quadratic at a coarse scale, with the GP kernel correcting for finer-scale variations. 
Despite all of these challenges, we emphasize that the reduction to a scalar-valued output space is a significant benefit which 
may outweigh the downsides of log-likelihood emulation in many applications. 

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.3\textwidth} % GP dist [fwd]
         \centering
         \includegraphics[width=\textwidth]{{test_1d_visualization/gp_dist_fwdem.png}}
         \caption{$\fwdEm$}
         \label{fig:y equals x}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth} % llik dist [fwd]
         \centering
         \includegraphics[width=\textwidth]{{test_1d_visualization/llik_dist_fwdem.png}}
         \caption{$\llikEmFwd$}
         \label{fig:three sin x}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth} % lik dist [fwd]
         \centering
         \includegraphics[width=\textwidth]{{test_1d_visualization/lik_dist_fwdem.png}}
         \caption{$\Exp{\llikEmFwd}$}
         \label{fig:five over x}
     \end{subfigure}
          \begin{subfigure}[b]{0.3\textwidth} % GP dist [llik]
         \centering
         \includegraphics[width=\textwidth]{{test_1d_visualization/gp_dist_llikem.png}}
         \caption{$\llikEm$}
         \label{fig:y equals x}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth} % llik dist [llik]
         \centering
         \includegraphics[width=\textwidth]{{test_1d_visualization/llik_dist_llikem.png}}
         \caption{$\llikEm$}
         \label{fig:three sin x}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth} % lik dist [llik]
         \centering
         \includegraphics[width=\textwidth]{{test_1d_visualization/lik_dist_llikem.png}}
         \caption{$\Exp{\llikEm}$}
         \label{fig:five over x}
     \end{subfigure}
        \caption{Comparison of forward model and log-likelihood emulation in a simple one-dimensional problem ($\dimPar = \dimObs = 1$).
        The top and bottom rows correspond to the forward model emulation and log-likelihood emulation settings, respectively. 
        The columns correspond to (1) the underlying GP emulator; (2) the log-likelihood emulator induced by the underlying GP; 
        (3) the likelihood emulator induced by the underlying GP. Note that in the log-likelihood emulation case (1) and (2)
        are identical. The blue dashed black line is the ground truth forward model or (log) likelihood and the red points are the 
        design points used for emulator training. The blue line is the mean of the respective emulators, and the gray area 
        covers the area within one standard deviation of the mean.}
        \label{fig:em_dist_1d}
\end{figure}

% Posterior Approximation
\section{Posterior Approximation}
The forward model and log-likelihood emulation approaches result in the random log-likelihood approximations 
$\llikEmFwd[\Ndesign]$ and $\llikEm[\Ndesign]$, respectively. The latter results in a Gaussian predictor 
$\llikEm[\Ndesign](\parMat) \sim \Gaussian(\emMean[\Ndesign]{\llik}(\parMat), \emKer[\Ndesign]{\llik}(\parMat))$ 
at any parameter set $\parMat$. On the other hand, the distribution of $\llikEmFwd[\Ndesign](\parMat)$ is 
typically non-Gaussian and will depend on the specific likelihood under consideration.
Plugging these log-likelihood surrogates into the posterior density expression \ref{post_dens} yields

\begin{align}
\fwdEmRdm[\Ndesign]{\postDensNorm}(\Par) 
&\Def \frac{1}{\fwdEmRdm[\Ndesign]{\normCst}} \fwdEmRdmDens[\Ndesign](\Par) 
\Def  \frac{1}{\fwdEmRdm[\Ndesign]{\normCst}} \priorDens(\Par) \Exp{\llikEmFwd[\Ndesign](\Par)} \label{post_dens_random_fwd} \\
\llikEmRdm[\Ndesign]{\postDensNorm}(\Par) 
&\Def \frac{1}{\llikEmRdm[\Ndesign]{\normCst}} \llikEmRdmDens[\Ndesign](\Par) 
\Def \frac{1}{\llikEmRdm[\Ndesign]{\normCst}} \priorDens(\Par) \Exp{\llikEm[\Ndesign](\Par)} \label{post_dens_random_llik},
\end{align}
with normalizing constants given by 
\begin{align}
\fwdEmRdm[\Ndesign]{\normCst}
&= \int \priorDens(\Par) \Exp{\llikEmFwd[\Ndesign](\Par)} d\Par \label{norm_cst_random_fwd} \\
\llikEmRdm[\Ndesign]{\normCst}
&= \int \priorDens(\Par) \Exp{\llikEm[\Ndesign](\Par)} d\Par \label{norm_cst_random_llik}. 
\end{align}
The unnormalized posterior densities $\fwdEmRdmDens[\Ndesign]$ and $\llikEmRdmDens[\Ndesign]$ are stochastic; 
each sample trajectory from the underlying GP induces a different posterior. 
The papers \cite{StuartTeck1, StuartTeck2} refer to this as the \textit{sample-based}
approximation of the posterior. Given that it is not obvious how to perform inference using these random objects, one typically 
proceeds by choosing a single, deterministic posterior approximation that somehow summarizes the uncertainty in the 
random posterior measure. Popular choices for such deterministic approximations are described below. Going forward, 
for succinctness we will specify only the \textit{unnormalized} posterior densities, with the associated normalizing constants  
implicitly defined. 

\subsection{Approximating the Unnormalized Posterior Density}
Arguably the simplest approach to constructing a deterministic approximation is to reduce the random emulator 
to a deterministic one by considering only the GP predictive mean. 
The plug-in mean estimators yield the unnormalized posterior density approximations 
\begin{align}
\fwdEmMeanDens[\Ndesign](\Par) &\Def \priorDens(\Par) \Exp{\llik(\Par; \emMean[\Ndesign]{\fwd}(\Par))}
 \label{post_dens_mean_fwd} \\
\llikEmMeanDens[\Ndesign](\Par) &\Def \priorDens(\Par) \Exp{\emMean[\Ndesign]{\llik}(\Par)} 
\label{post_dens_mean_llik}.
\end{align}
These approximations fail to account for the uncertainty introduced by the surrogate model, which can lead 
to overconfidence in downstream posterior inference. These mean-based approximations are analyzed 
theoretically in \cite{StuartTeck1}, with error bounds provided in Hellinger distance. In \cite{VehtariParallelGP}, 
the approximation \ref{post_dens_mean_llik} is considered and is instead referred to as the 
\textit{median approximation}, owing to the fact that the median formula for a log-normal random variable 
gives $\text{median}(\Exp{\llikEm[\Ndesign](\Par)}) = \Exp{\emMean[\Ndesign]{\fwd}(\Par)}$. In other words, 
the median of the likelihood emulator yields the same estimator as plugging in the mean of the 
\textit{log}-likelihood emulator.
The approximation $\llikEmMeanDens[\Ndesign](\Par)$ can be shown to be optimal in a certain  
$L^1$ sense \cite{VehtariParallelGP, StuartTeck2}. 

One approach to propagate the 
emulator uncertainty is to marginalize the likelihood with respect to the emulator. Doing so yields the approximations 
\begin{align}
\fwdEmMargDens[\Ndesign](\Par) &\Def \priorDens(\Par) \E_{\fwdEm}\left[\Exp{\llikEmFwd[\Ndesign](\Par)} \right] \label{post_dens_marg_fwd} \\
\llikEmMargDens[\Ndesign](\Par) &\Def \priorDens(\Par) \E_{\llikEm}\left[\Exp{\llikEm[\Ndesign](\Par)} \right], \label{post_dens_marg_llik}
\end{align}
with the expectation subscripts indicating integration with respect to the GP predictive distributions 
$\Gaussian(\emMean[\Ndesign]{\fwd}(\Par), \emKer[\Ndesign]{\fwd}(\Par))$ and
$\Gaussian(\emMean[\Ndesign]{\llik}(\Par), \emKer[\Ndesign]{\llik}(\Par))$, respectively. Note that these expectations are 
computed independently for each $\Par$; the GP predictive covariance structure is not considered. 
We refer to these posteriors as \textit{marginal approximations} following the terminology used in
 \cite{StuartTeck1, StuartTeck2}, which analyze the error of these approximations in the Gaussian likelihood setting. 

Marginal approximations can be justified in various ways. \todo

We now consider performing inference for the approximations \ref{post_dens_marg_fwd} and \ref{post_dens_marg_llik}. 
In the log-likelihood emulation setting, observe that the induced likelihood approximation is log-normally distributed
\begin{align}
\Exp{\llikEm[\Ndesign](\Par)} &\sim \LN(\emMean[\Ndesign]{\llik}(\Par), \emKer[\Ndesign]{\llik}(\Par)), 
\end{align}
which implies that \ref{post_dens_marg_llik} simplifies to 
\begin{align}
\llikEmMargDens[\Ndesign](\Par) 
&\Def \priorDens(\Par) \Exp{\emMean[\Ndesign]{\llik}(\Par) + \frac{1}{2} \emKer[\Ndesign]{\llik}(\Par)} 
\label{post_dens_marg_llik_simplified} \\
&= \llikEmMeanDens[\Ndesign](\Par) \Exp{\frac{1}{2} \emKer[\Ndesign]{\llik}(\Par)}. 
\label{post_dens_marg_var_inflation}
\end{align}
The final expression \ref{post_dens_marg_var_inflation} can be interpreted as a form of posterior inflation 
in regions where the emulator is uncertain. When $\emKer[\Ndesign]{\llik}(\Par) = 0$ the marginal approximation 
reduces to the mean approximation. While the unnormalized posterior density \ref{post_dens_marg_var_inflation}
can be plugged into off-the-shelf sampling algorithms, the uncertainty inflation term can cause highly multi-modal 
distributions which prove difficult for standard MCMC schemes. (\todo: add plot) This difficulty is noted in the 
supplement of \cite{VehtariParallelGP}. 

The expectation in the forward model emulation case 
\ref{post_dens_marg_fwd} is generally intractable, though pseudo-marginal MCMC methods offer a potential 
avenue for inference \cite{pseudoMarginalMCMC}. In the special case of a Gaussian likelihood \ref{llik_Gaussian}, 
the expectation admits the closed-form expression
\begin{align}
\fwdEmMargDens[\Ndesign](\Par) 
&= \priorDens(\Par) \ \E_{\fwdEm}\left[\Gaussian(\obs | \fwdEm[\Ndesign](\Par), \likPar) \right]
\nonumber \\
&= \priorDens(\Par) \ \Gaussian(\obs | \emMean[\Ndesign]{\fwd}(\Par), \likPar + \emKer[\Ndesign]{\fwd}(\Par)),
\label{post_dens_marg_fwd_Gaussian}
\end{align}
which follows immediately from the fact that the expectation is of the form of a convolution 
of two Gaussian densities (see \Cref{prop:Gaussian_marginal_moments} in the appendix). 
The marginal approximation \ref{post_dens_marg_fwd_Gaussian}
can be interpreted as the posterior distribution of the modified Bayesian inverse problem 
\begin{align}
\obs &= \emMean[\Ndesign]{\fwd}(\Par) + v(\Par) + \noise \label{inv_prob_Gaussian_marginal_approx} \\
v(\Par) &\sim \Gaussian(0, \emKer[\Ndesign]{\fwd}(\Par)) \nonumber \\
\noise &\sim \Gaussian(0, \likPar) \nonumber \\
\Par &\sim \priorDens \nonumber, 
\end{align}
where $v(\Par)$ and $\noise$ are independent. It is interesting to note that the GP uncertainty in 
\ref{post_dens_marg_fwd_Gaussian} manifests in the form of additive variance inflation, distinct 
from the multiplicative uncertainty inflation in \ref{post_dens_marg_var_inflation} \cite{GP_PDE_priors}. 
The Gaussian posterior approximation \ref{post_dens_marg_fwd_Gaussian} has been considered in 
\cite{StuartTeck2, hydrologicalModel, GP_PDE_priors, GP_PDE_priors, CES, idealizedGCM, weightedIVAR}. 

Finally we note that generalizations of $\text{median}(\Exp{\llikEm[\Ndesign](\Par)})$ have been considered, 
resulting in the $\quantileProb$-quantile, $\quantileProb \in (0,1)$ approximations 
\begin{align}
\fwdEmQDens[\Ndesign](\Par) &\Def \priorDens(\Par) \quantile \left[\Exp{\llikEmFwd[\Ndesign](\Par)} \right] \label{post_dens_quantile_fwd} \\
\llikEmQDens[\Ndesign](\Par) &\Def \priorDens(\Par) \quantile \left[\Exp{\llikEm[\Ndesign](\Par)} \right]. \label{post_dens_quantile_llik}
\end{align}
The log-likelihood emulation case \ref{post_dens_quantile_llik} simplifies to 
\begin{align}
\llikEmQDens[\Ndesign](\Par) 
&= \priorDens(\Par) \Exp{\emMean[\Ndesign]{\llik}(\Par) + \GaussianCDF^{-1}(\quantileProb) \sqrt{\emKer[\Ndesign]{\llik}(\Par)}} 
\label{post_dens_quantile_llik_simplified} \\
&= \llikEmMeanDens[\Ndesign](\Par) \Exp{\GaussianCDF^{-1}(\quantileProb) \sqrt{\emKer[\Ndesign]{\llik}(\Par)}},
\label{post_dens_quantile_llik_var_inflation}
\end{align}
with $\GaussianCDF$ denoting the standard Gaussian distribution function. The approximate posterior 
\ref{post_dens_quantile_llik_simplified} is considered in \cite{VehtariParallelGP, quantileApprox}. Larger choices 
of $\alpha$ correspond to more conservative approximations in the sense of inducing a larger degree of posterior 
inflation in regions where the GP is uncertain. As compared with $\llikEmMeanDens[\Ndesign](\Par)$
\ref{post_dens_marg_var_inflation}, we notice that the quantile approximation \ref{post_dens_quantile_llik_var_inflation}
induces a ``softer'' form of uncertainty inflation, with the inflation factor scaling with the exponentiated GP standard 
deviation instead of the variance. 

\subsection{Approximate MCMC}
We now interpret the approximate posteriors introduced in the previous section as inducing different 
approximations of the Metropolis-Hastings (MH) acceptance probability. This perspective motivates 
an alternative approach to GP-induced posterior approximation; instead of first deriving an 
approximation of the unnormalized posterior density, we consider the approximation to the MCMC 
scheme as the starting point. In addition to deterministic approximations,
we consider a class of MCMC algorithms which produce stochastic approximations to the MH acceptance 
probability, and thus fall into the noisy MCMC 
framework which has been considered in the literature 
\cite{noisyMCMC, stabilityNoisyMH, noisyMCSurvey, pseudoMarginalMCMC}. 

\subsubsection{Deterministic Metropolis-Hastings Approximations}
We recall that the MH algorithm is defined by a proposal kernel, with density that we denote by 
$\propDens(\Par, \cdot)$. If the Markov chain is in the current state $\Par \in \parSpace$ then 
the next state is defined by sampling a proposal $\propPar \sim \propDens(\Par, \cdot)$ which is 
accepted with probability $\accProbMH(\Par, \propPar)$, defined by 

\begin{align}
&\accProbMH(\Par, \propPar) = 
\min\left\{1, \frac{\priorDens(\propPar) \Exp{\llik(\propPar)} \propDens(\propPar, \Par)}{\priorDens(\Par) \Exp{\llik(\Par)} \propDens(\Par, \propPar)} \right\}.
\label{MH_acc_prob_exact}
\end{align}
If accepted, the next state is defined to be $\propPar$, else it is set to the current state $\Par$. 
This procedure is summarized in \Cref{alg:MH}. For notational convenience in deriving approximations 
of the acceptance probability, we define 
\begin{align}
&\accProbRatio(\Par, \propPar) 
= \frac{\priorDens(\propPar) \Exp{\llik(\propPar)} \propDens(\propPar, \Par)}{\priorDens(\Par) \Exp{\llik(\Par)} \propDens(\Par, \propPar)}, 
&&\likRatio(\Par, \propPar) = \frac{\Exp{\llik(\propPar)}}{\Exp{\llik(\Par)}},
\label{MH_acc_prob_exact}
\end{align}
and refer to $\accProbRatio(\Par, \propPar)$ and $\likRatio(\Par, \propPar)$ 
as the \textit{acceptance ratio} and \textit{likelihood ratio}, respectively.

\begin{algorithm}
    \caption{Metropolis-Hastings}
    \label{alg:MH}
    \begin{algorithmic}[1] % The number tells where the line numbering should start
    \Function{MH}{$\indexMCMC[0]{\Par}, \NMCMC$}     
        \For{$\mcmcIndex \gets1,\NMCMC$} 
        		\State $\propPar \sim \propDens(\indexMCMC{\Par}, \cdot)$ \Comment{Proposal}
		\State $\accProbMH(\indexMCMC{\Par}, \propPar) \gets \min\left\{1, \frac{\priorDens(\propPar)\Exp{\llik(\propPar)}
				\propDens(\propPar, \indexMCMC{\Par})}{\priorDens(\indexMCMC{\Par}) \Exp{\llik(\indexMCMC{\Par})}  \propDens(\indexMCMC{\Par}, \propPar)} \right\}$
		\State $b \sim \text{Bernoulli}(\accProbMH(\indexMCMC{\Par}, \propPar))$
		\If{$b = 1$}
			\State $\indexMCMC[\mcmcIndex+1]{\Par} \gets \propPar$ 
		\Else
			\State $\indexMCMC[\mcmcIndex+1]{\Par} \gets \indexMCMC{\Par}$
		\EndIf
	\EndFor
	\EndFunction
    \end{algorithmic}
\end{algorithm}

We can define approximate surrogate-based MCMC algorithms by utilizing $\llikEm[\Ndesign]$ 
or $\llikEmFwd[\Ndesign]$ to approximate $\accProbMH(\Par, \propPar)$, either directly or by plugging 
in approximations of $\accProbRatio(\Par, \propPar)$ or $\likRatio(\Par, \propPar)$.  
For brevity, we focus on the log-likelihood emulation setting here, as the definitions are similar in the forward 
model emulation case. 
We thus consider the random variables $\Em[\Ndesign]{\likRatio}(\Par, \propPar)$,
$\Em[\Ndesign]{\accProbRatio}(\Par, \propPar)$, and 
$\Em[\Ndesign]{\accProbMH}(\Par, \propPar)$ defined by inserting the emulated log-likelihoods 
$\llikEm[\Ndesign](\propPar)$ and $\llikEm[\Ndesign](\Par)$ in place of their exact counterparts
in \ref{MH_acc_prob_exact}. The mean, marginal, and quantile approximate posteriors introduced in the previous 
section correspond to the following deterministic, plug-in estimates of $\Em[\Ndesign]{\likRatio}(\Par, \propPar)$:
\begin{align}
\llikEmMean[\Ndesign]{\likRatio}(\Par, \propPar) 
\coloneqq \Exp{\emMean[\Ndesign]{\llik}(\propPar) - \emMean[\Ndesign]{\llik}(\Par)} \\
\llikEmMarg[\Ndesign]{\likRatio}(\Par, \propPar) 
\Def \llikEmMean[\Ndesign]{\likRatio}(\Par, \propPar) 
\Exp{\frac{1}{2}\left[\emKer[\Ndesign]{\llik}(\propPar) - \emKer[\Ndesign]{\llik}(\Par)\right]} \\
\llikEmQ[\Ndesign]{\likRatio}(\Par, \propPar) 
\Def \llikEmMean[\Ndesign]{\likRatio}(\Par, \propPar) 
\Exp{\GaussianCDF^{-1}(\quantileProb)  \left[\sqrt{\emKer[\Ndesign]{\llik}(\propPar)} - \sqrt{\emKer[\Ndesign]{\llik}(\Par)}\right]}.
\end{align}
Notice that the marginal and quantile approximations inflate the probability of acceptance when the 
uncertainty at the proposed state $\propPar$ is greater than that at the current state $\Par$.
All three of these approximations are derived by independently approximating the numerator and denominator 
in the ratio $\Exp{\llik(\propPar)} / \Exp{\llik(\Par)}$ and then dividing the two approximations.
However, the GP emulator $\llikEm[\Ndesign]$ is a random function, meaning that 
$\llikEm[\Ndesign](\propPar)$ and $\llikEm[\Ndesign](\Par)$ are typically correlated, the covariance
being given by $\emKer[\Ndesign]{\llik}(\propPar, \Par)$. It is therefore natural to consider incorporating 
this information into approximations of the likelihood ratio. We define a second marginal approximation 
by marginalizing $\Em[\Ndesign]{\likRatio}(\Par, \propPar)$ with respect to the \textit{joint} 
distribution of $[\llikEm[\Ndesign](\propPar), \llikEm[\Ndesign](\Par)]$, which gives
\begin{align}
\llikEmJointMarg[\Ndesign]{\likRatio}(\Par, \propPar) 
&\Def \E_{\llikEm[\Ndesign]} \left[\Em[\Ndesign]{\likRatio}(\Par, \propPar) \right] \\
&= \llikEmMean[\Ndesign]{\likRatio}(\Par, \propPar)  \Exp{\frac{1}{2} \Var\left[\llikEm[\Ndesign](\propPar) - \llikEm[\Ndesign](\Par) \right]} \\
&=  \llikEmMean[\Ndesign]{\likRatio}(\Par, \propPar)  
\Exp{\frac{1}{2}\left[\emKer[\Ndesign]{\llik}(\propPar) + \emKer[\Ndesign]{\llik}(\Par) - 2\emKer[\Ndesign]{\llik}(\propPar, \Par) \right]}. 
\label{lik_ratio_joint_marg}
\end{align} 
The joint marginal approximation inflates the acceptance probability when the GP is uncertain about the 
\textit{difference} in log-likelihood values at the locations $\propPar$ and $\Par$. For example, it is possible
that the marginal uncertainty at each location is large, but a large covariance between the locations 
might imply little uncertainty in the difference $\llikEm[\Ndesign](\propPar) - \llikEm[\Ndesign](\Par)$. 

The marginal approximation can be taken a step further by integrating $\llikEm$ out of 
$\Em[\Ndesign]{\accProbRatio}(\Par, \propPar)$ or $\Em[\Ndesign]{\accProbMH}(\Par, \propPar)$ instead of 
$\Em[\Ndesign]{\likRatio}(\Par, \propPar)$. The former case 
\begin{align}
\llikEmJointMarg[\Ndesign]{\accProbRatio}(\Par, \propPar) 
&= \frac{\priorDens(\propPar) \propDens(\propPar, \Par)}{\priorDens(\Par) \propDens(\Par, \propPar)} 
\llikEmJointMarg[\Ndesign]{\likRatio}(\Par, \propPar)
\label{acc_ratio_joint_marg}
\end{align}
follows immediately from \ref{lik_ratio_joint_marg}, 
given that $\accProbRatio(\Par, \propPar)$ differs from $\likRatio(\Par, \propPar)$ only by a multiplicative 
constant (with respect to the emulator). Thus, the approximations 
$\llikEmJointMarg[\Ndesign]{\accProbRatio}(\Par, \propPar)$ and 
$\llikEmJointMarg[\Ndesign]{\likRatio}(\Par, \propPar)$ induce identical inference algorithms. 
Marginalizing the acceptance probability directly proves more interesting, yielding
\begin{align}
\llikEmJointMarg[\Ndesign]{\accProbMH}(\Par, \propPar) 
&\Def \E_{\llikEm[\Ndesign]} \left[\min\left\{1, \Em[\Ndesign]{\accProbRatio}(\Par, \propPar)\right\} \right] \\
&= w_1(\Par, \propPar) + w_2(\Par, \propPar) \llikEmJointMarg[\Ndesign]{\accProbRatio}(\Par, \propPar),
\label{acc_prob_joint_marg}
\end{align} 
where 
\begin{align*}
w_1(\Par, \propPar) &\Def \Prob(\Em[\Ndesign]{\accProbRatio}(\Par, \propPar) \geq 1) \\
w_2(\Par, \propPar) &\Def \Prob\left(\Em[\Ndesign]{\accProbRatio}(\Par, \propPar) \leq \Exp{-\Var\left[\llikEm[\Ndesign](\propPar) - \llikEm[\Ndesign](\Par) \right]}\right).
\end{align*}
This is derived in \Cref{prop:joint-marg-accept-prob} in the appendix. While the plug-in acceptance probability 
approximation based on $\llikEmJointMarg[\Ndesign]{\accProbRatio}(\Par, \propPar)$ yields 
$\min\left\{1, \llikEmJointMarg[\Ndesign]{\accProbRatio}(\Par, \propPar) \right\}$, 
$\llikEmJointMarg[\Ndesign]{\accProbMH}(\Par, \propPar)$ instead takes the form of a linear combination of 
$1$ and $\llikEmJointMarg[\Ndesign]{\accProbRatio}(\Par, \propPar)$, with weights given by the probabilities 
$w_1(\Par, \propPar)$, $w_2(\Par, \propPar)$.

\subsubsection{Noisy Metropolis-Hastings Approximations}
The previous section considered deterministic approximations to the MH acceptance probability. Here we turn 
to random approximations, thus entering the realm of so-called ``noisy'' MCMC algorithms 
\cite{noisyMCMC, stabilityNoisyMH, noisyMCSurvey, pseudoMarginalMCMC}. The general framework,
summarized in \Cref{alg:noisy-MH}, is characterized by the choice of distribution $\llikSampDist_{\Par, \propPar}$
from which to sample log-likelihood values $\llikSamp$ and $\llikSampProp$ which replace the exact 
log-likelihood evaluations $\llik(\Par)$ and $\llik(\propPar)$ when computing the MH acceptance probability. 
For brevity we will continue to specialize the definitions to the log-likelihood emulation setting, but the forward model 
emulation case is nearly identical, with $\llikEmFwd$ replacing $\llikEm$. 

\begin{algorithm}
    \caption{Noisy Metropolis-Hastings}
    \label{alg:noisy-MH}
    \begin{algorithmic}[1] % The number tells where the line numbering should start
    \Function{noisyMH}{$\indexMCMC[0]{\Par}, \NMCMC, \propDens, \llikSampDist$}     
        \For{$\mcmcIndex \gets1,\NMCMC$} 
        		\State $\propPar \sim \propDens(\indexMCMC{\Par}, \cdot)$ 
		\State $(\llikSamp, \llikSampProp) \sim \llikSampDist_{\indexMCMC{\Par}, \propPar}$  \Comment{Sample log-likelihood values}
		\State $\accProbMH(\indexMCMC{\Par}, \propPar) \gets \min\left\{1, \frac{\priorDens(\propPar)e^{\llikSampProp} 
				\propDens(\propPar, \indexMCMC{\Par})}{\priorDens(\indexMCMC{\Par}) e^{\llikSamp}  \propDens(\indexMCMC{\Par}, \propPar)} \right\}$
		\State $b \sim \text{Bernoulli}(\accProbMH(\indexMCMC{\Par}, \propPar))$
		\If{$b = 1$}
			\State $\indexMCMC[\mcmcIndex+1]{\Par} \gets \propPar$ 
		\Else
			\State $\indexMCMC[\mcmcIndex+1]{\Par} \gets \indexMCMC{\Par}$
		\EndIf
	\EndFor
	\EndFunction
    \end{algorithmic}
\end{algorithm}

We define the following noisy MCMC algorithms based on different choices for $\llikSampDist_{\Par, \propPar}$.
Below we consider three such choices. In each case, we show that the transition kernel defining the Markov chain 
of the respective algorithm coincides with that of one of the deterministic approximations considered in the previous 
section. Thus, the noisy MCMC algorithms provide alternative inference algorithms for sampling from the previously 
considered approximate posteriors. Moreover, in the forward model emulation setting, they also generalize to 
situations in which the deterministic approximations yield intractable integrals. For example, these algorithms are 
not restricted to the setting of a Gaussian likelihood when using forward model surrogates. 
We use the notation $\parMat \Def \{\Par, \propPar\}$ in the below definitions. 

\paragraph{Joint Monte Carlo Within Metropolis-Hastings.}
We first consider setting $\llikSampDist_{\Par, \propPar}$ to be the joint distribution of 
$[\llikEm[\Ndesign](\Par), \llikEm[\Ndesign](\propPar)]$, 
\begin{align}
\llikSampDist_{\Par, \propPar} = \Gaussian(\emMean[\Ndesign]{\llik}(\parMat), \emKer[\Ndesign]{\llik}(\parMat)).
\label{llik_dist_MCWMH-joint}
\end{align}
We call the resulting algorithm \textit{Joint Monte Carlo within Metropolis-Hastings (MCWMH-joint)}. 
Like the deterministic joint marginal approximations \ref{lik_ratio_joint_marg} and \ref{acc_prob_joint_marg}, 
the \textit{MCWMH-joint} algorithm takes into account the GP covariance structure. Note that since the log-likelihood
values are sampled independently across iterations, the \textit{MCWMH-joint} algorithm does indeed define a valid 
Markov chain. The transition kernel of this Markov chain is given by 
\footnote{We denote by $\delta_{x}$ the Dirac measure centered at a point $x$, which assigns a unit point mass to $x$.}
\begin{align}
\llikEmJointMCWMH[\Ndesign]{\MarkovKernel}(\Par, A) 
&= \int_{A} \propDens(\Par, \propPar) \llikEmJointMarg[\Ndesign]{\accProbMH}(\Par, \propPar) d\propPar 
+ \delta_{\Par}(A) \left[1 - \int_{\parSpace} \propDens(\Par, \propPar) \llikEmJointMarg[\Ndesign]{\accProbMH}(\Par, \propPar) d\propPar \right],
\label{MCWMH-joint-kernel}
\end{align}
which is derived in the appendix, \Cref{transition_kernel_derivations}. This is precisely the transition kernel of the Markov chain 
defined by the deterministic joint marginal approximation of the acceptance probability \ref{acc_prob_joint_marg}. 

\paragraph{Independent Monte Carlo Within Metropolis-Hastings.}
An alternative is to sample the log-likelihood values independently, without considering the GP covariance; i.e., 
\begin{align}
\llikSampDist_{\Par, \propPar} 
= \Gaussian\left(\emMean[\Ndesign]{\llik}(\parMat), \text{diag}\left\{\emKer[\Ndesign]{\llik}(\Par), \emKer[\Ndesign]{\llik}(\propPar)\right\}\right).
\label{llik_dist_MCWMH-ind}
\end{align}
We refer to the resulting algorithm as \textit{Independent Monte Carlo Within Metropolis-Hastings (MCWMH-ind)}. Analogously to the 
joint case, the transition kernel of this algorithm is given by \ref{MCWMH-joint-kernel} but with the marginal acceptance probabilities computed
with respect to $\Gaussian\left(\emMean[\Ndesign]{\llik}(\parMat), \text{diag}\left\{\emKer[\Ndesign]{\llik}(\Par), \emKer[\Ndesign]{\llik}(\propPar)\right\}\right)$
in place of the full joint distribution. The \textit{MCWMH-ind} algorithm is utilized to calibrate expensive ecosystem models 
in \cite{FerEmulation}. 

\paragraph{Pseudo-Marginal Metropolis-Hastings.}
Note that the two previous algorithms produce new log-likelihood samples at both the current $\Par$ and proposed 
$\propPar$ locations at each iteration of the algorithm. An alternative is to draw a new sample only at the 
proposed location, while recycling the sample at the current location from the previous iteration. Formally, this procedure
implies the product measure 
\begin{align}
\llikSampDist_{\Par, \propPar} 
= \delta_{\llikSamp} \otimes  
\Gaussian(\emMean[\Ndesign]{\llik}(\propPar), \emKer[\Ndesign]{\llik}(\propPar)), \label{llik_dist_pseudo_marg}
\end{align}
which precisely falls into the pseudo-marginal MCMC framework \cite{pseudoMarginalMCMC}. 
We thus term 
this algorithm \textit{Pseudo-Marginal Metropolis-Hastings (MH-pseudo-marg)}. Given a log-likelihood sample 
$\llikSamp$, then by definition the unnormalized posterior approximation $\priorDens(\Par) \Exp{\llikSamp}$ is 
an unbiased estimator of 
$\llikEmMargDens[\Ndesign](\Par) = \E_{\llikEm[\Ndesign]}\left[\priorDens(\Par) \Exp{\llikEm[\Ndesign](\Par)} \right]$. 
This implies that the \textit{MH-pseudo-marg} algorithm defines a Markov chain with target distribution 
$\llikEmMargDens[\Ndesign]$, which was already defined in \ref{post_dens_marg_llik}. 


% Sequential Design
\section{Sequential Design}
In applications, the GP emulator is often constructed sequentially. In this section, we consider augmenting the 
initial design $\designIn[\Ndesign]$ with new input points that are chosen to yield maximal improvement in the 
GP-induced posterior approximation. This can be thought of as a question of experimental design for the 
solution of a Bayesian inverse problem. We begin by reviewing the relevant background on GP-based 
sequential design, and then proceed to discuss design methods that are tailored to the goal of 
posterior approximation.  

\subsection{Background: Sequential Design for Gaussian Processes}

\subsubsection{The Sequential Design Loop}
Sequential design algorithms for GPs have been studied extensively across several fields. We introduce this topic 
for a generic GP predictive distribution $\funcEm[\Ndesign] \Def \funcPrior|[\funcPrior(\designIn) = \func(\designIn)]$, as defined in 
\ref{generic_gp_conditional}. We now consider the question of identifying a new set of $\Nbatch$
inputs $\designBatchIn$ to augment the initial design. The augmented design is thus given by 
$\{\designIn[\Naugment], \func(\designIn[\Naugment])\}$, where 
$\designIn[\Naugment] \Def \designIn[\Ndesign] \cup \designBatchIn$. 
Conditioning on the augmented design results in the updated GP
\begin{align}
\funcEm[\Naugment] \Def \funcPrior|[\funcPrior(\designIn[\Naugment]) = \func(\designIn[\Naugment])] \sim \GP(\gpMean[\Naugment], \gpKer[\Naugment]),
\end{align}
where the predictive mean $\gpMean[\Naugment]$ and covariance $\gpKer[\Naugment]$ functions are given by \ref{kriging_eqns}
with $\{\designIn, \func(\designIn)\}$ replaced by $\{\designIn[\Naugment], \func(\designIn[\Naugment])\}$. 
We note that conditioning in stages is equivalent to conditioning on the entire design at once; i.e.,
$\funcPrior|[\funcPrior(\designIn[\Naugment]) = \func(\designIn[\Naugment])] \eqDist 
\funcEm[\Ndesign]|[\funcEm[\Ndesign](\designIn[\Nbatch]) = \func(\designBatchIn)]$, which is justified 
by \Cref{lemma:gp-condition-order} in the appendix. 
The update $\funcEm[\Ndesign] \to \funcEm[\Naugment]$ 
can be computed in $\BigO(\Nbatch^3 + \Ndesign^2 \Nbatch + \Nbatch^2 \Ndesign)$ operations, 
a significant improvement over the naive $\BigO([\Ndesign + \Nbatch]^3)$
implementation (courtesy of the partitioned matrix inverse identity stated in \Cref{partitioned-matrix-inverse}). 
Naturally, the new batch $\designBatchIn$ should be chosen to 
yield maximal improvement in the GP model with respect to the downstream task that is to be performed. 
This idea is formalized by an optimization problem of the form 
\begin{align}
\designBatchIn = \argmin_{\designBatchIn^\prime \subset \parSpace} \acq[\Ndesign](\designBatchIn^\prime), \label{acq_func_opt}
\end{align}
where $\acq[\Ndesign]: \parSpace^{\Nbatch} \to \R$ is an \textit{acquisition function} (i.e., \textit{design criterion}) that 
encodes the quality of a batch of design points in terms of the task at hand. Iterating this procedure $\Nrounds$ times yields the so-called 
\textit{sequential design} (a.k.a., \textit{active learning}) loop, summarized in \Cref{alg:seq_des_loop}.  

\begin{algorithm}
    \caption{Gaussian Process Sequential Design Loop}
    \label{alg:seq_des_loop}
    \begin{algorithmic}[1] % The number tells where the line numbering should start
    \Function{SeqDesign}{$\func, \funcPrior, \designIn[\Ndesign], \Nrounds$}     
	\State $\hat{\func} \gets \funcPrior|[\funcPrior(\designIn[\Ndesign]) = \func(\designIn[\Ndesign])] \sim \GP(\gpMean[\Ndesign], \gpKer[\Ndesign])$
	\Comment{Initial Design}
        \For{$\designIndex \gets 1,\Nrounds$} \Comment{Sequential Design Loop}
        		\State $\designBatchIn \gets \argmin_{\designBatchIn^\prime \subset \parSpace} \acq[\Ndesign](\designBatchIn^\prime)$ 
		\State $\hat{f} \gets \hat{f} | [\hat{f}(\designBatchIn) = \func(\designBatchIn)]$
	\EndFor
	\EndFunction
    \end{algorithmic}
\end{algorithm}

One could of course consider 
varying the batch size $\Nbatch$ across the $\Nrounds$ iterations, but to simplify notation we keep $\Nbatch$ constant. We note that the 
special case $\Nbatch = 1$ corresponds to the pure sequential design setting, in which the function $\func$ is evaluated at each acquired 
point before considering the subsequent acquisition. The \textit{batch} sequential design setting $\Nbatch > 1$ is more challenging,  
owing to the fact that $\func$ is only evaluated after an entire batch $\designBatchIn$ has been acquired, meaning that inputs must 
be selected without observing the outputs of the other points in the batch. 
We note that, while the design framework summarized in \Cref{alg:seq_des_loop} does allow for consideration 
of within-batch interactions between design points, 
it is still ``myopic'' in the sense that it disregards the potential for future acquisitions. Non-myopic strategies have been considered
through a dynamic programming lens, though they typically come at the cost of significant 
computational expense \cite{SURThesis, supermartingaleSUR}.


\subsubsection{Acquisition Functions}
We now summarize some particular choices of acquisition function $\acq[\Ndesign]$ which are popular in practice. These will form 
the basis for extensions that target the specific goal of GP-induced posterior approximation. We begin by emphasizing that the 
choice of acquisition function should reflect how the GP is being used in downstream tasks. Perhaps the most common goal is that 
of optimization. The field of \textit{Bayesian optimization} is dedicated to this question, which has produced a wide variety
acquisition functions that are tailored to the task of optimizing expensive black-box functions $\func$; 
see \cite{reviewBayesOpt, gramacy2020surrogates} for in-depth reviews.  
The related field of \textit{Bayesian quadrature} \cite{BayesQuadrature, BayesQuadrature, BayesQuadRatios}
instead focuses on the goal of approximating integrals with expensive integrands. The computer experiments community 
focuses on various tasks, including contour estimation and reliability analysis \cite{contourEstimation, cole2021entropybased}. 
Perhaps the most common goal considered in the sequential design of computer experiments is to fit a response surface
(see \cite{gramacy2020surrogates, SanterCompExp, design_analysis_computer_experiments} for reviews), 
which typically leads to design procedures 
that seek to ``evenly'' fill the input space $\parSpace$ in some sense.  
In such settings, a reasonable approach is to choose points in locations where the GP is most uncertain. 
Defining ``uncertainty'' in terms 
of variance or entropy yields the \textit{maximum variance (max-var)} and \textit{maximum entropy (max-ent)} acquisition functions, 
respectively, 
\begin{align}
&\labelAcq[\Ndesign]{\maxvarLabel}(\Par) \Def -\Var[\funcEm[\Ndesign](\Par)] , 
&&\labelAcq[\Ndesign]{\maxentLabel}(\designBatchIn) \Def -\Ent[\funcEm[\Ndesign](\designBatchIn)], \label{acq_max_var_ent}
\end{align} 
where $\Ent[Z]$ denotes the entropy of a random variable $Z$. The variance and entropy are negated to align with our convention 
of minimizing acquisition functions. The $\maxvarLabel$ criterion does not have an obvious generalization to the batch setting, 
so is defined to only accept a single input $\Par$. A noted weakness of these criteria is their locality; 
they target the single point (sets) with maximal uncertainty, ignoring the effect of the acquisition on the remainder of the design space. 
This often favors points at the boundaries of $\parSpace$, behavior that is typically not desirable \cite{gramacy2020surrogates}. 
To address this weakness, an alternative approach is to minimize the \textit{conditional} uncertainty 
($\Var[\funcEm[\Ndesign](\Par) | \designBatchIn]$ or $\Ent[\funcEm[\Ndesign](\Par) | \designBatchIn]$), \textit{averaged} over 
the input space $\parSpace$. The resulting \textit{integrated uncertainty criteria}
\begin{align}
\labelAcq[\Ndesign]{\intvarLabel}(\designBatchIn)
&= \int_{\parSpace} \Var[\funcEm[\Ndesign](\Par) | \funcEm(\designBatchIn)=\func(\designBatchIn)] \weightDens(\Par) d\Par, \label{acq_int_var} \\ 
\labelAcq[\Ndesign]{\intentLabel}(\designBatchIn)
&= \int_{\parSpace} \Ent[\funcEm[\Ndesign](\Par) | \funcEm(\designBatchIn)=\func(\designBatchIn)] \weightDens(\Par) d\Par, \label{acq_int_ent}
\end{align}
are naturally defined in the batch setting and account for the global effect of acquiring new design points. Here $\weightDens$ is 
a probability density on $\parSpace$, allowing for the expectation over $\parSpace$ to weighted. 
A nice summary and analysis of integrated variance criteria of the form \ref{acq_int_var} is given 
in \cite{Mercer_kernels_IVAR}, with an emphasis on one-shot, rather than sequential, design. 
Since $\Var[\funcEm[\Ndesign](\Par) | \funcEm(\designBatchIn)=\func(\designBatchIn)] = \gpKer[\Naugment](\Par)$ and 
$\Ent[\funcEm[\Ndesign](\Par) | \funcEm(\designBatchIn)=\func(\designBatchIn)] = \frac{1}{2} \log(2\pi e \gpKer[\Naugment](\Par))$, the integrands of 
\ref{acq_int_var} and \ref{acq_int_ent} are available in closed-form. However, outside of special choices for $\parSpace$ and 
$\weightDens$ (see, e.g., \cite{Binois_2018}), the integrals (with respect to $\Par$) are typically replaced with Monte Carlo 
approximations. It is worth noting that 
these acquisition functions must be computed without observing the function evaluations $\func(\designBatchIn)$, which is 
not an issue for any of the above acquisitions given that $\gpKer[\Naugment](\Par)$ is independent of $\func(\designBatchIn)$ 
(see \ref{kriging_eqns}). However, the predictive mean $\gpMean[\Naugment](\Par)$ does depend on $\func(\designBatchIn)$, 
a fact which will become relevant in the subsequent section. We therefore state a lemma below which summarizes the 
dependence of the predictive mean on $\func(\designBatchIn)$, under the assumption that the latter is distributed according 
to the current GP $\funcEm[\Ndesign]$. A similar result is also provided in \cite{VehtariParallelGP}. 
\begin{lemma} \label{lemma:pred-mean-dist}
Let $\{\designBatchIn, \funcVal[\Nbatch]\}$ be a new batch of $\Nbatch$ design points, with
$\designBatchFunc | \designBatchIn \sim \Gaussian(\gpMean[\Ndesign](\designBatchIn), \gpKer[\Ndesign](\designBatchIn))$. 
Let  
\begin{align}
\gpMean[\Ndesign](\Par | \designBatchFunc) 
\Def \E[\funcEm[\Ndesign](\Par) | \funcEm[\Ndesign](\designBatchIn) = \designBatchFunc] \label{conditional-mean-notation}
\end{align}
denote the predictive mean after conditioning $\funcEm[\Ndesign]$ on 
$\{\designBatchIn, \funcVal[\Nbatch]\}$. Then, viewed as a function of the random variable 
$\designBatchFunc$, the conditional mean $\gpMean[\Ndesign](\Par| \designBatchFunc)$ is 
distributed as 
\begin{align}
\gpMean[\Ndesign](\Par | \designBatchFunc)
\sim \Gaussian\left(\gpMean[\Ndesign](\Par), 
                               \gpKer[\Ndesign](\Par) - \gpKer[\Naugment](\Par) \right) \label{pred-mean-dist}.
\end{align}
\end{lemma}
We see that the expectation (with respect to $\designBatchFunc$) of $\gpMean[\Ndesign](\Par| \designBatchFunc)$ 
is equal to the current predictive mean $\gpMean[\Ndesign](\Par)$. Since 
$\gpMean[\Ndesign](\Par) = \gpMean[\Ndesign](\Par | \gpMean[\Ndesign](\designBatchIn))$, this can also be 
interpreted as the predictive mean resulting from the conditioning of $\funcEm[\Ndesign]$ on the ``pseudo-data''
$\{\designBatchIn, \gpMean[\Ndesign](\designBatchIn)\}$; i.e., treating the current GP predictive mean as if it were 
the true response. The variance of 
$\gpMean[\Ndesign](\Par | \designBatchFunc)$ is equal to the change in GP variance at the input $\Par$ due to the act 
of conditioning on the batch $\designBatchIn$. Therefore, the uncertainty in $\gpMean[\Ndesign](\Par | \designBatchFunc)$ 
is large when the design points $\designBatchIn$ are more ``influential.''

\subsection{Acquisition Functions for Posterior Approximation}
The previous section introduced several acquisition functions that seek to improve GP predictions by targeting
design point batches that cause large reductions of uncertainty in the GP predictive distribution. While such criteria may be appropriate 
when quality predictions are required across all of $\parSpace$, this may not align well with the goal of constructing a GP emulator 
which induces an accurate posterior approximation. In the Bayesian inverse problem setting, it has repeatedly been noted that the 
posterior is often highly concentrated relative to the prior. Intuitively, one would therefore expect that the GP emulator need only 
achieve a high predictive accuracy in the region with high posterior mass, and avoid wasteful acquisition of design points in 
regions with negligible mass. This intuition is placed on more rigorous ground in \cite{StuartTeck2}, in which bounds on the 
posterior error are given in terms of the likelihood approximation error in a $L^2_{\postDens}(\parSpace)$ sense. The authors 
stated motivation was to provide rigorous justification for the use of ensemble Kalman methods as a cheap experimental 
design method for the training of GP emulators \cite{CES, idealizedGCM}. These so-called ensemble Kalman inversion methods 
often require $\BigO(10^3)$ sequential forward model evaluations, a significant improvement over MCMC but still intractable for 
forward models of significant computational expense, such as those that motivated the present work. 

We instead focus on design methods that fit within the sequential design framework summarized in \Cref{alg:seq_des_loop}, and 
are tailored to the goal of posterior approximation. One such approach is to define acquisition functions based on 
the unnormalized posterior density emulator ($\fwdEmRdm[\Ndesign]{\postDens}(\Par)$ or $\llikEmRdmDens[\Ndesign](\Par)$), 
rather than the underlying GPs themselves. The problem of GP-based sequential design for Bayesian inverse problems  
has gained recent attention, and is addressed in 
\cite{SinsbeckNowak, Surer2023sequential, VehtariParallelGP, briol2017sampling, ranjan2016inverse, 
	landslideCalibration, KandasamyActiveLearning2015, Kandasamy_2017, wang2018adaptive,   
	weightedIVAR, quantileApprox, hydrologicalModel, briol2017sampling}. 
[\todo: need to look at the method used in \cite{quantileApprox, hydrologicalModel} more closely.]
As an initial example, we consider tailoring the $\maxvarLabel$ and $\maxentLabel$ criteria to the task of posterior approximation 
using a log-likelihood emulator. The natural analogs of these acquisition functions in this setting are 
\begin{align}
&\labelAcq[\Ndesign]{\maxexpvarLabel}(\Par) \Def -\Var[\llikEmRdmDens[\Ndesign](\Par)] , 
&&\labelAcq[\Ndesign]{\maxexpentLabel}(\designBatchIn) \Def -\Ent[\llikEmRdmDens[\Ndesign](\designBatchIn)]. \label{acq_max_exp_var_ent}
\end{align} 
The ``exp'' in the naming convention highlights that these criteria are being applied to
$\llikEmRdmDens[\Ndesign](\Par) = \priorDens(\Par) \Exp{\llikEm(\Par)}$, which is an exponentiated GP. 
Thus, many of the acquisition functions 
considered here can be viewed as the analogs of existing GP acquisition functions instead applied to log-normal 
processes (LNP). Since the 
variance and entropy of a log-normal random variable are well-known, the acquisitions \ref{acq_max_exp_var_ent}
are easily computable in closed-form. The $\maxexpvarLabel$ acquisition is considered in 
\cite{KandasamyActiveLearning2015, Kandasamy_2017}, 
while $\maxexpentLabel$ is utilized in \cite{wang2018adaptive, landslideCalibration}. 

\subsubsection{Integrated Expected Variance}
We now focus on adapting the integrated variance criterion \ref{acq_int_var} to LNPs. 
In the log-likelihood emulation 
setting, we will be able to derive a closed-form integrand, as was the case in \ref{acq_int_var}. In the special case 
of a Gaussian likelihood, the analogous criterion in the forward model emulation setting also admits a closed-form 
integrand. 

\paragraph{Log-Likelihood Emulation}
We begin with the log-likelihood emulation case. Recall that the 
$\intvarLabel$ criterion relied on the GP property that the predictive variance $\emKer[\Naugment]{\llik}(\Par)$
does not depend on the response $\llik(\designBatchIn)$. This is no longer the case for 
$\Var[\llikEmRdmDens[\Naugment](\Par)]$, given that $\llikEmRdmDens[\Naugment](\Par)$ is a log-normal 
random variable and hence its variance depends on $\emMean[\Naugment]{\llik}(\Par)$,  
which in turn depends on $\llik(\designBatchIn)$ (see \ref{kriging_eqns}). 
We follow the approach of \cite{VehtariParallelGP} and assume that the unobserved response follows the 
current GP predictive distribution; that is, we consider conditioning on a new design batch 
$\{\designBatchIn, \designBatchLlik\}$ which is assumed 
to satisfy 
\begin{align}
\designBatchLlik | \designBatchIn &\sim \Gaussian(\emMean[\Ndesign]{\llik}(\designBatchIn), \emKer[\Ndesign]{\llik}(\designBatchIn)).
\end{align}
Following the notation in \Cref{conditional-mean-notation}, we will denote 
\begin{align}
\emMean[\Ndesign]{\llik}(\Par | \designBatchLlik)
&\Def \E\left[\llikEmRdmDens[\Ndesign](\Par) | \llikEm[\Ndesign](\designBatchIn) = \designBatchLlik \right]. 
\end{align}
The dependence of $\emMean[\Ndesign]{\llik}(\Par | \designBatchLlik)$ on $\designBatchLlik$ is given in 
\Cref{lemma:pred-mean-dist}. This lemma allows us to average over the uncertainty in  
$\Var[\llikEmRdmDens[\Ndesign](\Par) | \llikEm[\Ndesign](\designBatchIn) = \designBatchLlik]$ stemming from the
 random quantity $\designBatchLlik$. 
 
\begin{lemma} \label{lemma:evar}
Let $\{\designBatchIn, \designBatchLlik\}$ be a new batch of $\Nbatch$ design points, with 
$\designBatchLlik | \designBatchIn \sim \Gaussian(\emMean[\Ndesign]{\llik}(\designBatchIn), \emKer[\Ndesign]{\llik}(\designBatchIn))$. 
For any $\Par \in \parSpace$, it then follows that 
\begin{align}
\E_{\designBatchLlik} \Var[\llikEmRdmDens[\Ndesign](\Par) | \llikEm[\Ndesign](\designBatchIn) = \designBatchLlik]
&= \Var\left[\llikEmRdmDens[\Ndesign](\Par) | \llikEm[\Ndesign](\designBatchIn) = \emMean[\Ndesign]{\llik}(\designBatchIn)\right]
	\varInflation(\Par; \designBatchIn), \label{evar-expression}
\end{align}
where 
\begin{align}
\Var\left[\llikEmRdmDens[\Ndesign](\Par) | \llikEm[\Ndesign](\designBatchIn) = \emMean[\Ndesign]{\llik}(\designBatchIn)\right]
&= \priorDens(\Par)^2 \Exp{2\emMean[\Ndesign]{\llik}\left(\Par| \emMean[\Ndesign]{\llik}(\designBatchIn)\right) + \emKer[\Naugment]{\llik}(\Par)} 
\left[\Exp{\emKer[\Naugment]{\llik}(\Par)} - 1 \right], \nonumber \\
\varInflation(\Par; \designBatchIn)
&= \Exp{2\left(\emKer[\Ndesign]{\llik}(\Par) - \emKer[\Naugment]{\llik}(\Par)\right)}. \label{var_inflation_factor}
\end{align}
\end{lemma}

A similar result is derived in \cite{VehtariParallelGP}.
The interpretation of \Cref{lemma:evar} is similar to that of \Cref{lemma:pred-mean-dist}. 
The first term in \ref{evar-expression} is the variance of the log-normal random variable
 $\llikEmRdmDens[\Ndesign](\Par)$ conditioned on the ``pseudo-data''
 $\{\designBatchIn, \emMean[\Ndesign]{\llik}(\designBatchIn)\}$. The second term 
 $\varInflation(\Par; \designBatchIn)$, which we refer to as the \textit{variance inflation factor}, 
 accounts for the uncertainty due to $\designBatchLlik$. We note that 
$\varInflation(\Par; \designBatchIn) \geq 1$ and the variance inflation is larger when the 
input batch $\designBatchIn$ is more influential. If $\designBatchIn$ is a subset of the 
current design $\designIn[\Ndesign]$, then $\varInflation(\Par; \designBatchIn) = 1$ (no 
variance inflation is applied). 

With these preliminary results established, we define the $\intExpVarLabel$ criterion
\begin{align}
\labelAcq[\Ndesign]{\intExpVarLabel}(\designBatchIn) \Def
\int_{\parSpace} \E_{\designBatchLlik} \Var[\llikEmRdmDens[\Ndesign](\Par) | \llikEm[\Ndesign](\designBatchIn) = \designBatchLlik] \weightDens(\Par) d\Par, \label{def:int_exp_var_llik}
\end{align}
emphasizing the the expectation is still with respect to 
$\designBatchLlik \sim \Gaussian(\emMean[\Ndesign]{\llik}(\designBatchIn), \emKer[\Ndesign]{\llik}(\designBatchIn))$. 
Applying \Cref{lemma:evar} immediately gives the closed-form expression for the integrand in \ref{def:int_exp_var_llik}. 
This criterion is considered in \cite{VehtariParallelGP} in training a log-likelihood emulator to be used for 
approximate Bayesian computation.

\paragraph{Forward Model Emulation}
To define a similar criterion in the forward model emulation setting, we first recall that we are considering ``plug-in'' forward model emulators. 
Thus, the unnormalized posterior density emulator is conditioned on a new batch $\{\designBatchIn, \designBatchFwd\}$ and then 
the updated emulator is plugged into the density in place of the true forward model. This implies that the analogous expression to 
the expected variance in \ref{def:int_exp_var_llik}, 
\begin{align*}
\E_{\designBatchFwd} \Var[\fwdEmRdm[\Ndesign]{\postDens}(\Par) | \fwdEm[\Ndesign](\designBatchIn) = \designBatchFwd],
\end{align*}
does \textit{not} correctly convey how the emulator is updated in this setting. To correctly specify the expected variance in this 
context, we define $\fwdEmCond{\designBatchFwd} \Def \fwdEm | [\fwdEm(\designBatchIn) = \designBatchFwd]$
\footnote{Note that $\fwdEmCond{\designBatchFwd}$ is conditioned on $\{\designBatchIn, \designBatchFwd\}$, though the 
dependence on the inputs $\designBatchIn$ is suppressed in the notation.}, so that 
$\postDens(\Par; \fwdEmCond{\designBatchFwd})$ represents the unnormalized posterior density 
with the updated emulator plugged in. Following the convention from previous sections, we let 
$\emMean[\Ndesign]{\fwd}(\Par | \designBatchFwd)$ denote the expectation of 
$\fwdEm(\Par) | [\fwdEm(\designBatchIn) = \designBatchFwd]$.
We can now define the integrated variance criterion as 
\begin{align}
\labelAcq[\Ndesign]{\fwdintExpVarLabel}(\designBatchIn) \Def
\int_{\parSpace} \E_{\designBatchFwd} \Var[\postDens(\Par; \fwdEmCond{\designBatchFwd}) | \designBatchFwd]\weightDens(\Par) d\Par, \label{fwd-int-exp-var}
\end{align}
under the assumption $\designBatchFwd|\designBatchIn \sim \Gaussian(\emMean{\fwd}(\designBatchIn), \emKer{\fwd}(\designBatchIn))$.
To our knowledge, 
the paper \cite{SinsbeckNowak} is the first to consider a design criterion of the form \ref{fwd-int-exp-var}. 
While in general the tractability of this criterion depends on the specific likelihood, \cite{Surer2023sequential} 
demonstrates that the integrand admits a closed-form expression when the likelihood is Gaussian. 
To derive this result, we begin by stating a lemma that characterizes the uncertainty in the Gaussian 
likelihood as a function of the random quantity $\designBatchFwd$, assumed to be distributed according 
to the current GP predictive distribution. 

\begin{lemma} \label{lemma:fwd-Gaussian-density-dist}
Let $\llik(\Par; \fwdEm) = \Gaussian(\obs | \fwdEm(\Par), \likPar)$, with 
$\fwdEm(\Par) \sim \GP(\emMean[\Ndesign]{\fwd}, \emKer[\Ndesign]{\fwd})$ interpreted as a set of $\dimObs$ independent 
GPs, one for each model output. Let $\{\designBatchIn, \designBatchFwd\}$ be a new batch of $\Nbatch$ 
design points, with
$\designBatchFwd | \designBatchIn \sim \Gaussian(\emMean[\Ndesign]{\fwd}(\designBatchIn), \emKer[\Ndesign]{\fwd}(\designBatchIn))$. 
Define $\CovComb[\Ndesign](\Par) \Def \likPar + \emKer[\Ndesign]{\fwd}(\Par)$. 
Then for $\Par \in \parSpace$,
\begin{enumerate}
\item $\E \left[\Exp{\llik(\Par; \fwdEmCond{\designBatchFwd})} | \designBatchFwd \right] 
= \Gaussian(\obs | \emMean[\Ndesign]{\fwd}(\Par | \designBatchFwd), \CovComb[\Naugment](\Par))$
\item $\E_{\designBatchFwd} \left[\Gaussian(\obs | \emMean[\Ndesign]{\fwd}(\Par | \designBatchFwd), \CovComb[\Naugment](\Par))\right]
= \Gaussian\left(\obs | \emMean[\Ndesign]{\fwd}(\Par), \CovComb[\Ndesign](\Par)\right)$
\item 
\begin{align*}
\E_{\designBatchFwd} \left[\Gaussian(\obs | \emMean[\Ndesign]{\fwd}(\Par| \designBatchFwd), \CovComb[\Naugment](\Par))^2\right] &= 
(2\pi)^{-\dimObs/2} \det(2\pi \CovComb[\Naugment](\Par))^{-1/2} \cdot \\
&\Gaussian\left(\obs | \emMean[\Ndesign]{\fwd}(\Par), \CovComb[\Ndesign](\Par) - \frac{1}{2} \CovComb[\Naugment](\Par) \right). 
\end{align*}
\end{enumerate}
\end{lemma}

\begin{prop} \label{prop:evar-fwd-emulation}
Consider 
$\fwdEm(\Par) \sim \GP(\emMean[\Ndesign]{\fwd}, \emKer[\Ndesign]{\fwd})$, interpreted as a set of $\dimObs$ independent 
GPs, one for each model output. Let $\{\designBatchIn, \designBatchFwd\}$ be a new batch of $\Nbatch$ 
design points, with
$\designBatchFwd | \designBatchIn \sim \Gaussian(\emMean[\Ndesign]{\fwd}(\designBatchIn), \emKer[\Ndesign]{\fwd}(\designBatchIn))$.
Then for $\Par \in \parSpace$, 
\begin{align*}
\E_{\designBatchFwd} &\Var[\postDens(\Par; \fwdEmCond{\designBatchFwd}) | \designBatchFwd] \\
&= \frac{\Gaussian\left(\obs | \emMean{\fwd}(\Par), \frac{1}{2}\likPar + \emKer{\fwd}(\Par) \right)}{2^{\dimObs/2} \det(2\pi \likPar)^{1/2}} - 
\frac{\Gaussian\left(\obs | \emMean{\fwd}(\Par), \left[\frac{1}{2}\likPar + \emKer{\fwd}(\Par)\right] - \frac{1}{2}\emKer[\Naugment]{\fwd}(\Par) \right)}{2^{\dimObs/2} \det(2\pi [\likPar + \emKer[\Naugment]{\fwd}(\Par)])^{1/2}} \\
&= \frac{\Gaussian\left(\obs | \emMean{\fwd}(\Par), \CovComb(\Par) - \frac{1}{2}\likPar \right)}{2^{\dimObs/2} \det(2\pi \likPar)^{1/2}} - 
\frac{\Gaussian\left(\obs | \emMean{\fwd}(\Par), \CovComb(\Par) - \frac{1}{2}\CovComb[\Naugment](\Par) \right)}{2^{\dimObs/2} \det(2\pi \CovComb[\Naugment](\Par))^{1/2}}.
\end{align*}
\end{prop}
 
% Learning Likelihood Parameters
\section{Learning Likelihood Parameters} \label{section_lik_par}

% Appendix 
\section{Appendix}

\subsection{Marginal Approximation with Gaussian Likelihood: Forward Model Emulation}
The closed-form computations related to the marginal approximation with a Gaussian 
likelihood follow from standard results regarding the convolution of Gaussian densities.  

\begin{prop} \label{Gaussian_convolution}
Let $\Gaussian(A \mu, \likPar)$ and $\Gaussian(m, C)$ be Gaussian distributions on $\R^{\dimObs}$ and $\R^{\dimPar}$, 
respectively, with $A \in \R^{\dimObs \times \dimPar}$ and $\likPar, C$ symmetric, positive definite matrices. Then 
\begin{align*}
\int_{\R^{\dimPar}} \Gaussian(\obs | A \mu, \likPar) \Gaussian(\mu | m, C) d\mu
&= \Gaussian(\obs | Am, \likPar + ACA^\top). 
\end{align*}
\end{prop}

\begin{proof} 
\todo
\end{proof}

We next prove a lemma that will be used in the proofs of \Cref{prop:Gaussian_marginal_moments} 
and \Cref{lemma:fwd-Gaussian-density-dist} below. 
\begin{lemma} \label{lemma:squared_Gaussian_density}
Let $\Gaussian(m, C)$ be a Gaussian distribution on $\R^{\dimObs}$ with $C$ a symmetric, positive-definite 
matrix. Then, for $\obs \in \R^{\dimObs}$, 
\begin{align*}
\Gaussian(\obs | m, C)^2 
&= 2^{-\dimObs/2} \det(2\pi C)^{-1/2} \Gaussian\left(\obs | m, \frac{1}{2}C\right). 
\end{align*}
\end{lemma}

\begin{proof}
\begin{align*}
\Gaussian(\obs | m, C)^2 
&= \det(2\pi C)^{-1} \Exp{-\frac{1}{2} (\obs - m)^\top \left[\frac{1}{2}C \right]^{-1}(\obs - m)} \\
&= \det(2\pi C)^{-1} \det(2\pi (1/2)C)^{1/2} \Gaussian\left(\obs | m, \frac{1}{2}C\right) \\
&= 2^{-\dimObs/2} \det(2\pi C)^{-1/2} \Gaussian\left(\obs | m, \frac{1}{2}C\right). 
\end{align*}
\end{proof}

\begin{prop} \label{prop:Gaussian_marginal_moments}
Assume $\obs | \mu \sim \Gaussian(A \mu, \likPar)$ and $\mu \sim \Gaussian(m, C)$, where $\mu \in \R^{\dimPar}$, 
$A \in \R^{\dimObs \times \dimPar}$, and $\likPar$, $C$ are both symmetric, positive definite. Then 
\begin{align}
\E\left[\Gaussian(\obs | A \mu, \likPar) \right] &= \Gaussian(\obs | Am, \likPar + ACA^\top) \\
\Var\left[\Gaussian(\obs | A \mu, \likPar) \right] 
&= \frac{\Gaussian\left(\obs | Am, \frac{1}{2} \likPar + ACA^\top \right)}{2^{\dimObs/2} \det(2\pi \likPar)^{1/2}} - 
\frac{\Gaussian\left(\obs | Am, \frac{1}{2}[\likPar + ACA^\top] \right)}{2^{\dimObs/2} \det(2\pi[\likPar + ACA^\top])^{1/2}}
\end{align}
\end{prop}

\begin{proof} 
The first result follows immediately from \Cref{Gaussian_convolution}. For the variance, we have 
\begin{align}
\Var\left[\Gaussian(\obs | A \mu, \likPar) \right] 
&= \E\left[\Gaussian(\obs | A \mu, \likPar)^2 \right] - \E\left[\Gaussian(\obs | A \mu, \likPar) \right]^2 \nonumber \\
&= \E\left[\Gaussian(\obs | A \mu, \likPar)^2 \right] - \Gaussian(\obs | Am, \likPar + ACA^\top)^2. \label{two_terms_variance}
\end{align}
Starting with the first term, we apply \Cref{lemma:squared_Gaussian_density} and 
\Cref{Gaussian_convolution}, respectively, to obtain 
\begin{align*}
\E\left[\Gaussian(\obs | A \mu, \likPar)^2 \right]
&= 2^{-\dimObs/2} \det(2\pi \likPar)^{-1/2} \E\left[\Gaussian\left(\obs | A\mu, \frac{1}{2}\likPar \right)\right] \\
&= 2^{-\dimObs/2} \det(2\pi \likPar)^{-1/2} \Gaussian\left(\obs | Am, \frac{1}{2}\likPar + ACA^\top \right).
\end{align*}
For the second term in \ref{two_terms_variance}, another application of \Cref{lemma:squared_Gaussian_density} gives
\begin{align*}
\Gaussian(\obs | Am, \likPar + ACA^\top)^2
&= 2^{-\dimObs/2} \det(2\pi[\likPar + ACA^\top])^{-1/2} \Gaussian\left(\obs | Am, \frac{1}{2}[\likPar + ACA^\top]\right).
\end{align*}
Plugging these expressions back into \ref{two_terms_variance} completes the proof. 
\end{proof}


\subsection{Marginal Acceptance Probability}
In this section we derive an expression for 
\begin{align}
\llikEmJointMarg[\Ndesign]{\accProbMH}(\Par, \propPar) 
&= \E_{\llikEm[\Ndesign]} \left[\min\left\{1, \Em[\Ndesign]{\accProbRatio}(\Par, \propPar)\right\} \right], 
\end{align}
as considered in \ref{acc_prob_joint_marg}. We start by noting 
\begin{align}
\Em[\Ndesign]{\accProbRatio}(\Par, \propPar)
&\sim \LN\left(\log C + \emMean[\Ndesign]{\llik}(\propPar) - \emMean[\Ndesign]{\llik}(\Par), \Var\left[\llikEm[\Ndesign](\propPar) - \llikEm[\Ndesign](\Par) \right] \right),
\label{acc_ratio_LN}
\end{align}
where 
\begin{align*}
C \Def \frac{\priorDens(\propPar) \propDens(\propPar, \Par)}{\priorDens(\Par) \propDens(\Par, \propPar)}.
\end{align*}
Thus, the required computation reduces to computing the expectation of $\min\left\{1, Y \right\}$, where $Y$ is a log-normally distributed 
random variable. 

\begin{lemma} \label{lemma:exp_max_one_LN}
Let $Y \sim \LN(m, s^2)$. Then, 
\begin{align}
\E\left[\min\left\{1, Y \right\} \right] &= \GaussianCDF\left(\frac{m}{s} \right) + \GaussianCDF\left(-\frac{m + s^2}{s} \right) e^{m + \frac{1}{2}s^2}
\end{align}
\end{lemma}

\begin{proof}
We have 
\begin{align}
\E\left[\min\left\{1, Y \right\} \right]
&= \int_{0}^{\infty} \min\{1, y\} \LN(y | m, s^2) dy \nonumber \\
&= \int_{-\infty}^{\infty} \min\{1, e^x\} \Gaussian(x | m, s^2) dx \nonumber \\
&= \int_{0}^{\infty} \Gaussian(x | m, s^2) dx +  \int_{-\infty}^{0} e^x \Gaussian(x | m, s^2) dx \nonumber \\
&= \GaussianCDF(m/s) + \int_{-\infty}^{0} e^x \Gaussian(x | m, s^2) dx. \nonumber \\
&= \GaussianCDF(m/s) + \int_{-\infty}^{0} \frac{1}{\sqrt{2\pi s^2}} e^x \Exp{-\frac{1}{2s^2} (x - m)^2} dx.  
\label{second_integral} 
\end{align}
For the integral in \ref{second_integral} we combine the exponential terms and complete the square. 
This yields 
\begin{align*}
&\int_{-\infty}^{0} \frac{1}{\sqrt{2\pi s^2}} e^x \Exp{-\frac{1}{2s^2} (x - m)^2} dx \\
&= \Exp{-\frac{m^2}{2s^2}} \int_{-\infty}^{0} \frac{1}{\sqrt{2\pi s^2}} \Exp{-\frac{1}{2} \left[\frac{x^2}{2} - 2\left(\frac{m}{s^2} + 1\right)x  \right]} dx \\
&= \Exp{-\frac{m^2}{2s^2} + \frac{[m + s^2]^2}{2s^2}} \int_{-\infty}^{0} \frac{1}{\sqrt{2\pi s^2}} \Exp{-\frac{1}{2s^2}(x - [m+s^2])^2} dx \\
&= \Exp{m + \frac{1}{2}s^2} \GaussianCDF\left(-\frac{m+s^2}{s}\right).
\end{align*}
Plugging this back into \ref{second_integral} completes the proof. 
\end{proof}

We now apply \Cref{lemma:exp_max_one_LN} to obtain the expression for the marginal acceptance probability. 
\begin{prop} \label{prop:joint-marg-accept-prob}
\begin{align*}
\llikEmJointMarg[\Ndesign]{\accProbMH}(\Par, \propPar) 
&= \E_{\llikEm[\Ndesign]} \left[\min\left\{1, \Em[\Ndesign]{\accProbRatio}(\Par, \propPar)\right\} \right] 
= w_1 + w_2 \llikEmJointMarg[\Ndesign]{\accProbRatio}(\Par, \propPar), 
\end{align*}
where 
\begin{align*}
w_1 &= \Prob(\Em[\Ndesign]{\accProbRatio}(\Par, \propPar) \geq 1) \\
w_2 &= \Prob\left(\Em[\Ndesign]{\accProbRatio}(\Par, \propPar) \leq \Exp{-\Var\left[\llikEm[\Ndesign](\propPar) - \llikEm[\Ndesign](\Par) \right]}\right).
\end{align*}
\end{prop}

\begin{proof}
We recall from \ref{acc_ratio_LN} that $\Em[\Ndesign]{\accProbRatio}(\Par, \propPar) \sim \LN(m, s^2)$, with 
\begin{align*}
m &= \log C + \emMean[\Ndesign]{\llik}(\propPar) - \emMean[\Ndesign]{\llik}(\Par) \\
s^2 &= \Var\left[\llikEm[\Ndesign](\propPar) - \llikEm[\Ndesign](\Par) \right]. 
\end{align*}
We thus have 
\begin{align*}
e^{m + \frac{1}{2}s^2} = \E_{\llikEm}\left[\Em[\Ndesign]{\accProbRatio}(\Par, \propPar) \right] 
=  \llikEmJointMarg[\Ndesign]{\accProbRatio}(\Par, \propPar). 
\end{align*}
It remains to verify the expressions for the weights $w_1$ and $w_2$. Letting $Z \sim \Gaussian(0,1)$, 
we apply \Cref{lemma:exp_max_one_LN} to obtain
\begin{align*}
w_1 &= \GaussianCDF(m/s) = \Prob(Z \leq m/s) = \Prob(\Exp{m+sZ} \geq 1) = \Prob(\Em[\Ndesign]{\accProbRatio}(\Par, \propPar) \geq 1) \\
w_2 &= \GaussianCDF(-(m+s^2)/s) = \Prob(\Exp{m + sZ} \leq e^{-s}) = \Prob(\Em[\Ndesign]{\accProbRatio}(\Par, \propPar) \leq e^{-s}).
\end{align*}
\end{proof}

\subsection{Transition Kernels of Approximate MCMC Algorithms} \label{transition_kernel_derivations}
We derive the expression for the \textit{MCWMH-joint} transition 
kernel $\llikEmJointMCWMH[\Ndesign]{\MarkovKernel}$ given in \ref{MCWMH-joint-kernel}. 
Let $\Par \in \parSpace$ and $A \subset \parSpace$ a Borel set. 
We recall that the kernel for a standard MH algorithm is given by 
\begin{align}
\MarkovKernel(\Par, A)
&= \int_{A} \propDens(\Par, \propPar) \accProbMH(\Par, \propPar) d\propPar
+ [1 - \avgAccProbMH(\Par)] \delta_{\Par}(A) \label{MH-kernel}, 
\end{align}
where $\avgAccProbMH(\Par)$ is the overall acceptance probability, averaged over all proposals,
\begin{align}
\avgAccProbMH(\Par)
&= \int_{\parSpace} \propDens(\Par, \propPar) \accProbMH(\Par, \propPar) d\propPar. 
\end{align}
Denote $\parMat \Def \{\Par, \propPar\}$. 
The \textit{MCWMH-joint} algorithm replaces the exact log-likelihood evaluations 
$\llik(\parMat) \Def [\llik(\Par), \llik(\propPar)]^\top$ used to define $\propDens(\Par, \propPar)$ by 
sampled approximate values 
\begin{align}
\llikSamp_{\parMat} \Def [\llikSamp, \llikSampProp]^\top \sim \Gaussian(\emMean[\Ndesign]{\llik}(\parMat), \emKer[\Ndesign]{\llik}(\parMat)). 
\end{align}
We let $\Em[\Ndesign]{\accProbMH}(\Par, \propPar | \llikSamp_{\parMat})$ denote the approximate acceptance probability defined using 
the sampled values $\llikSamp_{\parMat}$. The probability of accepting a state in the set $A$, conditional on the sample $\llikSamp_{\parMat}$, 
is thus 
\begin{align}
\Prob(\indexMCMC[\mcmcIndex+1]{\Par} \in A | \Par, \llikSamp_{\parMat})
&=  \int_{A} \propDens(\Par, \propPar) \Em[\Ndesign]{\accProbMH}(\Par, \propPar | \llikSamp_{\parMat}) d\propPar. 
\end{align}
The unconditional probability follows from the law of total probability:
\begin{align}
\Prob(\indexMCMC[\mcmcIndex+1]{\Par} \in A | \Par)
&= \int_{\R^2} \Prob(\indexMCMC[\mcmcIndex+1]{\Par} \in A | \Par, \llikSamp_{\parMat}) \Gaussian(\llikSamp_{\parMat} | \emMean[\Ndesign]{\llik}(\parMat), \emKer[\Ndesign]{\llik}(\parMat)) d\llikSamp_{\parMat} \\
&= \int_{\R^2} \int_{A} \propDens(\Par, \propPar) \Em[\Ndesign]{\accProbMH}(\Par, \propPar | \llikSamp_{\parMat})  
\Gaussian(\llikSamp_{\parMat} | \emMean[\Ndesign]{\llik}(\parMat), \emKer[\Ndesign]{\llik}(\parMat)) d\propPar \ d\llikSamp_{\parMat} \\
&= \int_{A} \propDens(\Par, \propPar) \int_{\R^2}  \Em[\Ndesign]{\accProbMH}(\Par, \propPar | \llikSamp_{\parMat})  
\Gaussian(\llikSamp_{\parMat} | \emMean[\Ndesign]{\llik}(\parMat), \emKer[\Ndesign]{\llik}(\parMat)) d\llikSamp_{\parMat} \ d\propPar \label{flip_integral_order} \\
&= \int_{A} \propDens(\Par, \propPar) \llikEmJointMarg[\Ndesign]{\accProbMH}(\Par, \propPar)  d\propPar
\end{align}
where \ref{flip_integral_order} follows from Tonelli's theorem, and using the definition of $\llikEmJointMarg[\Ndesign]{\accProbMH}(\Par, \propPar)$
in \ref{acc_prob_joint_marg}. Setting $A \Def \parSpace$ in the above integral yields the overall acceptance probability 
\begin{align}
\llikEmJointMarg[\Ndesign]{\avgAccProbMH}(\Par) \Def \int_{\parSpace} \propDens(\Par, \propPar) \llikEmJointMarg[\Ndesign]{\accProbMH}(\Par, \propPar)  d\propPar. 
\end{align}
These quantities thus give 
\begin{align}
\llikEmJointMCWMH[\Ndesign]{\MarkovKernel}(\Par, A)
&= \int_{A} \propDens(\Par, \propPar) \llikEmJointMarg[\Ndesign]{\accProbMH}(\Par, \propPar) d\propPar
+ [1 - \llikEmJointMarg[\Ndesign]{\avgAccProbMH}(\Par)] \delta_{\Par}(A) \label{MH-kernel}, 
\end{align}
which follows from same derivations for the standard MH kernel \ref{MH-kernel} with the marginal acceptance probabilities substituted for
the original ones. The transition kernel for the \textit{MCWMH-ind} algorithm follows immediately by marginalizing with respect 
to $\Gaussian(\emMean[\Ndesign]{\llik}\left(\parMat), \text{diag}\left\{\emKer[\Ndesign]{\llik}(\Par), \emKer[\Ndesign]{\llik}(\propPar) \right\}\right)$
in place of the full joint distribution. 

\subsection{Sequential Design Calculations}
We start by stating an identity for the inversion of partitioned matrices, which is very useful in updating GPs by 
conditioning on new design points. The generic result can be found in the lecture notes \cite{MinkaMatrixLectures}, 
but we specialize the statement to the GP setting.  

\subsubsection{Useful Lemmas}

\begin{prop} \label{partitioned-matrix-inverse}
Let $\funcPrior \sim \GP(\gpMeanPrior, \gpKerPrior)$ be a GP prior, with 
$\funcEm[\Ndesign] \Def \funcPrior | [\funcPrior(\designIn) = \func(\designIn)] \sim \GP(\gpMean[\Ndesign], \gpKer[\Ndesign])$  
the GP predictive distribution after conditioning on the design $(\designIn, \func(\designIn))$. Let 
$\designBatchIn$ be a set of $\Nbatch$ new design points. Define 
$\kerMat[\Ndesign] \Def \gpKerPrior(\designIn)$, $\kerMat[\Nbatch] \Def \gpKerPrior(\designBatchIn)$,
and $\kerMat[\Ndesign,\Nbatch] \Def \gpKerPrior(\designIn[\Ndesign], \designBatchIn)$.  
Then, letting 
$\designIn[\Naugment] \Def \designIn \cup \designBatchIn$, the inverse of the kernel matrix 
evaluated on the augmented design satisfies 
\begin{align}
\gpKerPrior(\designIn[\Naugment])^{-1}
&= \begin{pmatrix} \kerMat[\Ndesign] & \kerMat[\Ndesign,\Nbatch] \\
\kerMat[\Ndesign,\Nbatch]^\top & \kerMat[\Nbatch] \end{pmatrix}^{-1}
=  \begin{pmatrix} \tilde{K} & -\kerMat[\Ndesign]^{-1} \kerMat[\Ndesign,\Nbatch] \gpKer[\Ndesign](\designBatchIn)^{-1} \\
-\gpKer[\Ndesign](\designBatchIn)^{-1} \kerMat[\Ndesign,\Nbatch]^\top  \kerMat[\Ndesign]^{-1} & \gpKer[\Ndesign](\designBatchIn)^{-1} \end{pmatrix},
\end{align}
where 
\begin{align}
\tilde{K} = \kerMat[\Ndesign]^{-1} + \kerMat[\Ndesign]^{-1} \kerMat[\Ndesign,\Nbatch] \gpKer[\Ndesign](\designBatchIn)^{-1} \kerMat[\Nbatch,\Ndesign] \kerMat[\Ndesign]^{-1}.
\end{align}
Thus, assuming $\kerMat[\Ndesign]^{-1}$ has already been computed, $\gpKerPrior(\designIn[\Naugment])^{-1}$ can be constructed in an additional 
$\BigO(\Nbatch^3 + \Nbatch \Ndesign^2 + \Nbatch^2 \Ndesign)$ operations. 
\end{prop}

We repeatedly use the fact that $\funcPrior|[\funcPrior(\designIn[\Naugment]) = \func(\designIn[\Naugment])]$ and 
$\funcEm[\Ndesign] | [\funcEm[\Ndesign](\designBatchIn) = \func(\designBatchIn)]$ are equal in distribution ; 
i.e., conditioning the GP prior on the entire design 
is equivalent to sequentially conditioning on subsets of the design. For the sake of completeness, we provide the rigorous justification for this below.

\begin{lemma} \label{lemma:gp-condition-order}
Let $\funcPrior \sim \GP(\gpMeanPrior, \gpKerPrior)$ be a GP prior. Consider a set of $\Naugment$ design points $\{\designIn[\Naugment], \funcVal[\Naugment]\}$
partitioned as $\designIn[\Naugment] = \designIn[\Ndesign] \cup \designBatchIn$ and $\funcVal[\Naugment] = \funcVal[\Ndesign] \cup \funcVal[\Nbatch]$.
Let $\funcEm[\Ndesign] \Def \func | [\func(\designIn[\Ndesign]) = \funcVal[\Ndesign]] \sim \GP(\gpMean[\Ndesign], \gpKer[\Ndesign])$. 
Then the random process $\funcPrior | [\funcPrior(\designIn[\Naugment]) = \funcVal[\Naugment]]$ is equal in distribution to the random process
$\funcEm[\Ndesign] | [\funcEm[\Ndesign](\designIn[\Nbatch]) = \funcVal[\Nbatch]]$. 
\end{lemma} 

\begin{proof} 
Since both processes in question are Gaussian it suffices to check that 
\begin{align*}
\E[\funcPrior(\parMat) | \funcPrior(\designIn[\Naugment]) = \funcVal[\Naugment]] 
&= \E[\funcEm[\Ndesign](\parMat) | \funcEm[\Ndesign](\designIn[\Nbatch]) = \funcVal[\Nbatch]] \\
\Cov[\funcPrior(\parMat) | \funcPrior(\designIn[\Naugment]) = \funcVal[\Naugment]] 
&= \Cov[\funcEm[\Ndesign](\parMat) | \funcEm[\Ndesign](\designIn[\Nbatch]) = \funcVal[\Nbatch]],
\end{align*}
for an arbitrary finite set of inputs $\parMat \subset \parSpace$. The quantities on the lefthand side are 
$\gpMean[\Naugment](\parMat)$ and $\gpKer[\Naugment](\parMat)$, by definition. We begin by expanding 
the expression $\gpKerPrior(\parMat, \designIn[\Naugment]) \kerMat[\Naugment]^{-1}$, noting that we 
are borrowing the notation from \Cref{partitioned-matrix-inverse}. Applying the partitioned matrix 
inversion identity from \Cref{partitioned-matrix-inverse} yields 
\begin{align*}
\gpKerPrior(\parMat, \designIn[\Naugment]) \kerMat[\Naugment]^{-1}
&= \begin{pmatrix} \gpKerPrior(\parMat, \designIn[\Ndesign]) &  \gpKerPrior(\parMat, \designIn[\Nbatch]) \end{pmatrix}
\begin{pmatrix} \tilde{K} & -\kerMat[\Ndesign]^{-1} \kerMat[\Ndesign,\Nbatch] \gpKer[\Ndesign](\designBatchIn)^{-1} \\
-\gpKer[\Ndesign](\designBatchIn)^{-1} \kerMat[\Ndesign,\Nbatch]^\top  \kerMat[\Ndesign]^{-1} & \gpKer[\Ndesign](\designBatchIn)^{-1} \end{pmatrix}.
\end{align*}
Denoting $\funcVal[\Ndesign]^\prime \Def \funcVal[\Ndesign] - \gpMeanPrior(\designIn[\Ndesign])$ and 
$\funcVal[\Nbatch]^\prime \Def \funcVal[\Nbatch] - \gpMeanPrior(\designIn[\Nbatch])$, the predictive mean $\gpMean[\Naugment](\parMat)$ is thus given by 
\begin{align*}
\gpMean[\Naugment](\parMat)
&= \gpMeanPrior(\parMat) + \gpKerPrior(\parMat, \designIn[\Naugment]) \kerMat[\Naugment]^{-1}[\funcVal[\Naugment] - \gpMeanPrior(\designIn[\Naugment])] \\
&= \gpMeanPrior(\parMat) + \begin{pmatrix} \gpKerPrior(\designIn[\Ndesign], \parMat) \\  \gpKerPrior(\designIn[\Nbatch], \parMat) \end{pmatrix}^\top
\begin{pmatrix} \tilde{K} & -\kerMat[\Ndesign]^{-1} \kerMat[\Ndesign,\Nbatch] \gpKer[\Ndesign](\designBatchIn)^{-1} \\
-\gpKer[\Ndesign](\designBatchIn)^{-1} \kerMat[\Ndesign,\Nbatch]^\top  \kerMat[\Ndesign]^{-1} & \gpKer[\Ndesign](\designBatchIn)^{-1} \end{pmatrix} 
\begin{pmatrix}  \funcVal[\Ndesign]^\prime \\  \funcVal[\Nbatch]^\prime \end{pmatrix} \\
&= \gpMeanPrior(\parMat) + \gpKerPrior(\parMat, \designIn[\Ndesign])\kerMat[\Ndesign]^{-1}\funcVal[\Ndesign]^\prime + 
\gpKer[\Ndesign](\parMat, \designIn[\Nbatch]) \gpKer[\Ndesign](\designIn[\Nbatch])^{-1} \funcVal[\Nbatch]^\prime - 
\gpKer[\Ndesign](\parMat, \designIn[\Nbatch]) \gpKer[\Ndesign](\designIn[\Nbatch])^{-1} \kerMat[\Ndesign,\Nbatch]^\top \kerMat[\Ndesign]^{-1} \funcVal[\Ndesign]^\prime \\
&= \gpMean[\Ndesign](\parMat) + \gpKer[\Ndesign](\parMat, \designIn[\Nbatch]) \gpKer[\Ndesign](\designIn[\Nbatch])^{-1}[\funcVal[\Nbatch]^\prime - \kerMat[\Ndesign,\Nbatch]^\top \kerMat[\Ndesign]^{-1} \funcVal[\Ndesign]^\prime] \\
&= \gpMean[\Ndesign](\parMat) + \gpKer[\Ndesign](\parMat, \designIn[\Nbatch]) \gpKer[\Ndesign](\designIn[\Nbatch])^{-1}[\funcVal[\Nbatch]^\prime - \gpMean[\Ndesign](\designIn[\Nbatch]) + \gpMeanPrior(\designIn[\Nbatch])] \\
&= \gpMean[\Ndesign](\parMat) + \gpKer[\Ndesign](\parMat, \designIn[\Nbatch]) \gpKer[\Ndesign](\designIn[\Nbatch])^{-1}[\funcVal[\Nbatch] - \gpMean[\Ndesign](\designIn[\Nbatch])] \\
&= \E[\funcEm[\Ndesign](\parMat) | \funcEm[\Ndesign](\designIn[\Nbatch]) = \funcVal[\Nbatch]] 
\end{align*}
where we have used the fact that the predictive covariance of the GP $\funcEm[\Ndesign]$ gives 
\[
\gpKer[\Ndesign](\parMat, \designIn[\Nbatch]) = \gpKerPrior(\parMat, \designIn[\Nbatch]) - 
\gpKerPrior(\parMat, \designIn[\Ndesign])\kerMat[\Ndesign]^{-1} \gpKerPrior(\designIn[\Ndesign], \parMat).
\]
The covariance calculation proceeds similarly by replacing $\funcVal[\Ndesign]^\prime$ and $\funcVal[\Nbatch]^\prime$ with 
$\gpKerPrior(\designIn[\Ndesign], \parMat)$ and $\gpKerPrior(\designIn[\Nbatch], \parMat)$, respectively. We obtain 
\begin{align*}
\gpKer[\Naugment](\parMat)
&= \gpKerPrior(\parMat) - \gpKerPrior(\parMat, \designIn[\Naugment]) \kerMat[\Naugment]^{-1}[\funcVal[\Naugment] - \gpMeanPrior(\designIn[\Naugment])] \\
&= \gpKerPrior(\parMat) - \begin{pmatrix} \gpKerPrior(\designIn[\Ndesign], \parMat) \\  \gpKerPrior(\designIn[\Nbatch], \parMat) \end{pmatrix}^\top
\begin{pmatrix} \tilde{K} & -\kerMat[\Ndesign]^{-1} \kerMat[\Ndesign,\Nbatch] \gpKer[\Ndesign](\designBatchIn)^{-1} \\
-\gpKer[\Ndesign](\designBatchIn)^{-1} \kerMat[\Ndesign,\Nbatch]^\top  \kerMat[\Ndesign]^{-1} & \gpKer[\Ndesign](\designBatchIn)^{-1} \end{pmatrix} 
\begin{pmatrix}  \gpKerPrior(\designIn[\Ndesign], \parMat) \\  \gpKerPrior(\designIn[\Nbatch], \parMat) \end{pmatrix} \\
&= \gpKer[\Ndesign](\parMat) - 
\gpKer[\Ndesign](\parMat, \designIn[\Nbatch]) \gpKer[\Ndesign](\designIn[\Nbatch])^{-1} [\gpKerPrior(\designIn[\Nbatch], \parMat) - \kerMat[\Ndesign,\Nbatch]^\top \kerMat[\Ndesign]^{-1}\gpKerPrior(\designIn[\Ndesign], \parMat)] \\
&= \gpKer[\Ndesign](\parMat) -  \gpKer[\Ndesign](\parMat, \designIn[\Nbatch]) \gpKer[\Ndesign](\designIn[\Nbatch])^{-1} \gpKer[\Ndesign](\designIn[\Nbatch], \parMat) \\
&= \Cov[\funcEm[\Ndesign](\parMat) | \funcEm[\Ndesign](\designIn[\Nbatch]) = \funcVal[\Nbatch]].
\end{align*}

\end{proof}


\subsubsection{Uncertainty in GP Predictive Mean Due to Unobserved Response}

\begin{proof} [Proof of \Cref{lemma:pred-mean-dist}]
Though the result is only required for a single input $\Par$, it is no more difficult to establish for a set of inputs $\parMat$. 
We recall that 
\begin{align*}
\gpMean[\Ndesign](\parMat | \designBatchFunc) 
&= \E[\funcEm[\Ndesign](\parMat) | \funcEm(\designBatchIn) = \designBatchFunc] \\
&= \gpMean[\Ndesign](\parMat) + \gpKer[\Ndesign](\parMat, \designBatchIn) \gpKer[\Ndesign](\designBatchIn)^{-1} [\designBatchFunc - \gpMean[\Ndesign](\designBatchIn)],
\end{align*}
following from the GP predictive equations \ref{kriging_eqns}. Since, $\designBatchFunc|\parMat \sim \Gaussian(\gpMean[\Ndesign](\designBatchIn), \gpKer[\Ndesign](\designBatchIn))$, 
we see that $\gpMean[\Ndesign](\parMat | \designBatchFunc)$ is a linear function of a Gaussian random variable. It is thus Gaussian distributed, with mean and covariance
\begin{align*}
\E_{\designBatchFunc}[\gpMean[\Ndesign](\parMat | \designBatchFunc)]
&= \gpMean[\Ndesign](\parMat) + \gpKer[\Ndesign](\parMat, \designBatchIn) \gpKer[\Ndesign](\designBatchIn)^{-1} [\gpMean[\Ndesign](\designBatchIn) - \gpMean[\Ndesign](\designBatchIn)] = \gpMean[\Ndesign](\parMat) \\
\Cov_{\designBatchFunc}[\gpMean[\Ndesign](\parMat | \designBatchFunc)]
&= \gpKer[\Ndesign](\parMat, \designBatchIn) \gpKer[\Ndesign](\designBatchIn)^{-1}  \gpKer[\Ndesign](\designBatchIn) \gpKer[\Ndesign](\designBatchIn)^{-1} \gpKer[\Ndesign](\designBatchIn, \parMat) \\
&=  \gpKer[\Ndesign](\parMat, \designBatchIn) \gpKer[\Ndesign](\designBatchIn)^{-1} \gpKer[\Ndesign](\designBatchIn, \parMat) \\
&= \gpKer[\Ndesign](\parMat) - \gpKer[\Naugment](\parMat).
\end{align*}
\end{proof}

\subsubsection{Integrated Conditional Variance Calculations: Log-Likelihood Emulation}

\begin{proof} [Proof of \Cref{lemma:evar}]
We start by noting that 
\begin{align*}
\Var[\llikEmRdmDens[\Ndesign](\Par) | \llikEm[\Ndesign](\designBatchIn) = \designBatchLlik]
&= \Var[\priorDens(\Par) \Exp{\llikEm(\Par)} | \llikEm[\Ndesign](\designBatchIn) = \designBatchLlik] \\
&= \priorDens(\Par)^2 \Var[\Exp{\llikEm(\Par)} | \llikEm[\Ndesign](\designBatchIn) = \designBatchLlik],
\end{align*}
so we will focus on the likelihood emulator, ignoring the prior for now. Since 
\begin{align*}
\Exp{\llikEm(\Par)} | [\llikEm[\Ndesign](\designBatchIn) = \designBatchLlik] \sim 
\LN(\emMean[\Ndesign]{\llik}(\Par| \designBatchLlik), \emKer[\Naugment]{\llik}(\Par)),
\end{align*}
we apply the formula for a log-normal variance to obtain 
\begin{align}
\Var[\Exp{\llikEm(\Par)} | \llikEm[\Ndesign](\designBatchIn) = \designBatchLlik]
&= \cst \Exp{2\emMean[\Ndesign]{\llik}(\Par | \designBatchLlik)}, \label{formula_plug_in}
\end{align}
where 
\begin{align*}
\cst \Def \left[\Exp{\emKer[\Naugment]{\llik}(\Par)} -1 \right] \Exp{\emKer[\Naugment]{\llik}(\Par)}
\end{align*}
is not a function of the random variable $\designBatchLlik$. \Cref{lemma:pred-mean-dist} gives 
\begin{align*}
\emMean[\Ndesign]{\llik}(\Par | \designBatchLlik)
&\sim \LN(\emMean[\Ndesign]{\llik}(\Par), \emKer[\Ndesign]{\llik}(\Par) - \emKer[\Naugment]{\llik}(\Par)), 
\end{align*}
which implies 
\begin{align*}
\Exp{2\emMean[\Ndesign]{\llik}(\Par | \designBatchLlik)}
&\sim \Gaussian(2\emMean[\Ndesign]{\llik}(\Par), 4[\emKer[\Ndesign]{\llik}(\Par) - \emKer[\Naugment]{\llik}(\Par)]).
\end{align*}
Applying the formula for a log-normal mean thus yields 
\begin{align*}
\E_{\designBatchLlik} \left[\Exp{2\emMean[\Ndesign]{\llik}(\Par | \designBatchLlik)} \right]
&= \Exp{2\emMean[\Ndesign]{\llik}(\Par)} \Exp{2[\emKer[\Ndesign]{\llik}(\Par) - \emKer[\Naugment]{\llik}(\Par)]} \\
&=  \Exp{2\emMean[\Ndesign]{\llik}(\Par)} \varInflation(\Par; \designBatchIn), 
\end{align*}
where $\varInflation(\Par; \designBatchIn)$ is defined in \ref{var_inflation_factor}. Plugging this expression back 
into \ref{formula_plug_in} gives 
\begin{align*}
\E_{\designBatchLlik} \Var[\Exp{\llikEm(\Par)} | \llikEm[\Ndesign](\designBatchIn) = \designBatchLlik]
&= \cst \Exp{2\emMean[\Ndesign]{\llik}(\Par)} \varInflation(\Par; \designBatchIn) \\
&= \Var[\Exp{\llikEm(\Par)} | \llikEm[\Ndesign](\designBatchIn) = \emMean[\Ndesign]{\llik}(\Par)] \varInflation(\Par; \designBatchIn).
\end{align*}
Multiplying both sides by $\priorDens(\Par)$ completes the proof, the closed-form expression for the first term following 
immediately from the formula for a log-normal variance. 
\end{proof}

\subsubsection{Integrated Conditional Variance Calculations: Forward Model Emulation}

\begin{proof} [Proof of \Cref{lemma:fwd-Gaussian-density-dist}]

\begin{enumerate}
\item The first result follows immediately from \Cref{post_dens_marg_fwd_Gaussian}. 
\item From \Cref{lemma:pred-mean-dist}, we know that 
\begin{align*}
\emMean[\Ndesign]{\fwd}(\Par|\designBatchFwd)
&\sim \Gaussian(\emMean[\Ndesign]{\fwd}(\Par), \emKer[\Ndesign]{\fwd}(\Par) - \emKer[\Naugment]{\fwd}(\Par)). 
\end{align*}
We can thus write the expectation of interest as 
\begin{align*}
\E_{\designBatchFwd}\left[\Gaussian(\obs | \emMean[\Ndesign]{\fwd}(\Par| \designBatchFwd), \CovComb[\Naugment](\Par) \right]
&= \int_{\R^{\dimObs}} \Gaussian(\obs| v, \CovComb[\Naugment](\Par)) 
\Gaussian(v | \emMean[\Ndesign]{\fwd}(\Par), \emKer[\Ndesign]{\fwd}(\Par) - \emKer[\Naugment]{\fwd}(\Par)) dv \\
&= \Gaussian\left(\obs | \emMean[\Ndesign]{\fwd}(\Par), \CovComb[\Naugment](\Par) + \emKer[\Ndesign]{\fwd}(\Par) - \emKer[\Naugment]{\fwd}(\Par)\right), 
\end{align*}
where the final line uses the Gaussian convolution formula in \Cref{Gaussian_convolution}. Recalling that 
$\CovComb[\Naugment](\Par) = \likPar + \emKer[\Naugment]{\fwd}(\Par)$, the covariance in the above expression simplifies to 
$\likPar + \emKer[\Ndesign]{\fwd}(\Par) = \CovComb[\Ndesign](\Par)$. 

\item We start by applying \Cref{lemma:squared_Gaussian_density}, which gives 
\begin{align*}
\Gaussian(\obs | \emMean[\Ndesign]{\fwd}(\Par| \designBatchFwd), \CovComb[\Naugment](\Par))^2
&= (2\pi)^{-\dimObs/2} \det(2\pi \CovComb[\Naugment](\Par))^{-1/2} 
\Gaussian\left(\obs | \emMean[\Ndesign]{\fwd}(\Par| \designBatchFwd), \frac{1}{2}\CovComb[\Naugment](\Par)\right). 
\end{align*}
Just as in part 2 above, the convolution formula in \Cref{Gaussian_convolution} yields 
\begin{align*}
\E_{\designBatchFwd}\left[\Gaussian\left(\obs | \emMean[\Ndesign]{\fwd}(\Par| \designBatchFwd), \frac{1}{2}\CovComb[\Naugment](\Par)\right) \right] 
&= \Gaussian\left(\obs | \emMean[\Ndesign]{\fwd}(\Par), \frac{1}{2}\CovComb[\Naugment](\Par) + \emKer[\Ndesign]{\fwd}(\Par) - \emKer[\Naugment]{\fwd}(\Par)\right), 
\end{align*}
with the covariance simplifying to 
\begin{align*}
\frac{1}{2}\CovComb[\Naugment](\Par) + \emKer[\Ndesign]{\fwd}(\Par) - \emKer[\Naugment]{\fwd}(\Par)
&= \frac{1}{2}\likPar + \frac{1}{2}\emKer[\Naugment]{\fwd}(\Par) + \emKer[\Ndesign]{\fwd}(\Par) - \emKer[\Naugment]{\fwd}(\Par) \\ 
&= \left[\likPar + \emKer[\Ndesign]{\fwd}(\Par) \right] - \frac{1}{2}\left[\likPar + \emKer[\Naugment]{\fwd}(\Par) \right] \\
&= \CovComb[\Ndesign](\Par) - \frac{1}{2} \CovComb[\Naugment](\Par). 
\end{align*}
 
\end{enumerate}

\end{proof}


% Questions and TODOs
\section{Questions and TODOs}
\subsection{Notation}
\begin{enumerate}
\item How to clean up notation for all of the different approximations being considered here? 
\end{enumerate}

\subsection{Numerical Experiments}
\begin{enumerate}
\item Banana, unimodal, bimodal, unidentifiable
\item Heat equation (see Sinsbeck and Nowak) 
\end{enumerate}

\subsection{Need to add}
\begin{enumerate}
\item Summary of results from noisy MCMC literature. 
\end{enumerate}

\subsection{Things to consider trying}
\begin{enumerate}
\item Perhaps give some context by discussing connections to Bayesian optimization and to log-likelihood approximation used in 
simulation-based inference. 
\item Hyperparameter marginalization: need to look into opportunities for closed-form hyperparameter marginalization during 
sequential design phase. 
\item Think about comparing the two forms of variance inflation in the frequency domain. 
\end{enumerate}

\subsection{Limitations of existing literature}
\begin{enumerate}
\item Very little discussion of case where likelihood parameters are unknown. 
\item Lack of emphasis on batch design (with some exceptions).
\item Little guidance on which approximation/design criterion to choose.
\end{enumerate}

\bibliography{post_approx_with_GP_emulators} 
\bibliographystyle{ieeetr}

\end{document}







