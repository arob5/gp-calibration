\documentclass[12pt]{article}
\RequirePackage[l2tabu, orthodox]{nag}
\usepackage[main=english]{babel}
\usepackage[rm={lining,tabular},sf={lining,tabular},tt={lining,tabular,monowidth}]{cfr-lm}
\usepackage{amsthm,amssymb,latexsym,gensymb,mathtools,mathrsfs}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{epstopdf,enumitem,microtype,dcolumn,booktabs,hyperref,url,fancyhdr}
\usepackage{algorithmic}
\usepackage[ruled,vlined,commentsnumbered,titlenotnumbered]{algorithm2e}
\usepackage{bbm}

% Plotting
\usepackage{pgfplots}
\usepackage{xinttools} % for the \xintFor***
\usepgfplotslibrary{fillbetween}
\pgfplotsset{compat=1.8}
\usepackage{tikz}

% Local custom commands. 
\include{latex_macros_general}
\include{latex_macros_gp_inv_prob}
\newcommand{\bphi}{\boldsymbol{\phi}}

\setlist{topsep=1ex,parsep=1ex,itemsep=0ex}
\setlist[1]{leftmargin=\parindent}
\setlist[enumerate,1]{label=\arabic*.,ref=\arabic*}
\setlist[enumerate,2]{label=(\alph*),ref=(\alph*)}

% For embedding images
\graphicspath{ {./images/} }

% Specifically for paper formatting 
\renewcommand{\baselinestretch}{1.2} % Spaces manuscript for easy reading

% Formatting definitions, propositions, etc. 
\newtheorem{definition}{Definition}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{corollary}{Corollary}

% Title and author
\title{Gaussian Process Surrogates for Bayesian Inverse Problems: Posterior Approximation and Sequential Design}
\author{Andrew Roberts}

\begin{document}

\maketitle


% Introduction 
\section{Introduction}

% Setting and Background 
\section{Setting and Background}

\subsection{Bayesian Inverse Problems}
We consider a \textit{forward model} $\fwd: \R^{\dimPar} \to \R^{\dimObs}$ describing some system of interest, parameterized by 
input parameters $\Par \in \parSpace \subset \R^{\dimPar}$. In addition, we suppose that we have noisy observations 
$\obs \in \obsSpace \subset \R^{\dimObs}$ of the output signal which $\fwd(\Par)$ seeks to approximate. The 
\textit{inverse problem} concerns learning the parameter values $\Par$ such that $\fwd(\Par) \approx \obs$; i.e., \textit{calibrating}
the model so that it agrees with reality. The statistical approach to this problem assumes that the link between model outputs and 
observations is governed by a probability distribution on  $\obs | \Par$. We assume that this distribution admits a density 
with corresponding log-likelihood 
\begin{align}
\llik: \parSpace \to \R, \label{log_likelihood}
\end{align}
such that $p(\obs | \Par) = \Exp{\llik(\Par)}$ 
\footnote{For succinctness, the notation $\llik(\Par)$ suppresses dependence on the data $\obs$, 
as we assume the data is fixed throughout the analysis.}
\footnote{We are currently focusing on the forward model parameter $\Par$, treating other likelihood 
parameters (e.g., observation covariance) as fixed. We will relax this assumption 
in section \ref{section_lik_par}.}.

The Bayesian approach completes this specification with a prior distribution 
on $\Par$. Letting $\priorDens(\Par)$ denote the density of this distribution, the Bayesian solution of the inverse problem is given 
by the posterior distribution $\postDensNorm(\Par) := p(\Par | \obs)$. The posterior density is computed by Bayes' rule 
\begin{align}
\postDensNorm(\Par) = \frac{1}{\normCst}\postDens(\Par) := \frac{1}{\normCst} \priorDens(\Par) \Exp{\llik(\Par)}, \label{post_dens}
\end{align}
with normalizing constant given by 
\begin{align}
\normCst &= \int \priorDens(\Par) \Exp{\llik(\Par)} d\Par. \label{norm_cst}
\end{align}
We emphasize that throughout this paper $\postDens(\Par)$ denotes the \textit{unnormalized} 
posterior density. In addition to considering 
generic likelihoods, we will give special attention to the additive Gaussian noise model
\begin{align}
\obs &= \fwd(\Par) + \noise \label{inv_prob_Gaussian} \\
\noise &\sim \Gaussian(0, \likPar) \nonumber \\
\Par &\sim \priorDens \nonumber, 
\end{align}
with corresponding log-likelihood 
\begin{align}
\llik(\Par) &= \log\Gaussian(\obs| \fwd(\Par), \likPar) 
= -\frac{1}{2} \log\det (2\pi \likPar) - \frac{1}{2} (\obs - \fwd(\Par))^\top \likPar^{-1} (y - \fwd(\Par)). \label{llik_Gaussian}
\end{align}

In general, the nonlinearity of $\fwd$ precludes a closed-form characterization of the posterior. In this case, the 
standard approach is to instead draw samples from the distribution using a Markov chain Monte Carlo (MCMC) 
algorithm. Such algorithms are iterative in nature, often requiring $\BigO(10^6)$ iterations, with each 
iteration involving an evaluation of the unnormalized posterior density $\postDens$. In the inverse problem 
context, this computational requirement is often prohibitive when the forward model evaluations $\fwd(\Par)$
incur significant computational cost. This becomes clear upon observing that each log-likelihood evaluation 
$\llik(\Par)$ (e.g., \ref{llik_Gaussian} in the additive Gaussian case) requires the corresponding forward 
model evaluation $\fwd(\Par)$. Motivated by this problem, a large body of work has focused on deriving 
cheap approximations to either $\fwd(\Par)$ or $\llik(\Par)$ (note that approximating the former induces 
an approximation of the latter). We refer to such approximations interchangeably 
as \textit{surrogates} or \textit{emulators}. The focus of the present work is on utilizing \textit{Gaussian processes} 
to derive emulators for the forward model or log-likelihood. 

\subsection{Gaussian Processes}
In this section, we provide a brief review of Gaussian processes; for in depth treatments, we refer readers to 
(TODO: cite Gramacy, Stuart, Rasmussen and Williams). A \textit{Gaussian process} (GP) is a probability 
distribution over a set of functions $\func: \parSpace \to \R$, defined by the property that 
he random vector $[\func(\Par_1), \dots, \func(\Par_\Ndesign)]^\top$ has
a joint Gaussian distribution for any set of inputs $\parMat := [\Par_1, \dots, \Par_\Ndesign] \subset \parSpace$. 
A GP is defined by its \textit{mean function} $\gpMean: \parSpace \to \R$ and positive-definite \textit{kernel} 
(i.e., \textit{covariance function}) $\gpKer: \parSpace \times \parSpace \to \R$ and is thus denoted by 
$\GP(\gpMean, \gpKer)$. Throughout this paper, we will vectorize notation as follows. Letting 
$\tilde{\parMat} := [\tilde{\Par}_1, \dots, \tilde{\Par}_M]  \subset \parSpace$ be a second set of inputs, we define 
$\func(\parMat) := [\func(\Par_1), \dots, \func(\Par_\Ndesign)]^\top \in \R^{\Ndesign}$ (and similarly for $\gpMean(\parMat)$), 
$\gpKer(\parMat, \tilde{\parMat}) := \{\gpKer(\Par_{\idxDesign}, \tilde{\Par}_m)\}_{1 \leq \idxDesign, m \leq \Ndesign} \in \R^{\Ndesign \times M}$, 
and $\gpKer(\parMat) := \gpKer(\parMat, \parMat) \in \R^{\Ndesign \times \Ndesign}$. Given this notation, 
the defining property of the GP can be written as 
\begin{align}
\func(\parMat) &\sim \Gaussian(\gpMean(\parMat), \gpKer(\parMat)).
\end{align}
Now suppose that we have observed the (noiseless) function evaluations $\func(\designIn)$ at a set of \textit{design points} $\designIn$. 
It is well-known (TODO: citations) that the conditional distribution $\func|\func(\designIn)$ is also a GP
\begin{align}
\funcEm[\Ndesign] := \func|\func(\designIn) \sim \GP(\gpMean[\Ndesign], \gpKer[\Ndesign])
\end{align}
with mean and kernel given by 
\begin{align}
\gpMean[\Ndesign](\parMat) &= \gpMean(\parMat) + \gpKer(\parMat, \designIn) \gpKer(\designIn)^{-1} [\func(\designIn) - \gpMean(\designIn)] \label{kriging_eqns} \\
\gpKer[\Ndesign](\parMat) &= \gpKer(\parMat) - \gpKer(\parMat, \designIn) \gpKer(\designIn)^{-1} \gpKer(\designIn, \parMat). \nonumber
\end{align}
The predictive mean can be seen to interpolate the design points (i.e., $\gpMean[\Ndesign](\designIn) = \func(\designIn)$), with 
the predictive variance vanishing at these inputs (i.e., $\gpKer[\Ndesign](\designIn) = 0$). To break these interpolation properties,
a constant can be added to the diagonal of the kernel matrix so that $(\gpKer(\designIn) + \nuggetSD^2 I)^{-1}$ replaces 
$\gpKer(\designIn)^{-1}$ in \ref{kriging_eqns}. In the present context, the maps $\fwd$ and $\llik$ are assumed to be deterministic functions 
of $\Par$ and hence the interpolation property is desirable. Nevertheless, a small, positive fixed constant $\nuggetSD^2$ is still 
typically included for numerical stability. 

Our default choice of kernel function is the popular \textit{exponentiated quadratic} 
(also called \textit{squared exponential} or \textit{Gaussian}), which is given by 
\begin{align}
\gpKer(\Par, \tilde{\Par}) &= 
\margSD^2 \Exp{- \sum_{\idxParDim=1}^{\dimPar} \left(\frac{\Par_{\idxParDim} - \tilde{\Par}_{\idxParDim}}{\lengthscale_{\idxParDim}} \right)^2}, 
\end{align}
defined by the choice of positive hyperparameters $\margSD, \lengthscale_{1}, \dots, \lengthscale_{\dimPar}$. We typically take the mean 
function to be a constant or quadratic. We opt for the common empirical Bayes approach of fixing all mean and kernel hyperparameters 
at their maximum likelihood estimates. 

Finally, we consider the extension to multi-output functions $\func: \parSpace \to \R^{\dimObs}$. There is a large literature on multi-output 
kernels (TODO: cite), but we focus here on the simplest case in which each output is modeled separately as an independent GP. 
In this setting, the notation $\gpKer(\Par)$ is now interpreted as the $\dimObs \times \dimObs$ diagonal matrix collecting the variance of each 
independent GP. Similarly, for a set of inputs $\parMat \in \R^{M \times \dimPar}$, $\gpKer[\Ndesign](\parMat)$ is now the 
$M\dimObs \times M\dimObs$ block diagonal matrix constructed from the predictive covariances of each independent GP. 





% Posterior Approximation
\section{Posterior Approximation}
\subsection{Deterministic Approximations}
\subsection{Noisy MCMC}

% Sequential Design
\section{Sequential Design}
\subsection{Background}
\subsection{Integrated Expected Variance}

% Learning Likelihood Parameters
\section{Learning Likelihood Parameters} \label{section_lik_par}

% Appendix 
\section{Appendix}

% Questions and TODOs
\section{Questions and TODOs}
\subsection{Notation}
\begin{enumerate}
\item Cleanest way to handle notation for llik emulation vs. forward model emulation? 
\item Should I define $\postDens$ to be unnormalized, or introduce separate notation for the unnormalized density. 
\item Should I define everything in terms of the potential (i.e. negative llik) instead of the llik? 
\item Think about how to define the domain/co-domain of the forward model; should this differ from the par/observation space? 
\end{enumerate}



\bibliography{framework_calibrating_ecosystem_models} 
\bibliographystyle{ieeetr}

\end{document}







