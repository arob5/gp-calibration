\documentclass[11pt]{article}
\RequirePackage[l2tabu, orthodox]{nag}
\usepackage[main=english]{babel}
\usepackage[rm={lining,tabular},sf={lining,tabular},tt={lining,tabular,monowidth}]{cfr-lm}
\usepackage{amsthm,amssymb,latexsym,gensymb,mathtools,mathrsfs}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{epstopdf,enumitem,microtype,dcolumn,booktabs,hyperref,url,fancyhdr}
\usepackage[margin=1.0in]{geometry}

% Plotting
\usepackage{pgfplots}
\usepackage{xinttools} % for the \xintFor***
\usepgfplotslibrary{fillbetween}
\pgfplotsset{compat=1.8}
\usepackage{tikz}

% Custom Commands
\newcommand*{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand*{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand*{\suchthat}{\,\mathrel{\big|}\,}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\Ker}{\mathrm{Ker}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Cor}{\mathrm{Corr}}
\newcommand{\Prob}{\mathbb{P}}
\DeclarePairedDelimiterX\innerp[2]{(}{)}{#1\delimsize\vert\mathopen{}#2}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\def\R{\mathbb{R}}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\setlist{topsep=1ex,parsep=1ex,itemsep=0ex}
\setlist[1]{leftmargin=\parindent}
\setlist[enumerate,1]{label=\arabic*.,ref=\arabic*}
\setlist[enumerate,2]{label=(\alph*),ref=(\alph*)}

% Specifically for paper formatting 
\renewcommand{\baselinestretch}{1.2} % Spaces manuscript for easy reading

% Formatting definitions, propositions, etc. 
\newtheorem{definition}{Definition}
\newtheorem{condition}{Condition}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{notation}{Notation}

\begin{document}

\begin{center}
Scalable Emulator-Based Bayesian Methods for Scientific Machine Learning
\end{center}

The effectiveness of formally combining scientific theory with statistical and machine learning methods is increasingly being recognized \cite{Willcox, Laubmeier, Wikle}. 
Physical models encapsulate prior scientific knowledge, while a rigorous statistical framework allows for principled inference, prediction, and uncertainty quantification \cite{Wikle}. Computer models have become essential tools at this scientific-statistical interface, tackling problems from 
climate projections \cite{Canadell} to pandemic response \cite{Swallow}. The task of calibrating these models with noisy real-world data continues to present
significant challenges, requiring tradeoffs between realism and computational and statistical feasibility. Given the ubiquity of computer models in informing
 evidence-based policy, it is essential to develop methods to handle more complex models while simultaneously providing a full accounting of uncertainties.
 My proposed research aims to confront these challenges by developing novel methodological and computational approaches for integrating large-scale datasets with expensive computer simulations. 
 % Recent calls have highlighted the need to bring together state-of-the-art computational methods with modern statistical 
%  and machine learning models in order to address these problems \cite{Wikle, Baker}.

Complex processes are commonly analyzed using a computer model (i.e. \textit{simulation} or \textit{process model}) $f$, where the output is a function of some unknown, unobserved parameter $\theta$.
For example, to study the terrestrial carbon cycle, ecologists use a process model 
that predicts the net exchange of carbon between an ecosystem and the atmosphere \cite{Fer}. Running the model requires specifying values for parameters that 
may not be known, such as the soil respiration rate or seasonal leaf growth \cite{Fer}. A Bayesian statistical formulation is typically employed to estimate (i.e. calibrate) these unknown parameters by comparing simulation outputs to field observations $y$ of the true physical process \cite{Kennedy}. For example, assuming the simple data model $y \sim N(f(\theta), \sigma^2 I)$ and prior $(\theta, \sigma^2) \sim \pi_0$ allows for inference through summaries of the posterior distribution $p(\theta|y) \propto N(y|f(\theta), \sigma^2 I)\pi_0(\theta)$, which typically involves using an iterative inference procedure like Markov Chain Monte Carlo (MCMC). In practice, the runtime of the simulator $f$ may be prohibitively long, precluding the tens of thousands of evaluations typically required by MCMC. A standard approach to address this problem is to fit a Gaussian process 
emulator $\hat{f}(\cdot) \sim \mathcal{GP}(m(\cdot), k(\cdot, \cdot))$ to approximate $f$, which is then used in place of $f$ in the inference algorithm \cite{Fer, Kennedy, Cleary}. Computational challenges are compounded when statistical models must account for complex spatiotemporal dynamics of the underlying process, as is common in scientific applications. My research goal is to advance methodology in emulator-based inference for complex spatiotemporal systems, enabling accurate large-scale calibration of computer models.

 There remains significant work to be done in scaling computer model analysis to such complex settings. Existing large-scale calibration schemes typically sacrifice uncertainty quantification to ease computational demands, which is especially problematic when physical models inform policy decisions. To date, few studies consider the uncertainty introduced by the emulator approximation of the true model $f$. Exceptions include an assortment of ad hoc approaches that use the Gaussian process predictive variance $k(\cdot, \cdot)$ to augment the 
 noise term in the data model, e.g. $\sigma^2 + k(\theta, \theta)$ \cite{Fer, Cleary}. While a step in the right direction, these methods treat emulator uncertainty as independent across different values of $\theta$, ignoring the correlation structure captured in the predictive variance $k(\cdot, \cdot)$. Uncertainty is further neglected when process and statistical parameters are treated as fixed, despite the underlying spatiotemporal nature of the physical system. Modern dynamic hierarchical models provide an ideal framework for quantifying spatial and temporal variation in model parameters \cite{Wikle}, but such models are typically infeasible in computer modeling settings due to their computational demands. Existing examples of assimilating large-scale spatiotemporal datasets with computer models typically utilize thrifty alternatives to full Bayesian inference, ignoring posterior uncertainty \cite{Sun}.
 
% Uncertainty quantification is further complicated when spatiotemporal dynamics are included. The existing computer modeling literature has focused on spatially or temporally indexed simulator outputs \cite{Fadikar}, but rarely considers variation in process parameters. 

% ...acknowledges that emulation and approximation uncertainty practically have the same effect as data uncertainty, and hence should be reflected in the posterior distribution. 

\noindent
\textbf{Research Plan.} To address the challenges highlighted above, I will begin by investigating the fundamental issue of emulator uncertainty in the 
non-dynamic setting. Building upon this foundation, I will work to embed this core methodology in a dynamic spatiotemporal statistical model. These modeling steps will pose increasingly difficult computational challenges, which I will address by developing novel inference methodology and approximation schemes. I will validate these methods on benchmark problems in computer modeling and scientific machine learning \cite{Surjanovic, Takamoto}, as well as terrestrial carbon models actively used in ecological forecasts \cite{Dietze}. This work is collaborative in nature, co-advised by Jonathan Huggins, an expert in scalable Bayesian inference and machine learning, and Michael Dietze, an ecologist with expertise in ecological forecasting and the terrestrial carbon cycle. 
 
 \textbf{1. Propagating Emulator Uncertainty.} I will begin by conducting a rigorous study of emulator uncertainty to fill the current gap in the literature, developing novel algorithms for propagating Gaussian process uncertainty and comparing their performance to the baseline independent variance adjustment schemes detailed above. To incorporate the predictive Gaussian process correlation structure, I will leverage the correlation between the current and proposed parameter values in Metropolis-Hastings MCMC algorithms. I will generalize an existing re-sampling method \cite{Fer} to jointly sample over this bivariate distribution, as well as develop analytic approaches that integrate the Gaussian process out of the likelihood to average over emulator uncertainty. 
I will then generalize these ideas to error models for simulators with time-indexed output $f(\theta) = (f_1(\theta), f_2(\theta), \dots, f_T(\theta))$, a common feature in scientific applications. To accomplish this I will analyze two approaches: (i.) reducing the dimensionality of the output via basis-function 
 approximations, allowing the application of my previous methods to each basis dimension separately, and (ii.) directly emulating the likelihood under an auto-regressive error model instead of the simulator itself, thus reducing back to univariate outputs \cite{Fer}.
 My analysis will include the derivation of theoretical results obtained by extending existing parameter calibration theory \cite{Tuo}, numerical tests from the 
 \textit{Virtual Library of Simulation Experiments} \cite{Surjanovic}, and validation on the SIPNET terrestrial carbon model \cite{Fer}. This ecological simulation uses a simplified model of the carbon cycle, yielding a faster runtime which affords the opportunity to validate against more computationally intensive approaches. This work will contribute novel theoretical results and off-the-shelf, scalable algorithms for emulator uncertainty propagation in statistical inference, supplanting existing ad hoc approaches with rigorous, validated methodology. 

 \textbf{2. Spatiotemporal Dynamics.}
 The next phase of my proposal involves embedding computer simulations within a spatiotemporal statistical model, while continuing to build upon the uncertainty quantification methods detailed above. I will adopt a state-space framework, drawing from recent advances in Bayesian filtering 
 \cite{Sarkka} and dynamic spatiotemporal modeling \cite{Wikle, Hefley}. I will model outputs and parameters as spatiotemporal processes, considering models generally of the form $y(s, t) = f(\theta(s, t)) + \epsilon(s, t)$. In particular, I will continue to expand upon my previous basis function approach for outputs $y$, and will initially explore linear Markovian models for $\theta$ and $\epsilon$ \cite{Wikle}.
These extensions yield high-dimensional hierarchical models that pose problems for existing inference algorithms. To address these issues I will develop novel methodology that extends recent work on inference for high-dimensional partially observed dynamic systems \cite{Park}. Integrating these methods with my Gaussian process emulation and uncertainty quantification framework will face the additional computational bottleneck posed by the poor scalability of Gaussian process prediction. To deal with this I will adopt a well-known parallelizable local Gaussian process approximation \cite{Gramacy}, making adjustments to correctly propagate the additional approximation uncertainty. I will validate these methods using \textit{PDEBench}, which offers a suite of time-varying simulations as benchmarks for scientific machine learning methods \cite{Takamoto}. I will further validate my methodology by scaling the SIPNET carbon model calibration to cover the contiguous United States, and ultimately progress to similar tests of the more computationally-intensive ED2 carbon simulator \cite{Fer, Fer2}.

\section{Stuff to move to broader impacts or intellectual merit}
\begin{itemize}
\item For example, Gaussian process emulation is commonly employed in terrestrial carbon modeling to calibrate sophisticated carbon cycle simulations that can have a runtime of many hours \cite{Fer}.
\item For example, terrestrial carbon models are typically calibrated independently at different ecological data collection sites \cite{Fer2}.
\item Large-scale, multi-site parameter calibration has the potential to drastically increase the generalizability of carbon models and improve spatiotemporally-informed decision making. 
\end{itemize}

\begin{thebibliography}{20}
\bibitem{Willcox} Willcox, K.E., Ghattas, O. \& Heimbach, P. The imperative of physics-based modeling and inverse theory in computational science. Nat Comput Sci 1, 166–168 (2021). https://doi.org/10.1038/s43588-021-00040-z
\bibitem{Laubmeier} Laubmeier et al. Ecological Dynamics: Integrating Empirical, Statistical, and Analytical Methods. Trends Ecol Evol. 2020 Dec;35(12):1090-1099. doi: 10.1016/j.tree.2020.08.006. Epub 2020 Sep 12. PMID: 32933777.
\bibitem{Wikle} Wikle, C.K. (2015), Modern perspectives on statistics for spatio-temporal data. WIREs Comput Stat, 7: 86-98. https://doi.org/10.1002/wics.1341
\bibitem{Canadell} Canadell et al, 2021: Global Carbon and
other Biogeochemical Cycles and Feedbacks. In Climate Change 2021: The Physical Science Basis. Contribution of
Working Group I to the Sixth Assessment Report of the Intergovernmental Panel on Climate Change. Cambridge University Press,
Cambridge, United Kingdom and New York, NY, USA, pp. 673–816, doi:10.1017/9781009157896.007.
\bibitem{Swallow} Swallow et al, Challenges in estimation, uncertainty quantification and elicitation for pandemic modelling, Epidemics, Volume 38, 2022, 100547, ISSN 1755-4365, https://doi.org/10.1016/j.epidem.2022.100547.
\bibitem{Baker} Baker et al. "Analyzing Stochastic Computer Models: A Review with Opportunities." Statist. Sci. 37 (1) 64 - 89, February 2022. https://doi.org/10.1214/21-STS822
\bibitem{Fer} Fer, I., Kelly, R., Moorcroft, P. R., Richardson, A. D., Cowdery, E. M., and Dietze, M. C.: Linking big models to big data: efficient ecosystem model calibration through Bayesian model emulation, Biogeosciences, 15, 5801–5830, https://doi.org/10.5194/bg-15-5801-2018, 2018.
\bibitem{Kennedy} Kennedy, M.C. and O'Hagan, A. (2001), Bayesian calibration of computer models. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 63: 425-464. https://doi.org/10.1111/1467-9868.00294
\bibitem{Stuart} A. M. Stuart (2010). Inverse problems: A Bayesian perspective. Acta Numerica, 19, pp 451-559 doi:10.1017/
S0962492910000061.
\bibitem{Cleary} Emmet Cleary, Alfredo Garbuno-Inigo, Shiwei Lan, Tapio Schneider, Andrew M. Stuart, “Calibrate, emulate, sample”, Journal of Computational Physics, Volume 424, 2021, 109716, ISSN 0021-9991, https://doi.org/10.1016/j.jcp.2020.109716.
\bibitem{Fadikar} Fadikar et al, Calibrating a Stochastic, Agent-Based Model Using Quantile-Based Emulation. SIAM/ASA Journal on Uncertainty Quantification. https://doi.org/10.1137/17M1161233.
\bibitem{Sun} Sun et al, Synthesizing simulation and field data of solar irradiance. Stat Anal Data Min: The ASA Data Sci Journal. 2019; 12: 311– 324. https://doi.org/10.1002/sam.11414
\bibitem{Surjanovic} Surjanovic et al, Virtual Library of Simulation Experiments: Test Functions and Datasets.
\bibitem{Takamoto} Takamoto et al, PDEBench: An Extensive Benchmark for Scientific Machine Learning. Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track
\bibitem{Dietze} Dietze et al, “Iterative near-term ecological forecasting: Needs, opportunities, and challenges”, Proceedings of the National Academy of Sciences, 115, 7, 1424-1432, 2018.
\bibitem{Tuo} Tuo et al, A theoretical framework for calibration in computer models: parametrization, estimation and convergence properties. 
\bibitem{Sarkka} Särkkä, S. (2013). Bayesian Filtering and Smoothing (Institute of Mathematical Statistics Textbooks). Cambridge: Cambridge University Press. doi:10.1017/CBO9781139344203
\bibitem{Hefley} Trevor J. Hefley, Mevin B. Hooten, Ephraim M. Hanks, Robin E. Russell, Daniel P. Walsh, Dynamic spatio-temporal models for spatial data, Spatial Statistics.
\bibitem{Park} Park et al, Inference on high-dimensional implicit dynamic models using a guided intermediate resampling filter.
\bibitem{Gramacy} Gramacy et al, Local Gaussian process approximation for large computer experiments, 2013.
\bibitem{Fer2} Istem Fer, Alexey Shiklomanov, Kimberly A. Novick, Christopher M. Gough, M. Altaf Arain, Jiquan Chen, Bailey Murphy, Ankur R. Desai, Michael C. Dietze: Capturing site-to-site variability through Hierarchical Bayesian calibration of a process-based dynamic vegetation model, bioRxiv 2021.04.28.441243; doi: https://doi.org/10.1101/2021.04.28.441243. 

\end{thebibliography}



\end{document}



