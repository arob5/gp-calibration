\documentclass[12pt]{article}
\RequirePackage[l2tabu, orthodox]{nag}
\usepackage[main=english]{babel}
\usepackage[rm={lining,tabular},sf={lining,tabular},tt={lining,tabular,monowidth}]{cfr-lm}
\usepackage{amsthm,amssymb,latexsym,gensymb,mathtools,mathrsfs}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{epstopdf,enumitem,microtype,dcolumn,booktabs,hyperref,url,fancyhdr}
\usepackage{algorithmic}
\usepackage[ruled,vlined,commentsnumbered,titlenotnumbered]{algorithm2e}
\usepackage{bbm}

% Plotting
\usepackage{pgfplots}
\usepackage{xinttools} % for the \xintFor***
\usepgfplotslibrary{fillbetween}
\pgfplotsset{compat=1.8}
\usepackage{tikz}

% Custom Commands
\newcommand*{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand*{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand*{\suchthat}{\,\mathrel{\big|}\,}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\Ker}{\mathrm{Ker}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\bw}{\mathbf{w}}

\DeclarePairedDelimiterX\innerp[2]{(}{)}{#1\delimsize\vert\mathopen{}#2}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}


\setlist{topsep=1ex,parsep=1ex,itemsep=0ex}
\setlist[1]{leftmargin=\parindent}
\setlist[enumerate,1]{label=\arabic*.,ref=\arabic*}
\setlist[enumerate,2]{label=(\alph*),ref=(\alph*)}

% For embedding images
\graphicspath{ {./images/} }

% Specifically for paper formatting 
\renewcommand{\baselinestretch}{1.2} % Spaces manuscript for easy reading

% Formatting definitions, propositions, etc. 
\newtheorem{definition}{Definition}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{corollary}{Corollary}

% Title and author
\title{Loss Emulation for Scalable Ecosystem Model Calibration}
\author{Andrew Roberts}

\begin{document}

\maketitle
\tableofcontents
\newpage

% Notation 
\section{Notation}

\subsection{Statistical Setting}
We denote the vector of calibration parameters by $\btheta \in \Theta \subset \R^D$. The forward model (e.g. SIPNET or ED2) $G(\btheta)$ maps calibration parameters to the model outputs. Note that the forward 
model also depends on initial conditions and model drivers, but these are fixed throughout the analysis so are suppressed in the notation. Model outputs consist 
of $T$ time steps for each of $P$ output variables, so $G(\btheta) \in \mathbb{R}^{T \times P}$. We have observed data $Y \in \mathbb{R}^{T \times P}$, potentially with missing values. Let $T_p$ denote the number 
of non-missing observations of output variable $p$. We index the observations of output $p$ as $\{y_{tp}\}_{t = 1}^{T_p}$, meaning that $Y$ is then technically a ragged matrix. 
This is suitable for the model detailed below, as the ordering of the observations is inconsequential. 

To measure error between model 
predictions $G(\btheta)$ and observed data $Y$, we assume the following Gaussian noise model. 
\begin{align*}
\mathcal{L}(\btheta) := \log p(Y|\mathbf{\btheta}) = \sum_{p = 1}^{P} \sum_{t = 1}^{T_p} \log \mathcal{N}\left(y_{tp}| G_{tp}(\btheta), \sigma_p^2 \right)
\end{align*}
This likelihood assumes the errors are independent across time and output variable. Note that $\mathcal{L}(\btheta)$ depends on $Y$, but $Y$ is constant throughout the analysis so we drop it from the notation. Missing 
observations are simply ignored, hence the product from $t = 1, \dots, T_p$ for each output $p$.

For notational convenience, we collect the variance parameters in the matrix $\Sigma := \text{diag}\left(\sigma_1^2, \dots, \sigma_P^2 \right)$. 
We denote the priors on the calibration and variance parameters as  $\pi_0(\btheta, \Sigma) = \pi_0(\btheta)\pi_0(\Sigma)$, assuming prior independence.  
The variances $\sigma_p^2$ are assigned inverse gamma priors $\sigma_p^2 \overset{ind}{\sim} \mathcal{IG}(\alpha_p, \beta_p)$ so that 
\[\pi_0(\Sigma) = \prod_{p = 1}^{P} \mathcal{IG}(\sigma_p^2|\alpha_p, \beta_p)\]
Combining these priors with the likelihood yields the posterior 
\[\pi(\btheta, \Sigma) := p(\btheta, \sigma_1^2, \dots, \sigma_P^2|Y) \propto e^{\mathcal{L}(\btheta)}\pi_0(\btheta, \Sigma)\]

\subsection{Surrogate Model}
In this section we describe the emulator used to approximate the true log likelihood $\mathcal{L}(\btheta)$. The log likelihood can be written as 
\begin{align*}
\mathcal{L}(\btheta) &= \sum_{p = 1}^{P} \sum_{t = 1}^{T_p} \log \mathcal{N}\left(y_{tp}| G_{tp}(\btheta), \sigma_p^2 \right) \\
			       &= \sum_{p = 1}^{P}  \sum_{t = 1}^{T_p} \left[-\frac{1}{2} \log(2\pi \sigma_p^2) - \frac{1}{2\sigma_p^2} (y_{tp} - G_{tp}(\btheta))^2\right]  \\
			       &= -\frac{1}{2} \sum_{p = 1}^{P} \left[T_p \log\left(2\pi \sigma_p^2 \right) + \frac{1}{\sigma_p^2}\sum_{t = 1}^{T_p} \left(y_{tp} - G_{tp}(\btheta)\right)^2  \right] \\
			       &= -\frac{1}{2} \sum_{p = 1}^{P} \left[T_p \log\left(2\pi \sigma_p^2 \right) + \frac{\Phi_p(\btheta)}{\sigma_p^2} \right] 
\end{align*}
where 
\[\Phi_p(\btheta) := \sum_{t = 1}^{T_p} (y_{tp} - G_{tp}(\btheta))^2\]
is the sum of squared errors between the observed data and the model output for the $p^{\text{th}}$ output variable. Note that $\Phi_p(\btheta)$ also depends on $Y$, but again we suppress this in the notation. The key observation 
is that the model evaluations $G(\btheta)$ only appear in the likelihood through the $\Phi_p(\btheta)$, which means that approximating $\Phi_p(\btheta)$ will induce an approximation of the likelihood. The independence assumptions 
that yield the product form of the likelihood make it so that $\Phi(\btheta)$ is a sufficient statistic which is not a function of the variance parameters. This means that $\Phi(\btheta)$ can be emulated without regard for the variance parameters, 
and then inference can be performed over the variance parameters as usual. The choice to emulate the mappings $\Phi_p: \Theta \to \R$ also reduces the problem to 
approximating $P$ univariate functions, instead of approximating $G(\btheta)$, which has output dimension $T \times P$. 

Our emulator of choice uses Gaussian processes (GP). In particular, we treat the $\Phi_p$ as unknown and assign them independent GP priors
\[\Phi_p(\cdot) \overset{ind}{\sim} \mathcal{GP}(\mu_p, k_p(\cdot, \cdot))\] 
where $\mu_p$ is a constant mean and $k_p(\cdot, \cdot)$ a covariance function (i.e. kernel). The constant mean and kernel hyperparameters are fixed to their MLE estimates. We then run the forward model at carefully selected 
\textit{design points} $\btheta_1, \dots, \btheta_N \in \Theta$ and compute $\{\Phi_p(\btheta_n)\}_{1 \leq p \leq P, 1 \leq n \leq N}$. 

We denote the resulting observed data for output $p$ as 
$\mathcal{D}_p := \left\{(\btheta_1, \Phi_1(\btheta_1)), \dots, (\btheta_P, \Phi_p(\btheta_P))  \right\}$ and collect the observed outputs in a vector 
$\boldsymbol{\phi}_p = \left[\Phi_1(\btheta_1), \dots, \Phi_P(\btheta_1) \right]^{\top}$. Conditioning the GPs on the observed data yields the GP predictive distributions 
\[\Phi_p(\cdot)|\mathcal{D}_p := \Phi_p^*(\cdot) \sim \mathcal{GP}(\mu^*_p(\cdot), k_p^*(\cdot, \cdot)), \text{ for } p = 1, \dots, P\]

\subsection{Dynamic Simulators}
I believe this work nicely complements a series of papers on so-called \textit{dynamic} or \textit{time-series valued} emulation. The necessity of such emulators typically arise from forward models 
defined as a the solution of an autonomous system of ODEs. I will denote the state vector of such a system by 
\[\bx(t) = \left(x_1(t), \dots, x_P(t) \right)^{\top} \]
If we consider discretizing at constant time steps $\Delta t$ then since the system is autonomous the \textit{one-step map} or \textit{flow map} $g: \R^P \times \R^Q \to \R^P$ is time-invariant: 
 \[\bx(t + \Delta t) = g(\bx(t); \bw_{t + \Delta t})\]
 where $\bw_{t + \Delta t}$ is a \textit{forcing input}. The forcing inputs are assumed known, and vary across time. 
 Note that this could also be generalized to variable step sizes $\Delta t$ by considering the flow map to also be a function of the step size; i.e., $g = g(\bx(t), \Delta t)$. We can also view the 
 vector-valued $g$ as a collection of $P$ univariate flow maps
 \[g(\bx(t); \bw_{t + \Delta t}) = \left[g_1(\bx(t); \bw_{t + \Delta t}), \dots, g_P(\bx(t); \bw_{t + \Delta t}) \right]^{\top}\]
 so that $g_k: \R^P \to \R$ maps $\bx(t)$ to $\bx_p(t + \Delta t)$, the value of the $p^{\text{th}}$ state at the subsequent time step. Of course, $g$ and hence the states $\bx(t)$ depend on 
 the parameters $\btheta$. I will reflect this dependence by writing $g_{\btheta}$ and $\bx_{\btheta}(t)$. The forward model $G: \R^D \to \R^{T \times P}$ defined previously can thus be characterized 
 as 
 \[
 G(\btheta; \bx_0, \bw) = \begin{pmatrix} \bx_0^{\top} \\ \bx_\theta(\Delta t)^{\top} \\  \bx_\theta(2\Delta t)^{\top} \\ \vdots \\ \bx_\theta\left([T-1]\Delta t\right)^{\top} \end{pmatrix} = 
 \begin{pmatrix} \bx_0^{\top} \\ g_\theta(\bx_0; \bw_1)^{\top} \\  g_\theta\left(g_\theta(\bx_0; \bw_1); \bw_2 \right)^{\top} \\ \vdots \\ g_\theta^{(T-1)}(\bx_0; \bw)^{\top} \end{pmatrix}
 \]
 where I now explicitly write $G$ as a function of an initial condition $\bx_0 = \bx(0)$, which is independent of $\btheta$, and I use the shorthand $g_\theta^{(k)}$ to denote 
 the composition consisting of $k$ applications of the map $g_\theta$. I also let $\bw := \{\bw_t\}_{t = 1}^{T}$ denote the sequence of forcing inputs. 
 I have also assumed that the initial time is $0$, but some other time $t_0$ could of course be considered. 

\subsection{MCMC Algorithm}
We consider the joint posterior $p(\btheta, \Sigma, \Phi^*|Y)$ and seek to marginalize out $\Phi^*$ in order to obtain the posterior of interest $p(\btheta, \Sigma|Y)$. A Gibbs sampling approach would proceed as 
\begin{itemize}
\item $\Phi \sim p(\Phi^*|\btheta, \Sigma, Y) = p(\Phi^*)$
\item $\btheta \sim p(\btheta|\Sigma, \Phi^*, Y)$
\item $\Sigma \sim p(\Sigma|\btheta, \Phi^*, Y)$
\end{itemize}
Under the model assumptions, the $\Phi^*$ and $\Sigma$ conditionals can be directly sampled from, while the $\btheta$ conditional requires a Metropolis step. The current PEcAn algorithm modifies the above by re-sampling 
$p(\Phi^*)$ again before the final line. Thus, each sweep over the conditionals results in two samples from $\Phi^*$ and one sample each from $\btheta$ and $\Sigma$. It is also important to note that 
$\Phi^*$ is a function, and in practice we only sample the function values at the points of interest. This implies the distribution sampled from is actually the marginal distribution of $\Phi^*(\btheta)$:
\[\Phi(\btheta) \sim p(\Phi^*(\btheta))\]
A more accurate high-level view of the current algorithm is thus given by 
\begin{itemize}
\item $\Phi(\btheta) \sim p(\Phi^*(\btheta))$
\item $\btheta \sim p(\btheta|\Sigma, \Phi^*(\btheta), Y)$
\item $\Phi(\btheta) \sim p(\Phi^*(\btheta))$
\item $\Sigma \sim p(\Sigma|\btheta, \Phi^*(\btheta), Y)$
\end{itemize}

\subsubsection{Conditional Posteriors}


% The Big Picture
\section{The Big Picture}
In this section, I will take as given the motivation for using mechanistic models to study the terrestrial carbon cycle and the benefits of applying Bayesian statistical methods when using such models. 
Given these assumptions, I would say the main goal from a methodological point of view is to \textit{develop scalable methods for calibrating ecosystem models which also provide well-calibrated
uncertainty estimates.} To achieve the goal of scalability, we opt for an emulator-based approach. To help address the UQ goal, we use GPs as our emulator of choice. With these baseline assumptions 
established, I detail below the challenges with this approach and how this work can go beyond existing literature to address some of these challenges. 

\begin{enumerate}
\item \textbf{Challenge}: Terrestrial ecosystem models are typically systems of ODEs, and hence the output of the forward model is time-valued, presenting a challenge for standard GP-based emulation. 
	\begin{enumerate}
	\item This problem is fairly well-studied (e.g. \cite{10.1093/biomet/asp028, doi:10.1198/016214507000000888, acp-11-12253-2011, MOHAMMADI2019178, 10.1214/09-BA415, doi:10.1137/120900915, psf2021003011}). 
	My main takeaway from reviewing literature is that many of the proposed methods are \textit{not well-suited for large-scale Bayesian inverse problems}. 
		\begin{enumerate}
		\item The majority of the focus in the computer modeling literature is on directly emulating model outputs. This often results in either 1.) poor scalability (e.g. time-input emulator) or 2.) emulation approaches 
		that are only relevant when $\btheta$ is fixed. 
		\item In specific engineering/scientific fields there is more emphasis on parameter estimation, but it is difficult to find any sort of comprehensive examination of methods. 
		\end{enumerate}
		\item \textbf{Solution}: The loss-emulation approach is an attractive solution, as it directly targets the goal of model calibration, scales well in $T$, and reduces to the problem of emulation of univariate 
		functions. 
	\end{enumerate}
\item \textbf{Challenge}: What are the relative merits and weaknesses of the loss emulation approach? 
	\begin{enumerate} 
	\item Loss-emulation is a subset of methods which I have seen referred to as \textit{scalarization}. The main alternative method in this vein is direct emulation of the log-likelihood. 
	In our setting, the obvious benefit of loss-emulation over likelihood emulation is the ability to also perform inference over likelihood parameters. The alternative in likelihood emulation would 
	require fixing the likelihood parameters in advance. On the other hand, the log-likelihood emulation approach has the benefit of being unconstrained, while the loss function must be non-negative. 
	\item The primary alternatives to the loss emulation approach involve direct emulation of model outputs, which usually involves either 1.) emulating the coefficients of a basis representation for the 
	outputs or 2.) utilizing one of various forms of ``dynamic emulators.''
	\item \textbf{Solution}: We must demonstrate that the loss emulation approach represents an attractive choice when the goal is solving the inverse problem. In general this will require demonstrating that
	the loss function can be reliably emulated by GPs. We can also emphasize the scalable nature of this approach, as compared to some of the alternatives. 
	Some specifics to address when assessing the viability of emulators of the loss function: 
		\begin{enumerate} 
		\item Does the non-negativity of the loss function present a problem for GPs, either with regard to its mean estimation or uncertainty calibration? If so, are there effective methods to deal with the non-negativity constraint? 
		\item How sensitive are emulation results to the choice of GP kernel? Which kernels seem to work well in this setting? 
		\item How about alternative specifications of the mean function? Some authors have reported improved performance of GP emulators with polynomial mean functions. 
		\item This will also require developing reliable validation and model checking procedures for the emulator. 
		\end{enumerate}
	\end{enumerate}
\item \textbf{Challenge}: How should the emulator be integrated into the MCMC algorithm? 
	\begin{enumerate}
	\item How can the emulator be incorporated in such a way that allows for efficient sampling while also providing an adequate accounting of additional uncertainty introduced due to the GP 
	approximation? 
	\item \textbf{Solution}: We must demonstrate the benefits and weaknesses of the sampling-based approach. Benefits include better uncertainty propagation (assuming the GP is well-calibrated) 
	and general applicability (don't have to compute expectations). Some other specifics: 
		\begin{enumerate} 
		\item What is the effect of jointly sampling the proposed/current parameters using GP predictive covariance?
		\end{enumerate} 
	\end{enumerate}
\item \textbf{Challenge}: We would like an experimental design method that can be applies to settings where posteriors may be very concentrated relative to current knowledge about the parameters. 
	\begin{enumerate}
	\item It is very common in the literature for studies to default to LHS. This may be appropriate when the model response is over interest over the entire domain, but may be very inefficient 
	when the goal is to solve an inverse problem. 
	\item \textbf{Solution}: In our tests with relatively inexpensive models, it would enlightening to determine the extent of posterior contraction. We would also like to determine an experimental 
	design scheme that is well-suited to this setting and places design points where they are needed relative to the true posterior. After an initial literature review, some promising methods to 
	investigate include the sequential design scheme \ref{doi:10.1137/15M1047659} and the ensemble Kalman approach \ref{Cleary_2021, https://doi.org/10.1029/2020MS002454}. 
	\end{enumerate}
\end{enumerate}

I conclude this section by summarizing what I believe to be the biggest challenges in this work. These challenges all stem from the characteristics of the response surface in this loss emulation setting: 
the response surface tends to have 1.) a very large dynamic range and 2.) a very sharp dip at local optima. This presents a challenge both for selecting a design and fitting the GPs. A space-filling design 
may completely miss the ``dips'', while placing many design points in regions with essentially zero posterior mass. The shape of the response surface also exhibits some non-stationarity, presenting issues 
when optimizing GP hyperparameters. The main issue here is thus to develop an experimental design method and emulator fitting procedure that is able to cope with these challenges.


% Abstract 
% \section{Abstract}
% The primary scientific goal is to improve prediction and understanding of ecosystem dynamics. By combining the strengths of mechanistic models and data-driven analysis, model-data fusion 
% has proven to be a valuable approach in working towards this goal. Effective model-data fusion requires careful and robust uncertainty quantification. In particular, uncertainty must be acknowledged 
% in empirically-determined estimates of process model parameters. A Bayesian statistical framework...

% Background/Literature Review
\section{Background/Literature Review}
 
 \subsection{Likelihood Emulators/Loss Emulators/Scalarization}
 The idea of reducing dimensionality by emulating some sort of scalar-valued function summarizing the model-data discrepancy (e.g. loss function, log-likelihood) is certainly well-known, but 
 it is difficult to find any sort of systematic study of this approach or comparison to alternative approaches. 
 
 The paper \cite{LEBEL2019158} utilizes a GP approximation of the log-likelihood 
 $\mathcal{L}(\btheta)$. This requires fixing the likelihood parameters in advance. 
 
%Their GP-assisted MCMC algorithm consists of posterior evaluations of the form 
% \[\hat{\pi}(\btheta) = \frac{1}{L} \sum_{l = 1}^{L} \pi(\btheta|\ell_l) \text{, where } \ell_l \overset{iid}{\sim} \ell^*(\btheta)\]
% where $\pi(\btheta|\ell_l)$ indicates the deterministic posterior density approximation constructed by plugging in $\ell_l$ in place of the true log-likelihood $\ell(\btheta)$. 
% The current PEcAn approach generally utilizes the same approximation with $L = 1$ (and emulating $\Phi(\cdot)$ instead of $\ell(\cdot)$). For simplicity above, I wrote 
% $\ell_l \overset{iid}{\sim} \ell^*(\btheta)$, indicating pointwise GP predictive draws. Surprisingly, this is not actually what the authors do. They are 
% instead concerned with sampling GP trajectories (functions) $\ell_l(\cdot) \overset{iid}{\sim} \ell^*(\cdot)$; that is, they seek to draw GP realizations across the entire input space 
% which respect the correlation structure, and then evaluate those realizations at $\btheta$ to obtain the final values $\{\ell_l\}_{l = 1}^{L} = \{\ell_l(\btheta)\}_{l = 1}^{L}$. Since doing this 
% would require sampling the GP over a dense grid of points, they instead propose an approximation that only uses a sparse set of ``conditioning points''. The conditioning 
% set $\Theta_c \subset \Theta$ is defined to target regions where the GP is confident that the likelihood is larger. In particular, it is defined as 
% \begin{align*}
%\Theta_c &:= \left\{\btheta \in \Theta : \Prob\left(\ell^*(\btheta) > \ell^*(\btheta_{\text{max}}) \right) > \rho \right\} \\
%\btheta_{\text{max}} &:= \text{argmax}_{\btheta \in \Theta} \E\left[\ell^*(\btheta)\right]
% \end{align*}
% where $\rho \in [0, 1]$ is some specified threshold. i.e., the conditioning set is the subset of the input space where the GP assigns high probability of improvement. The authors' description is 
% not entirely clear to me, but I believe this is what they do to construct each $\ell_l$: 
% \begin{enumerate}
% \item Draw samples at the conditioning points: $\ell(\Theta_c) \sim \ell^*(\Theta_c)$. 
% \item Treat $\left(\Theta_c, \ell(\Theta_c)\right)$ as pseudo-data and condition the GP $\ell^*$ on this pseudo-data as if they were true observations. Denote the conditioned GP as $\ell^*_c$. 
% \item Return $\ell_i := \E\left[\ell_c^*(\btheta) \right]$
% \end{enumerate}
% I believe the idea is to condition so that the uncertainty in the distribution collapses and the mean function of the conditioned GP is hopefully a good approximation of the trajectory. Note that 
% $\left(\Theta_c, \ell(\Theta_c)\right)$ are observations of the true trajectory. It is interesting the authors' go to this trouble given that they could simply sample from the GP marginal 
% $\ell^*(\btheta)$. 
  
 \subsection{Dynamic Emulators}
 There have been many proposals to emulate such dynamical systems. Many such approaches consider emulation of the trajectory $\{\bx_\theta(k\Delta t)\}_{k = 1}^{T-1}$ as a function of 
 the initial condition $\bx_0$, but assuming fixed $\btheta$. This is not very useful when parameter calibration is the goal, as the dynamics change when $\btheta$ changes. I briefly summarize some 
 of the relevant literature below. 
 
 \subsubsection{Early work of Conti and O'Hagan}
 Here I summarize the papers \cite{10.1093/biomet/asp028} and \cite{CONTI2010640}. 
 
\subsubsection{Emulating the Flow Map}
Although they assume $\btheta$ to be fixed, Mohammadi et al (2018) \cite{MOHAMMADI2019178} propose some interesting ideas. They focus on emulating the flow map $g(\cdot)$ 
(I omit the $\btheta$ subscript as it is constant here). The idea is to fix a small step $\Delta t$, and learn a map from the initial condition $\bx_0$ to $\bx(\Delta t)$. This produces an emulator 
that can produce one-step-ahead predictions. Their emulator consists of $P$ univariate emulators, one for each $g_p(\cdot)$. Prediction requires iteratively composing GP predictions, which 
propagates the uncertainty forward. This can actually be viewed as a deep GP, though it avoids the main difficulty of having to fit kernel hyperparameters in a deep GP. Inference is conducted 
via a Monte Carlo approach, propagating GP samples iteratively through each time step. 

\section{Methods}

\subsection{Statistical Model}
\subsubsection{Likelihood: Multi-objective calibration}

\subsubsection{Priors}

\bigskip
\noindent
\textbf{Question}: Does PEcAn currently always assume prior independence across the $\theta_d$? 

\subsubsection{Exact MCMC-based inference}

\subsection{Loss Emulation}
Every iteration of MCMC requires evaluation of the forward model $G(\btheta)$. If this evaluation is costly, then this will render MCMC computationally intractable. To address this, we employ a surrogate modeling approach. 
The forward model $G(\btheta)$ is replaced by an an approximation $\hat{G}(\btheta)$ known as an \textit{emulator}. Assuming $\hat{G}(\btheta)$ is much faster to evaluate then we substitute it for the exact model and thus 
sample from the approximate posterior
 \[\hat{\pi}(\btheta, \Sigma) \propto \hat{\mathcal{L}}(\btheta)\pi(\btheta)p(\Sigma)\]
 where 
 \begin{align*}
\hat{\mathcal{L}}(\theta) = \prod_{p = 1}^{P} \prod_{t = 1}^{T_p} \mathcal{N}\left(y_{tp}| \hat{G}_{tp}(\btheta), \sigma_p^2 \right)
\end{align*}
 
\subsubsection{Emulator Details}
\begin{enumerate}
\item \textbf{Problem}: The model output $G(\btheta)$ has dimension $T \times P$. For any reasonable number of time steps $T$, the high-dimensionality of the output renders the problem of directly emulating $G$ intractable.  \\
	\textbf{Solution}: One approach is to reduce the dimensionality of the output by representing it with respect to a basis, and then instead emulate the mapping from $\btheta$ to the basis coefficients. We consider an alternative 
	of instead directly emulating the loss between the forward model and observed data. 
	
\item \textbf{Problem}: Issues with emulating the $\Phi_p$: non-linear, need to account for uncertainty in approximation. \\
\textbf{Solution}: GP emulators. \\
I need to write out this section, but to establish notation I briefly note that I will write $\Phi_p^*$ to denote the posterior/predictive distribution; i.e. the random field approximation of $\Phi_p$. I intentionally refrain from calling 
this the predictive \textit{GP} since technically this could be a rectified or truncated Gaussian, or sometimes a Student-t process. This induces a random field approximation of the likelihood, which I similarly write as 
$\mathcal{L}^*$, noting that the randomness in $\mathcal{L}^*$ comes from $\Phi^*_1, \dots, \Phi^*_P$. 
	
\end{enumerate}

\subsubsection{Emulator-based MCMC inference}


\subsection{Sequential Design}
\begin{enumerate}
\item \textbf{Problem}: While space-filling designs (e.g. via LHS or maximin) are common, they can be very inefficient in the Bayesian inverse problem setting. If the posterior distribution is concentrated in a small 
subset of the parameter space, then space-filling designs will yield many design points in regions that are not of interest, while simultaneously under-sampling the region of significance.  \\
	\textbf{Solution}: Sequential design approach that takes into account knowledge of the posterior as it proceeds. 
	
\subsection{Other things?}
\begin{itemize}
\item Scaling factors for PFTs. 
\item Imbalanced data constraints. 
\end{itemize}
	
\end{enumerate}

% Details on Specific Methods
\section{Details on Specific Methods}

\subsection{Evaluation Metrics}

\begin{enumerate}
\item Challenges: 
	\begin{enumerate} 
	\item Assessing GP performance in a way that takes into account the GP distribution, i.e. challenge of validating probabilistic forecasts. 
	\item The fact that we would like metrics to take into account the true posterior over $\btheta$ when possible. 
	\end{enumerate}
\item \textit{Diagnostics for Gaussian Process Emulators} (Bastos and O'Hagan, 2009) \\
Discusses various criteria for evaluating GP emulators. A lot of focus is on defining suitably standardized residuals, the tricky part being that GP residuals are correlated. For example, interpreting 
a QQ plot with typical standardized residuals is a bit difficult given the fact that the residuals are correlated. 
\item \textit{Diagnostics and Simulation-Based Methods for Validating Gaussian Process Emulators} (Al-Taweel, PhD thesis from 2018). 
\item \textit{Quantifying Model Error in Bayesian Parameter Estimation} (White, PhD thesis from 2015)
\item Strictly proper scoring rules/CRPS
\item Would like to read up on some more applied papers (e.g. Simon Mak's work on emulating spatiotemporal flows) to see what sort of metrics they report. 
\end{enumerate}

\subsection{Range-Constrained GP}
\subsubsection{Do nothing to constrain GP prior}
\begin{itemize}
\item Current approach; instead transform GP posterior to truncated or rectified Gaussian. 
\item Concern is that the prior is wrong, so may compromise GP fit and uncertainty calibration. 
\end{itemize}

\subsubsection{Warped GP}
\begin{itemize}
\item For example, a log-normal or square root GP. 
\item We have seen that blindly applying the log-normal process (LNP) leads to very bad results due to the funneling effect of the log-transformation; i.e. the sum of squared residuals (SSR) often exhibits a very high dynamic range, 
with some very large values and others that are almost zero. In the VSEM examples at least, this effect was especially bad as the funneling occurs at some values of SSR, which means the emulator fared worse in the 
most important region of the parameter space. 
\item In simple one-dimensional examples, I showed a while back that instead transforming the data as $\log\left(\Phi_p(\theta) + C \right)$ for a well-chosen constant $C$ could yield nice LNP results. One approach to explore 
would be to optimize the value of $C$. 
\end{itemize}

\subsubsection{Other range-constrained GP methods that have been proposed:}
\begin{enumerate}
\item ``fit all available model data and impute a set of ``artificial data"" throughout the input space points that maintain the constraint.'' (Spiller et al, 2023)
\item Another approach: modify the MLE for hyperparameter fitting. In particular, Pensoneault et al., 2020 propose a method whereby the lengthscale parameters are constrained so that the predictive GP obeys the 
range constraint at untested inputs with high probability. This has the benefit that the predictive GP distribution is still Gaussian, as opposed to some other methods. 
\item \textit{The Zero Problem: Gaussian Process Emulators for Range-Constrained Computer Models} (Spiller et al, 2023) \\
They are dealing with zero-censored computer models, and they consider a prior on the computer model $f(\btheta)$: $f(\btheta) \sim \max \left\{0, g(\btheta) \right\}$ where $g \sim \mathcal{GP}$. Not that relevant to our 
case but some interesting ideas here.  
\end{enumerate}


\subsection{Sequential Design}

\subsubsection{Notation and Background}
Throughout this section I let $\mathcal{D}_n := \{[\btheta_1, \Phi(\btheta_1)], \dots, [\btheta_n, \Phi(\btheta_n)]\}$ denote the set of current (observed) design points and corresponding outputs. 
The sequential design task is that of choosing the 
next design point $\btheta_{n+1}$. For a GP $\Phi^*$, I let
\[k_{\mathcal{D}_{n}}(\btheta) = \Var(\Phi^*(\btheta)|\mathcal{D}_{n})\]
denote the predictive variance of the GP $\Phi^*$, which has been conditioned on data $\mathcal{D}_n$. Up to this point, I have been using $k^*(\btheta)$ to denote the predictive GP variance 
at the input $\btheta$, where the asterisk differentiates the predictive (i.e. conditioned on $\mathcal{D}_n$) GP distribution from the GP prior. This notation did not make explicit the data being 
conditioned on. However, in this experimental design context the emphasis is on design points so the notation $k_{\mathcal{D}_{n}}(\btheta)$ will prove more useful. 
In the below notes, we will often be considering the merits of adding a new particular design point $\btheta_{n + 1}$. I denote by 
\[k_{\mathcal{D}_{n+1}}(\btheta) = \Var(\Phi^*(\btheta)|\mathcal{D}_{n+1}) = \Var(\Phi^*(\btheta)|\mathcal{D}_{n}, [\btheta_{n+1}, \Phi(\btheta_{n+1})])\]
the predictive variance at $\btheta$ for the GP that has also been conditioned on the new observation $[\btheta_{n+1}, \Phi(\btheta_{n+1})]$. Conveniently, the GP predictive 
variance is not a function of the observed output $\Phi(\btheta_{n+1})$, so the above expression for the variance could just as well be written as conditioning only on 
$\btheta_{n+1}$, rather than $[\btheta_{n+1}, \Phi(\btheta_{n+1})]$. 

A commonly used acquisition function for choosing $\btheta_{n+1}$ is the \textit{integrated mean squared prediction error} (IMSPE). 
\begin{align*}
J(\btheta_{n+1}) = \int_{\Theta} k_{\mathcal{D}_{n+1}}(\btheta) \rho(\btheta) d\btheta = \E_{\btheta \sim \rho} \left[k_{\mathcal{D}_{n+1}}(\btheta) \right]
\end{align*}
where $\rho$ is some density on the input space $\Theta$. The next design point is selected by solving 
\[\btheta_{n + 1} := \text{argmin}_{\tilde{\btheta}_{n+1} \in \Theta} J(\tilde{\btheta}_{n+1}) \]
The IMSPE is intuitive in that it seeks to select the point that reduces the overall uncertainty (integrated over the whole input space, weighted by $\rho$) as much as possible. 
The expectation defining the IMSPE is in general not analytically tractable, but is available in closed-form in simple settings, such as when the input space is a 
hyperrectangle and $\rho(\btheta) \equiv 1$. For our purposes, it is important to emphasize that $J(\btheta_{n+1})$ is a well-defined, non-random quantity due to the fact 
that the GP predictive variance does not depend on the output $\Phi(\btheta_{n+1})$. If it did depend on this output, then $J(\btheta_{n+1})$ would be random since 
$\Phi(\btheta_{n+1})$ is not yet known. 

The tricky thing about our setting is that we often want to consider the random field approximations $L^*(\tilde{\btheta})$ and $\pi^*(\tilde{\btheta})$ induced by the GP approximation 
$\Phi^*(\tilde{\btheta})$. These random fields are no longer Gaussian, hence they do not have the convenient property that their predictive variances are independent of the observed 
outputs. Therefore, defining an analog of IMSPE would require an additional integral averaging over the uncertainty in the model outputs. For example, 
\begin{align*}
J_{L}(\btheta_{n+1}) &= \E_{\Phi^*(\btheta_{n+1})} \E_{\btheta \sim \rho} \Var_{\Phi^*} \left(L^*(\btheta) | \mathcal{D}_{n+1} \right)
\end{align*}
Breaking this down, $\Var_{\Phi^*} \left(L^*(\btheta) | \mathcal{D}_{n+1} \right)$ is predictive variance at input $\btheta$ of the random field $L^*$. Note that this is conditioned on 
$\mathcal{D}_{n+1}$, which includes the new (not-yet-observed) data point $[\btheta_{n+1}, \Phi(\btheta_{n+1})]$. $L^*$ is not Gaussian-distributed and hence this predictive variance
does indeed depend on the unobserved output $\Phi(\btheta_{n+1})$. Therefore, the outer expectation $ \E_{\Phi^*(\btheta_{n+1})}$ integrates over the current 
(conditional on $\mathcal{D}_n$) predictive GP distribution over $\Phi(\btheta_{n+1})$. The remaining expectation $\E_{\btheta \sim \rho}$ simply averages over the input 
space as before, with $\rho$ allowing for the possibility of non-uniform weights. Although they do not motivate their proposed methods in this way, Sinsbeck and Nowak 
\cite{doi:10.1137/15M1047659} essentially propose the use of $J_{L}(\btheta_{n+1})$ as an acquisition function for experimental design in Bayesian inverse problems. They 
choose to weight by the prior when averaging over the input space (i.e., $\rho(\btheta) = \pi_0(\btheta)$). 

\subsubsection{Sinsbeck and Nowak (2017)}
Sinsbeck and Nowak \cite{doi:10.1137/15M1047659} propose a sequential design method which constructs and refines a deterministic approximation to the true likelihood $L(\btheta)$ at each iteration of the sequential design procedure. 
This likelihood approximation is then used to construct an acquisition function that targets regions of high posterior density. I summarize their method here, with adjustments to tailor the method to our loss-emulation setting. 

\bigskip
\noindent
\textbf{Motivation from Integrated Mean Squared Prediction Error.}
The authors do not introduce their method in this way, but I find this motivation to be helpful. The \textit{integrated mean squared prediction error} (IMSPE) is a commonly-used acquisition function for 
experimental design with GPs. 



\bigskip
\noindent
\textbf{Deterministic likelihood approximation.}
The first step is to construct the deterministic likelihood approximation $\hat{\mathcal{L}}_n(\btheta)$ from 
$\mathcal{L}^*_n(\btheta)$, where the subscript $n$ indicates that both the random field approximation $\mathcal{L}^*_n(\btheta)$ and deterministic approximation $\hat{\mathcal{L}}_n(\btheta)$ have been constructed from the data 
$\mathcal{D}_n = \left\{(\btheta_1, \mathcal{L}(\btheta_1)), \dots, (\btheta_n, \mathcal{L}(\btheta_n)) \right\}$. One option is to define $\hat{\mathcal{L}}_n(\btheta)$ as the approximation resulting from plugging in the GP predictive 
means. However, this fails to account for the uncertainty encoded by the GP predictive distribution. Sinsbeck and Nowak take a more principled approach, defining $\hat{\mathcal{L}}_n(\btheta)$ as the approximation that minimizes 
a certain loss criterion. The loss criterion of choice is the prior-weighted $L^2$ error:
\[\ell(\mathcal{L}, \mathcal{L}^\prime) := \E_{\btheta}\left[\left(\mathcal{L}(\btheta) - \mathcal{L}^\prime(\btheta) \right)^2 \right]\]
where the notation $\E_{\btheta}(\cdot)$ indicates integration with respect to the prior $\pi_0$. 
However, in this setting the goal is to choose the $\hat{\mathcal{L}}_n$ that minimizes error with respect to a \textit{random} approximation $\mathcal{L}_n^*$. Thus, the error measure is defined as the 
expectation of $\ell(\mathcal{L}^*_n, \hat{\mathcal{L}}_n)$ with respect to the GP $\Phi^*$. This yields the \textit{Bayes' risk}
\[\mathcal{R}(\mathcal{L}_n^*, \mathcal{L}_n) := \E_{\Phi^*} \E_{\btheta} \left[\left(\mathcal{L}^*(\btheta) - \mathcal{L}(\btheta) \right)^2 \right] \]
The deterministic approximation is taken as the minimizer  
\[\hat{\mathcal{L}}_n := \text{argmin}_{\mathcal{L}_n} \mathcal{R}(\mathcal{L}_n^*, \mathcal{L}_n) \]
which can be shown to be given by 
\[\hat{\mathcal{L}}_n(\btheta) = \E_{\Phi^*}\left[\mathcal{L}_n^*(\btheta) \right]\]
i.e. the approximation resulting from integrating the GP out of the random field $\mathcal{L}_n^*(\btheta)$. Evaluated at the optimizer, the Bayes' risk reduces to the prior-weighted predictive variance 
of $\mathcal{L}_n^*(\btheta)$: 
\begin{align}
\hat{\mathcal{R}}(\mathcal{L}_n^*) := \mathcal{R}(\mathcal{L}_n^*, \hat{\mathcal{L}}_n) = \E_{\btheta} \Var_{\Phi^*}\left[\mathcal{L}_n^*(\btheta)\right] \label{Bayes_Risk}
\end{align}
The authors also consider a ``fully Bayesian'' likelihood estimation approach, which turns out to result in the same exact estimator. 

\bigskip
\noindent
\textbf{Acquisition Function.}
The authors propose a greedy one-step look ahead strategy. Intuitively, we assess the quality of a new design point $\tilde{\btheta}$ by considering the GP predictive distribution 
at this proposed point $\tilde{\Phi} := \Phi(\tilde{\btheta})$. We use this predictive distribution to predict what the Bayes' risk \ref{Bayes_Risk} would be were we to observe 
$(\tilde{\btheta}, \tilde{\Phi})$ and condition on this additional observation. Recall that $\hat{\mathcal{R}}(\mathcal{L}_n^*)$ denotes the Bayes' risk between the 
random field approximation and optimal deterministic approximation. I introduce the notation $\tilde{\mathcal{L}}^*_n := \mathcal{L}^*_n|\tilde{\btheta}, \tilde{\Phi}$ to denote the 
random field likelihood approximation conditioned on $(\tilde{\btheta}, \tilde{\Phi})$. Note that the forward model has not yet been run at input parameter $\tilde{\btheta}$ so 
$\tilde{\Phi}$ is itself a random variable. I utilize the same notation for the GP itself, so that $\tilde{\Phi}^*_n := \Phi^*_n|\tilde{\btheta}, \tilde{\Phi}$

With this notation established, $\hat{\mathcal{R}}(\tilde{\mathcal{L}}^*_n)$ is an estimator for the future Bayes' risk, were we 
to run the forward model at $\tilde{\btheta}$. Unlike $\hat{\mathcal{R}}(\mathcal{L}^*_n)$, $\hat{\mathcal{R}}(\tilde{\mathcal{L}}^*_n)$ is a random quantity due to the fact that 
we have not run the model at $\tilde{\btheta}$ and thus $\tilde{\Phi}$ is unknown. In order to define a deterministic acquisition function, we must now integrate over the distribution of $\tilde{\Phi}$, in addition to the two other 
integrals in the original definition of the Bayes' risk. This yields the acquisition function, 
\begin{align}
J_n(\tilde{\btheta}) &:=  \E_{\tilde{\Phi}} \hat{\mathcal{R}}(\tilde{\mathcal{L}}^*_n) \\ 
			     &= \E_{\tilde{\Phi}} \E_{\btheta} \Var_{\tilde{\Phi}_n^*}\left[\tilde{\mathcal{L}}_n^*(\btheta) \right]
\end{align}
Note that the acquisition function is a function of $\tilde{\btheta}$, a new design point; not to be confused with the $\btheta$ in the second line above, which is averaged over by 
$\E_{\btheta}(\cdot)$. 

This is the final acquisition function considered in the paper. However, in our case the likelihood parameters $\Sigma$ are not known. We could consider augmenting to include these 
parameters and integrate over the prior on the likelihood parameters as well: 
\begin{align}
J_n(\tilde{\btheta}) &:= \E_{\tilde{\Phi}} \E_{\Sigma} \E_{\btheta} \Var_{\tilde{\Phi}_n^*}\left[\tilde{\mathcal{L}}_n^*(\btheta, \Sigma) \right]
\end{align}

\bigskip
\noindent
\textbf{Approximating the Acquisition Function.} \\
I now consider approximation of the above acquisition function. For simplicity, I drop the dependence on $\Sigma$ for now (we can think of $\btheta$ as containing both the 
calibration and likelihood parameters). I also suppose that the GP predictive distribution is Gaussian for now. First, recall that 
\begin{align*}
\tilde{L}_n^*(\btheta) &= \exp\left\{ -\frac{1}{2} \sum_{p = 1}^{P} \left[T_p \log\left(2\pi \sigma_p^2\right) + \frac{\tilde{\Phi}^*_p(\btheta)}{\sigma_p^2}\right] \right\} \\
				&= C_{\Sigma} \prod_{p = 1}^{P} \exp\left\{\frac{\tilde{\Phi}^*_p(\btheta)}{2\sigma_p^2}\right\}
\end{align*}
Recalling the GP predictive distribution, we have 
\[\frac{\tilde{\Phi}^*_p(\btheta)}{2\sigma_p^2}\bigg|\theta, \tilde{\Phi} \sim \mathcal{N}\left(\frac{\tilde{\mu}_p^*(\btheta)}{2\sigma_p^2}, \frac{\tilde{k}_p^*(\btheta)}{4\sigma_p^4} \right) \]
Note that the predictive mean $\tilde{\mu}_p^*(\btheta)$ and variance $\tilde{k}_p^*(\btheta)$ are not known, since these are the predictive quantities given that the GP has been 
conditioned on $\tilde{\Phi}$. 
The exponential of the above quantity is log-normally distributed
\[\exp\left\{\frac{\tilde{\Phi}^*_p(\btheta)}{2\sigma_p^2}\right\}\bigg|\theta, \tilde{\Phi} \sim \mathcal{LN}\left(\frac{\tilde{\mu}_p^*(\btheta)}{2\sigma_p^2}, \frac{\tilde{k}_p^*(\btheta)}{4\sigma_p^4} \right) \]
so the expectation is available in closed-form:
\[\E_{\tilde{\Phi}_p^*} \exp\left\{\frac{\tilde{\Phi}^*_p(\btheta)}{2\sigma_p^2}\right\} = \exp\left\{\frac{\tilde{\mu}_p^*(\btheta)}{2\sigma_p^2} + \frac{\tilde{k}_p^*(\btheta)}{8\sigma_p^4} \right\} \]
Since the univariate GPs are independent, we obtain 
\begin{align*}
\E_{\tilde{\Phi}^*} \tilde{L}_n^*(\btheta) &= C_\Sigma \prod_{p = 1}^{P} \E_{\tilde{\Phi}_p^*} \exp\left\{\frac{\tilde{\Phi}^*_p(\btheta)}{2\sigma_p^2}\right\} \\
							   &= C_\Sigma \exp\left\{\sum_{p = 1}^{P} \left[ \frac{\tilde{\mu}_p^*(\btheta)}{2\sigma_p^2} + \frac{\tilde{k}_p^*(\btheta)}{8\sigma_p^4} \right] \right\}
\end{align*}

\bigskip
\noindent
\textbf{Questions.}
\begin{enumerate}
\item How could this be modified to take advantage of parallel computing resources? 
\item Are the authors assuming the likelihood parameters are known/fixed? How would we account for unknown likelihood parameters? 
\end{enumerate}

\subsection{Sampling Design Points from the Approximate Posterior}
Let $\btheta_1, \dots, \btheta_n$ denote the current design. The task is to select the next design point $\btheta_{n + 1}$. Consider first sampling a set of candidates 
$\tilde{\btheta}_1, \dots, \tilde{\btheta}_M$ via a space-filling design (e.g. Latin hypercube). Supposing for the moment that posterior evaluations are cheap, then one approach 
could be to select $\btheta_{n + 1}$ by sampling from the candidate points with weights 
\begin{align*}
w\left(\tilde{\btheta}_j\right) := \frac{\pi(\tilde{\btheta}_j)}{\sum_{m = 1}^{M} \pi(\tilde{\btheta}_m)}
\end{align*}
Of course, these weights also depend on the likelihood parameters $\Sigma$ which are unknown. If the variances are all comparable then an alternative for the weights that 
does not depend on the likelihood parameters could be defined as 
\begin{align*}
w\left(\tilde{\btheta}_j\right) := \frac{\exp\left\{-\Phi(\tilde{\btheta}_j)\right\}}{\sum_{m = 1}^{M} \exp\left\{-\Phi(\tilde{\btheta}_m)\right\}}
\end{align*}
Alternatively, an estimate of the variances could be plugged in, or we could consider averaging over $\pi_0(\Sigma)$. Likelihood parameters aside, the obvious problem here is that 
evaluation of these weights requires running the forward model at every candidate point, which is exactly what we are trying to avoid. We can instead replace the $\Phi$ with 
the GP approximation $\Phi^*$, which induces an approximation to the posterior and hence the weights. We could then define the weights in various ways, one option being
\begin{align*}
w\left(\tilde{\btheta}_j\right) := \frac{\E_{\Phi^*} \left[\pi^*(\tilde{\btheta}_j)\right]}{\sum_{m = 1}^{M} \E_{\Phi^*} \left[\pi^*(\tilde{\btheta}_m)\right]}
\end{align*}

\subsection{Bayesian Optimization to find MAP Estimate}
Numerical examples with VSEM have shown that it is common for the posterior to be highly concentrated relative to the prior. Loosely speaking, the region of 
non-negligible posterior mass sometimes looks like a tiny point in the input space. Space-filling or sequential design schemes may completely miss this region 
of interest. Therefore, one approach is to first optimize, finding the MAP estimate to locate the general region of interest, and then proceed with a sequential 
design scheme from there. Ideally, we seek 
\begin{align*}
\btheta_{\text{MAP}}, \Sigma_{\text{MAP}} = \text{argmax}_{\btheta, \Sigma} \pi(\btheta, \Sigma)
\end{align*}
Note that we are currently considering optimizing with respect to both the calibration parameters $\btheta$ and likelihood parameters $\Sigma$. 
Function evaluations $\pi(\btheta, \Sigma)$ are expensive due to the fact that evaluating at a new value of $\btheta$ requires running the forward 
model. However, for a fixed value of $\btheta$, evaluations at different values of $\Sigma$ is significantly cheaper. We therefore might consider 
a coordinate ascent type algorithm, where $\btheta$ updates are given by a Bayesian optimization (BO) step, and $\Sigma$ updates by a traditional 
optimization. We begin by considering the former, where $\Sigma$ is assumed fixed. Throughout these optimization notes, I will use the notation 
\[\pi_n^{\min} := \min\left\{\pi(\btheta_1, \Sigma_1), \dots, \pi(\btheta_n, \Sigma_n) \right\}\]
where $\left\{(\btheta_i, \Sigma_i) \right\}_{i = 1}^{n}$ are the inputs associated with the currently observed outputs. 

\subsubsection{BO $\btheta$ update}
The BO $\btheta$ update is accomplished via the minimization of an acquisition function $\alpha(\btheta)$. 
\[\btheta_{n+1} := \text{argmin}_{\btheta \in \Theta} \alpha(\btheta) \]
I first consider the expected improvement (EI) acquisition function, which in 
this context looks like: 
\begin{align*}
\alpha_{\text{EI}}(\btheta) &= -\mathbbm{E}\left[\max\left\{0, \pi_n^{\min} - \pi_n^*(\btheta) \right\} \right]
\end{align*}
where the negative is included to align with the convention that acquisition functions are minimized. 
This expectation is available in closed-form when the distribution of $\pi_n^*(\btheta)$ is Gaussian. However, in this case the distribution is not Gaussian so 
we must resort to approximation. Consider a Monte Carlo approximation 
\begin{align*}
\alpha_{\text{EI}}(\btheta) &\approx \frac{1}{T} \sum_{t = 1}^{T} \max\left\{0, \pi_n^{\min} - \pi^{(t)}_n(\btheta) \right\}, \text{ where } \pi^{(t)}_n(\btheta) \overset{iid}{\sim} \pi_n^*(\btheta)
\end{align*}

\subsection{Minimizing $L^2$ error between log-likelihoods weighted by approximate posterior}
Building on the Sinsbeck and Nowak approach, here are a few things to consider modifying. 
\begin{enumerate}
\item Instead of weighting by the prior ($\rho(\btheta) = \pi_0(\btheta)$) instead weight by the approximate posterior $\pi^*(\btheta)$. 
\item Instead of minimizing error between likelihoods, minimize error between log-likelihoods. Or between log-posteriors. 
\end{enumerate}

Proceeding as in Sinsbeck and Nowak with these added modifications, I first define a loss function. 
\begin{align*}
\ell(f, g) := \E_{\btheta \sim \pi} \left[\left(f(\btheta) - g(\btheta) \right)^2 \right]
\end{align*}
This is a posterior-weighted $L^2$ error between the functions $f$ and $g$. In our case, $f$ and $g$ will be log-likelihoods. Since we are working with 
$\mathcal{L}_n^*$, a random field approximation of the log-likelihood, we consider additionally integrating over the uncertainty in $\mathcal{L}_n^*$. This 
yields the Bayes' risk 
\begin{align*}
\mathcal{R}_n(\tilde{\mathcal{L}}_n) := \E_{\btheta \sim \pi} \E_{\mathcal{L}_n^*}\left[\left(\mathcal{L}_n^*(\btheta) - \tilde{\mathcal{L}}_n(\btheta) \right)^2 \right]
\end{align*}
where $\tilde{\mathcal{L}}_n(\btheta)$ is some deterministic approximation to the likelihood, derived only using $\mathcal{D}_n$. The best deterministic approximation 
according to the Bayes' risk is given by 
\begin{align*}
\hat{\mathcal{L}}_n &= \text{argmin}_{\tilde{\mathcal{L}}_n} \E_{\btheta \sim \pi} \E_{\mathcal{L}_n^*}\left[\left(\mathcal{L}_n^*(\btheta) - \tilde{\mathcal{L}}_n(\btheta) \right)^2 \right]
\end{align*}
The inner expectation is minimized pointwise by $\tilde{\mathcal{L}}_n(\btheta) = \E_{\mathcal{L}_n^*}\left[\mathcal{L}_n^*(\btheta) \right]$. Plugging in this minimizer, the Bayes' 
risk becomes, 
\begin{align*}
\mathcal{R}_n(\hat{\mathcal{L}}_n) &=  \E_{\btheta \sim \pi} \E_{\mathcal{L}_n^*}\left[\left(\mathcal{L}_n^*(\btheta) - \E_{\mathcal{L}_n^*}\left[\mathcal{L}^*_n(\btheta)\right] \right)^2 \right] \\
						      &= \E_{\btheta \sim \pi} \Var_{\mathcal{L}_n^*}\left[\mathcal{L}_n^*(\btheta) \right]
\end{align*}
which is the posterior-weighted predictive variance of $\mathcal{L}_n^*$. The design criterion is then defined as a one-step lookahead version of the Bayes' risk; i.e. we seek to 
choose $\btheta_{n+1}$ such that conditioning $\mathcal{L}_n^*$ on the new observation $\left(\btheta_{n+1}, \mathcal{L}_{n+1} \right)$ 
(where $\mathcal{L}_{n+1} := \mathcal{L}(\btheta_{n+1})$) would yield the largest reduction in Bayes' risk. This is essentially applying the Active Learning Cohn (ALC) 
sequential design criterion to $\mathcal{L}_n^*$. There are two potential difficulties here: 
\begin{enumerate}
\item If $\mathcal{L}_n^*$ is not Gaussian, then the predictive variance $\Var_{\mathcal{L}_n^*}\left[\mathcal{L}_n^*(\btheta) \right]$ in general depends on the 
response values $\mathcal{L}_1, \dots, \mathcal{L}_n$. Thus, computation of the predictive variance at $\btheta$ given hypothetical observation of 
$\left(\btheta_{n+1}, \mathcal{L}_{n+1} \right)$ requires averaging over $\mathcal{L}_{n+1}$. This is in contrast to a GP, which has a predictive variance independent of 
the response data. 
\item The outer expectation requires integrating over the true posterior $\pi$, which is unknown. We instead consider replacing $\pi$ with an approximation constructed 
using $\mathcal{L}_n^*$. Sinsbeck and Nowak instead consider integrating with respect to the known 
prior $\pi_0$. In either case, generally there is no closed-form solution to this outer integral. 
\end{enumerate}
In general, the acquisition function is given by 
\begin{align}
J(\btheta_{n+1}) := \E_{\btheta \sim \rho} \E_{\mathcal{L}_{n+1}} \Var\left[\mathcal{L}^*(\btheta) | \mathcal{D}_{n+1} \right] \label{acquisition}
\end{align}
where $\mathcal{D}_{n+1} = \mathcal{D}_{n} \cup \left(\btheta_{n+1}, \mathcal{L}_{n+1} \right)$. I leave $\rho$ generic for now, but we can think of this as either 
$\pi_0$, $\pi$ (in the idealized case), or some approximation to $\pi$. 

\subsubsection{Case 1: Gaussian predictive distribution}
Here I suppose that the $\Phi_p^*$ have Gaussian distributions. This simplifies matters by yielding a closed-form solution for the predictive variance in \ref{acquisition}. Moreover, 
this predictive variance is independent of $\mathcal{L}_{n+1}$, thus eliminating the expectation $\E_{\mathcal{L}_{n+1}}$. We first recall that the log-likelihood has the form 
\begin{align*}
\mathcal{L}(\btheta) &= C_\Sigma - \frac{1}{2} \sum_{p = 1}^{P} \frac{\Phi_p(\btheta)}{\sigma_p^2}
\end{align*} 
Under the assumption of Gaussian predictive distributions, we then have 
\begin{align*}
\mathcal{L}^*(\btheta) &\sim \mathcal{N}\left(C_\Sigma - \frac{1}{2} \sum_{p = 1}^{P} \frac{\mu^*_p(\btheta)}{\sigma_p^2}, \frac{1}{4} \sum_{p = 1}^{P} \frac{k^*_p(\btheta)}{\sigma_p^4}  \right)
\end{align*}
I denote the above variance by $\Var\left[\mathcal{L}^*(\btheta) | \mathcal{D}_n \right]$ to make explicit the dependence on the data $\mathcal{D}_n$. In this setting, 
$\Var\left[\mathcal{L}^*(\btheta) | \mathcal{D}_n \right]$ does not depend on $\mathcal{L}_1, \dots, \mathcal{L}_n$. In particular, $\Var\left[\mathcal{L}^*(\btheta) | \mathcal{D}_{n+1} \right]$
does not depend on $\mathcal{L}_{n+1}$, which eliminates the expectation $\E_{\mathcal{L}_{n+1}}$ in \ref{acquisition}. 
Thus, the acquisition function simplifies to 
\begin{align}
J(\btheta_{n+1}) := \E_{\btheta \sim \rho} \Var\left[\mathcal{L}^*(\btheta) | \mathcal{D}_{n+1} \right] \label{acquisition_Gaussian_case}
\end{align}
where the inner variance can be computed explicitly. 


\subsection{Targeting the Metropolis Acceptance Ratio}
Let $\btheta_1, \dots, \btheta_n$ denote the current design. One idea is to select the next point $\btheta_{n + 1}$ in a way that reflects the transition probability in the MCMC sampler. 
In particular, if the Metropolis acceptance probability of transitioning from any of the current design points to some point $\tilde{\btheta}$ is essentially zero, then this point should be 
very unlikely to be selected as $\btheta_{n + 1}$. 

For now I ignore the likelihood parameters and consider a symmetric MCMC proposal density. Let $J_i(\tilde{\btheta})$ denote the Metropolis acceptance probability for transitioning 
from the design point $\btheta_i$, $1 \leq i \leq n$ to some other point $\tilde{\btheta} \in \Theta$; that is, 
\[J_i(\tilde{\btheta}) = \max\left\{1, \frac{\pi(\tilde{\btheta})}{\pi(\btheta_i)} \right\}, 1 \leq i \leq n \]
The model has already been run at the input $\tilde{\btheta}_i$ so $\pi(\btheta_i)$ may be evaluated up to the normalizing constant. However, $\pi(\tilde{\btheta})$ is unknown. For the moment 
consider that it were known. In this case, we might choose the next design point by sampling it in such a way that is proportional to the acceptance probabilities. Define 
\[J(\tilde{\btheta}) = \sum_{i = 1}^{n} J_i(\tilde{\btheta}) = \sum_{i = 1}^{n} \max\left\{1, \frac{\pi(\tilde{\btheta})}{\pi(\btheta_i)} \right\} \]
We could also consider an alternative without the maximum
\[J(\tilde{\btheta}) = \sum_{i = 1}^{n} \frac{\pi(\tilde{\btheta})}{\pi(\btheta_i)}\]
One simple strategy for utilizing $J(\cdot)$ to select the next design point would be to first sample a space-filling design $\tilde{\btheta}_1, \dots, \tilde{\btheta}_M$. We could then define the following
probability distribution over these points
\[\mu_J(\tilde{\btheta}_m) := \frac{J(\tilde{\btheta}_m)}{\sum_{j = 1}^{M} J(\tilde{\btheta}_j)}, 1 \leq m \leq M\]
and sample the next design point from $\tilde{\btheta}_1, \dots, \tilde{\btheta}_M$ according to this distribution. 

Now we address the fact that we cannot actually evaluate the $\pi(\tilde{\btheta}_m)$. The natural thing to do is to consider the GP-induced approximation $\pi^*(\tilde{\btheta}_m)$. Recalling the 
Gaussian likelihood, we have (for any point $\tilde{\btheta}$ at which the forward model has not been run and any current design point $\btheta_i$),
\begin{align*}
\frac{\pi^*(\tilde{\btheta})}{\pi(\btheta_i)} &= \frac{\pi_0(\tilde{\btheta}) C_\Sigma \exp\left(-\sum_{p = 1}^{P} \frac{\Phi_p^*(\tilde{\btheta})}{2\sigma_p^2} \right)}{\pi_0(\btheta_i)C_\Sigma \exp\left(- \sum_{p = 1}^{P} \frac{\Phi_p(\btheta_i)}{2\sigma_p^2} \right)} \\
							    &= \frac{\pi_0(\tilde{\btheta})}{\pi_0(\btheta_i)} \cdot \exp\left(\frac{1}{2} \sum_{p = 1}^{P} \frac{\Phi_p(\btheta_i) - \Phi_p^*(\tilde{\btheta})}{\sigma_p^2} \right)
\end{align*}
where $C_\Sigma$ is a constant which depends on $\sigma_1^2, \dots, \sigma_p^2$. In the case that the predictive distributions of the $\Phi^*_p$ are $\mathcal{GP}(\mu^*_p(\cdot), k_p^*(\cdot))$ then 
\begin{align*}
\frac{\pi^*(\tilde{\btheta})}{\pi(\btheta_i)} \sim \mathcal{LN}\left(\log\frac{\pi_0(\tilde{\btheta})}{\pi_0(\btheta_i)} + \frac{1}{2} \sum_{p = 1}^{P} \frac{\Phi_p(\btheta_i) - \mu_p^*(\tilde{\btheta})}{\sigma_p^2}, \frac{1}{4} \sum_{p = 1}^{P} \frac{k_p^*(\tilde{\btheta})}{\sigma_p^4} \right)
\end{align*}

\bigskip
\noindent
\textbf{Some issues to think about:}
\begin{itemize}
\item One concern is that the current design mostly consists of points in regions where the posterior density is very small. Then while the ``good'' points may have large $J(\tilde{\btheta}_m)$ these may get 
drowned out in the sampling due to the fact that there are so many ``bad'' points. 
\item Need to take into account the GP uncertainty to make sure we are sufficiently exploring the space in regions where the GP is uncertain. 
\item Could be interesting to explore some sort of scheme that alternates optimization steps like $\btheta_{n + 1} := \text{argmax}_{1 \leq m \leq M} \sum_{i = 1}^{n} \frac{\E_{\Phi^*}\left[\pi^*(\tilde{\btheta})\right]}{\pi(\btheta_i)}$
with sampling steps. 
\item What is the effect of defining $J(\cdot)$ that includes the maximum in the acceptance probability vs. excluding the maximum? Intuitively, I would think that excluding it would encourage more emphasis on 
optimization while including it would encourage more exploration. 
\item How to include the likelihood parameters? Add another expectation over $\Sigma$? 
\end{itemize}


\subsubsection{Ensemble Kalman Inversion}
TODO

% Range-Constrained GP
\section{Range-Constrained GP}

\subsection{Setup}
Here I summarize the method proposed in Pensoneault et al (2020) to constrain the GP MLE in order to enforce non-negativity in the GP predictive 
distribution. This has the benefit of maintaining the Gaussian predictive distribution, without having to resort to using truncated distributions. 
They consider a zero-mean GP 
\begin{align*}
y(\cdot) \sim \mathcal{GP}(0, k(\cdot, \cdot)) 
\end{align*}
Let $(X, y)$ denote $N$ observed input-output pairs. Let $K$ denote the kernel matrix with entries $K_{ij} = k(x_i, x_j)$. 
I assume (for reasons discussed below) the following kernel parameterization 
\begin{align*}
K = \nu\left(C + \tau^2 I_N \right)
\end{align*}
where $C$ has entries given by the squared exponential correlation function 
\begin{align*}
C_{ij} = \exp\left\{-\sum_{d = 1}^{D} \frac{((x_i)_d - (x_j)_d)^2}{\theta_d} \right\}
\end{align*}
Here, $\nu$ is the marginal variance, $\tau^2$ is the nugget, and $\theta_1, \dots, \theta_d$ are the lengthscale parameters. I will write 
$\phi$ to indicate the collection of all of these hyperparameters. 
Standard GP MLE proceeds by maximizing the marginal log-likelihood
\begin{align*}
\log p(y|X, \phi) &= -\frac{N}{2}  \log(2\pi) - \frac{N}{2}\log(\nu) - \frac{1}{2} \log \det(C) - \frac{1}{2\nu} y^\top C^{-1}y
\end{align*}
I denote the predictive distribution (the GP prior conditioned on observed data $(X, y)$) as 
\begin{align*}
y^*(\cdot) := y(\cdot)|X, y \sim \mathcal{GP}(\mu^*(\cdot), k^*(\cdot, \cdot)) 
\end{align*}

\subsection{Constrained Optimization}
Pensoneault et al (2020) propose a probabalistic constraint in the MLE optimization. 
\begin{align*}
\max \log p(y|X, \phi) \text{ s.t. } \Prob(y^*(x) < 0) \leq \eta \ \forall x
\end{align*}
where $\eta$ is fixed at some small value. Since the distribution of $y^*(x)$ is Gaussian, then the constraint can be re-written as 
\begin{align*}
\Phi\left(-\frac{\mu^*(x)}{\sqrt{k^*(x)}} \right) \leq \eta
\end{align*}
where $\Phi(\cdot)$ is the standard normal distribution function. This can then be re-written as 
\begin{align*}
\mu^*(x) + \sqrt{k^*(x)} \Phi^{-1}(\eta) \geq 0
\end{align*}
Note that this is a functional constraint that must hold for all $x$. In practice, the authors propose enforcing the constraint on a fixed set of 
control points $X_c = \{x^c_m\}_{m = 1}^{M}$. They also propose adding an additional constraint on the set of training points $X$. 
\begin{align*}
\abs{y_n - \mu^*(x_n)} \leq \epsilon \ \forall n = 1, \dots, N
\end{align*}
I don't completely understand why this is needed, but I guess it helps to stabilize the optimization algorithm in some sense. In any case, the final 
constrained optimization problem looks like. 
\begin{align*}
\max &\log p(y|X, \phi) \\
\text{s.t. } &0 \leq \mu^*(x^c_m) + \sqrt{k^*(x^c_m)} \Phi^{-1}(\eta) &&\forall m = 1, \dots M \\
&0 \leq \epsilon - \abs{y_n - \mu^*(x_n)} &&\forall n = 1, \dots, N
\end{align*}




\bibliography{first_paper} 
\bibliographystyle{ieeetr}


\section{References}
\begin{itemize}
\item Emulating dynamic non-linear simulators using Gaussian processes (Mohammadi et al, 2019)
\item Mechanism-based emulation of dynamic simulation models: Concept and application in hydrology (Reichert et al, 2016)
\item Gaussian process emulation of dynamic computer codes (Conti et al, 2009) (Resource for one-step-ahead emulator)
\item Evolving Bayesian emulators for structured chaotic time series, with application to large climate models. (Williamson and Blaker, 2014)
\item A dynamic modelling strategy for Bayesian computer model emulation (Liu and West, 2009)
\item Emulating complex dynamical simulators with random Fourier features (Mohammadi et al, 2023)
\item Sequential adaptive design for emulating costly computer codes (Mohammadi et al, 2022)
\item Emulation-accelerated Hamiltonian Monte Carlo algorithms for parameter estimation and uncertainty quantification in differential equation models (Paun and Husmeier, 2021)
\item Sequential design of computer exper- iments for the estimation of a probability of failure (Bect et al, 2012)
\item Kriging Is Well-Suited to Parallelize Optimization (Ginsbourger et al, 2010)
\end{itemize}



\end{document}




