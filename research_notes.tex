\documentclass[12pt]{article}
\RequirePackage[l2tabu, orthodox]{nag}
\usepackage[main=english]{babel}
\usepackage[rm={lining,tabular},sf={lining,tabular},tt={lining,tabular,monowidth}]{cfr-lm}
\usepackage{amsthm,amssymb,latexsym,gensymb,mathtools,mathrsfs}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{epstopdf,enumitem,microtype,dcolumn,booktabs,hyperref,url,fancyhdr}
\usepackage{algorithmic}
\usepackage[ruled,vlined,commentsnumbered,titlenotnumbered]{algorithm2e}

% Plotting
\usepackage{pgfplots}
\usepackage{xinttools} % for the \xintFor***
\usepgfplotslibrary{fillbetween}
\pgfplotsset{compat=1.8}
\usepackage{tikz}

% Custom Commands
\newcommand*{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand*{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand*{\suchthat}{\,\mathrel{\big|}\,}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\Ker}{\mathrm{Ker}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Prob}{\mathbb{P}}
\DeclarePairedDelimiterX\innerp[2]{(}{)}{#1\delimsize\vert\mathopen{}#2}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}

\setlist{topsep=1ex,parsep=1ex,itemsep=0ex}
\setlist[1]{leftmargin=\parindent}
\setlist[enumerate,1]{label=\arabic*.,ref=\arabic*}
\setlist[enumerate,2]{label=(\alph*),ref=(\alph*)}

% For embedding images
\graphicspath{ {./images/} }

% Specifically for paper formatting 
\renewcommand{\baselinestretch}{1.2} % Spaces manuscript for easy reading

% Formatting definitions, propositions, etc. 
\newtheorem{definition}{Definition}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{corollary}{Corollary}

% Title and author
\title{Research Notes: Ecological Modeling}
\author{Andrew Roberts}

\begin{document}

\maketitle
\tableofcontents
\newpage

\section{11/11/2022}

\subsection{Summary of Current Model}
\begin{itemize}
\item Notation: Calibration inputs $\theta$, computer model $f$, field data $y$.
\item Computer model: 
	\begin{itemize}
	\item Inputs: $\theta \in \R^{d_\theta}$, $n$ time steps defining the time range. 
	\item Outputs: $p$ time series, each of length $n$. Each time series corresponds to a constraint/output variable (e.g. NEE, LEE). 
	\item Option 1: think of $f(\theta)$ as mapping from $\R^{d_\theta}$ to $\R^{n \times p}$ (highly multi-output simulator).
	\item Option 2: think of $f(t, \theta)$ as mapping from $\{1, \dots, n\} \times \R^{d_\theta}$ to $\R^p$ (time-input, multi-output simulator).
	\item Option 3: think of $f(t, j, \theta)$ as mapping from $\{1, \dots, n\} \times \{1, \dots, p\} \times \R^{d_\theta}$ to $\R$ (time-variable input, univariate output simulator). 
	\item Option 4: $z_{t + 1} = f(z_t)$ and $f$ is independent of $t$, then we can emulate this operator $f$ which advances the system forward one time step.
	\item Likelihood emulation very beneficial for dimensionality reduction in this very high-dimensional output setting. Option 4 may be another promising option to explore in the future. 
	\item Let $Y \in \R^{n \times p}$ denote the field data corresponding to the computer outputs $f(\theta)$.
	\end{itemize}
\item Statistical model:
	\begin{itemize}
	\item Currently assuming independence between output variables, and across time within each output variable. For Gaussian likelihoods this means,
		\begin{align}
		p(Y|\Sigma_\epsilon, \theta) &= \prod_{t = 1}^{n} \prod_{j = 1}^{p} N(y_{tj}|f(t, j, \theta), \sigma^2_{\epsilon_j}) \label{ind_lik}
		\end{align}
	\end{itemize}
\item Emulator:
	\begin{itemize}
	\item The likelihood [\ref{ind_lik}] implies a log-likelihood
	\begin{align}
	\log p(Y|\Sigma_\epsilon, \theta) &= -\frac{np}{2} \log(2\pi \sigma^2_{\epsilon_j}) - \frac{1}{2} \sum_{j = 1}^{p} \frac{1}{\sigma^2_{\epsilon_j}} \sum_{t = 1}^{n} (y_{tj} - f(t, j, \theta))^2
	\end{align}
	\item Current model fits $p$ independent GP emulators to the functions
	\begin{align}
	T_j(\theta) &= \sum_{t = 1}^{n} (y_{tj} - f(t, j, \theta))^2, \ j = 1, \dots, p
	\end{align} 
	\item Replaces problem of fitting one $np$-output emulator with that of fitting $p$ univariate emulators. 
	\item The likelihood assumptions and choice of emulation target decouples the observation noise variances from the functions being emulated, allowing inference over both the variance parameters
	and $\theta$. Let $T_j^*$ denote the fit GP emulators. 
	\item Using stochastic approximation for log-likelihood 
	\begin{align}
	\ell_{T^*}(\theta) &:= -\frac{np}{2} \log(2\pi \sigma^2_{\epsilon_j}) - \frac{1}{2} \sum_{j = 1}^{p} \frac{1}{\sigma^2_{\epsilon_j}} T_j^*(\theta)
	\end{align}
	where the randomness stems from $T_j^*(\theta) \sim N(\mu^*_{T_j}(\theta), k^*_{T_j}(\theta))$. When a log-likelihood evaluation is required (e.g. in MCMC) then samples are produced from the $T_j^*$.
	\end{itemize}
\end{itemize}

\subsection{Low-cost Improvements}
\begin{itemize}
\item Emulate $L_j(\theta) := \log T_j(\theta)$ instead of the $T_j$ directly in order to enforce positivity on the emulated residual sum of squares. 
\item In MCMC, jointly sample $[T_j^*(\theta), T_j^*(\theta^\prime)]$ using GP covariance, where $\theta$ and $\theta^\prime$ are current and proposed parameter values, respectively. 
\item Fix bug in code that re-samples the precision parameter in MCMC when it should be fixed. 
\item Modularize code so that the GP emulator fit and the MCMC for parameter calibration are separate. Add in model checking to ensure that GP fit is good. 
\item Very basic discrepancy analysis: fit GP to residuals (after calibration) for 1.) model checking, and 2.) potential of bias-corrected prediction
\end{itemize}

\subsection{Mid-cost Improvements}
\begin{itemize}
\item Thinking more deeply about the kernel assumptions, e.g. if emulating likelihood, we may have prior knowledge that can be incorporated into the GP prior. Especially important if extrapolation is required.
\item Investigate correlation between output variables. Modify likelihood as needed, e.g.,
	\begin{align}
	p(Y|\Sigma_\epsilon, \theta) &= \prod_{t = 1}^{n} N_p(y_{t}|f(t, \theta), \Sigma_{\epsilon}) \label{ind_output_corr}
	\end{align}
\item Investigate correlation across time. Modify likelihood as needed, e.g., 
	\begin{align}
	p(Y|\Sigma_\epsilon, \theta) &= \prod_{j = 1}^{p} \prod_{t = 1}^{n} N\left(y_{tj}|\beta_0 + \beta_1 f(t, j, \theta) + \beta_2 y_{t-1,j} + \beta_3 \sin\left(\frac{2\pi}{365}t\right) + \beta_4 \cos\left(\frac{2\pi}{365}t\right), \sigma^2_{\epsilon_j}\right) \label{ind_autocorr}
	\end{align}
\end{itemize}

\subsection{High-cost Improvements}
\begin{itemize}
\item Exploring non-stationarity. 
\item More sophisticated model discrepancy approaches; e.g. incorporating prior information in attempt to simultaneously estimate $\theta$ and discrepancy. Particularly important if emphasis is placed on finding 
the true value of $\theta$, as opposed to simply wanting good predictions. 
\item Alternative to likelihood emulation--time-input/dynamic emulation, e.g., if simulator produces time series $\{z_t\}$ where $z_{t + 1} = f(z_t)$ and $f$ is independent of $t$, then we can emulate this operator $f$ 
which advances the system forward one time step.
\item Combining parameter calibration with state estimation.  
\item Extend hierarchical multi-site calibration to explicitly model spatial structure, e.g., for a single output ($p=1$), 
	\begin{align}
	y_{st} &\overset{iid}{\sim} N(f(t, \theta(s)), \sigma^2_\epsilon) \\
	\theta &\sim \mathcal{GP}(\mu_\theta, k_\theta)
	\end{align}
\item More sophisticated emulation in spatial setting; sharing information across space, etc. 
\end{itemize}

\end{document}








