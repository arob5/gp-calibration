\documentclass[12pt]{article}
\RequirePackage[l2tabu, orthodox]{nag}
\usepackage[main=english]{babel}
\usepackage[rm={lining,tabular},sf={lining,tabular},tt={lining,tabular,monowidth}]{cfr-lm}
\usepackage{amsthm,amssymb,latexsym,gensymb,mathtools,mathrsfs}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{epstopdf,enumitem,microtype,dcolumn,booktabs,hyperref,url,fancyhdr}
\usepackage[margin=0.5in]{geometry}

% Plotting
\usepackage{pgfplots}
\usepackage{xinttools} % for the \xintFor***
\usepgfplotslibrary{fillbetween}
\pgfplotsset{compat=1.8}
\usepackage{tikz}

% Custom Commands
\newcommand*{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand*{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand*{\suchthat}{\,\mathrel{\big|}\,}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\Ker}{\mathrm{Ker}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Cor}{\mathrm{Corr}}
\newcommand{\Prob}{\mathbb{P}}
\DeclarePairedDelimiterX\innerp[2]{(}{)}{#1\delimsize\vert\mathopen{}#2}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\def\R{\mathbb{R}}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\setlist{topsep=1ex,parsep=1ex,itemsep=0ex}
\setlist[1]{leftmargin=\parindent}
\setlist[enumerate,1]{label=\arabic*.,ref=\arabic*}
\setlist[enumerate,2]{label=(\alph*),ref=(\alph*)}

% Specifically for paper formatting 
\renewcommand{\baselinestretch}{1.2} % Spaces manuscript for easy reading

% Formatting definitions, propositions, etc. 
\newtheorem{definition}{Definition}
\newtheorem{condition}{Condition}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{notation}{Notation}

\begin{document}

\begin{center}
Scalable Bayesian Methods for Scientific Machine Learning
\end{center}

\begin{flushright}
Andrew Roberts
\end{flushright} 

The effectiveness of combining physics and simulation-based models with statistical and machine learning methods is increasingly being recognized \cite{Willcox}. Physical models
encapsulate prior scientific knowledge, while a Bayesian statistical framework allows for principled inference, prediction, and uncertainty quantification. This interplay between domain 
knowledge and statistical inference is essential for advancing scientific theory and informing high-consequence decision-making. My research goal is to develop novel methodological 
and computational approaches for model calibration, prediction, and uncertainty quantification for scientifically-informed modeling of complex systems. 

I will focus on the setting of a deterministic computer model $f: \R^d \to \R$ as a function of some unknown parameters $\theta \in \R^d$. [insert example here] In general, I will allow
for these models to depend on additional covariates and produce multiple outputs (i.e. $f: (x, \theta) \to \R^k$), but I omit these complexities here for notational convenience. Suppose 
we obtain observational data $y$ corresponding to the quantity that $f$ tries to predict; that is, we want $f(\theta) \approx y$. This is a standard \textit{inverse problem}, in which
the un-observed quantity $\theta$ must be inferred from the observed data $y$. Typically, $f$ is highly non-linear, non-invertible, and direct numerical solution to this equation may be extremely sensitive
to small perturbations of $y$ (i.e. this problem is ill-posed). It is therefore typically preferable to re-frame the inverse problem in a Bayesian statistical setting, as in the simple Gaussian error model below. 
\begin{align*}
y_i &= f(\theta) + \epsilon_i \\
\epsilon_i &\overset{iid}{\sim} N(0, \sigma^2) \\
(\theta, \sigma^2) &\sim \pi_0 \\
\end{align*}
Here, the prior distribution $\pi_0$ encodes domain knowledge about the unknown calibration parameters $\theta$ and statistical parameter $\sigma^2$, and we suppose the observed data is given by 
$y = (y_1, \dots, y_n)^T$. ``Solving'' this inverse problem is now interpreted as finding the joint posterior distribution
\[\pi(\theta, \tau|y) \propto N(y|f(\theta), \sigma^2 I_n)\pi_0(\theta, \sigma^2)\]
or the marginal posterior $\pi(\theta|y)$, which can be obtained by marginalizing over $\sigma^2$. My research will focus on characteristics of this setup commonly found in scientific applications: 1.) $f(\theta)$ is
costly to compute, 2.) uncertainty quantification is essential, and 3.) the underlying process of interest exhibits significant spatial and/or temporal variation. The first constraint typically precludes iterative inference 
procedures like Markov Chain Monte Carlo (MCMC), instead demanding approximation techniques such as statistical emulation. A common approach is to fit a Gaussian process emulator 
$\hat{f}(\cdot) \sim \mathcal{GP}(m(\cdot), k(\cdot, \cdot))$ to approximate the true model $f$, and then using $\hat{f}$ in place of $f$ in the inference algorithm. The second point implies that optimizing $\theta$ is not enough; informed  decision-making based on the model requires summarizing the distribution of possible parameter values, which should incorporate all possible sources of uncertainty. Finally, while the above statistical model is static, the physical phenomenon of interest often exhibits spatiotemporal variation, which should be included in the model if one hopes to draw general scientific conclusions and make predictions 
across unobserved space and time. \\

\noindent
\textbf{Research Plan.} To address these problems, I will begin by developing baseline methods for incorporating emulator uncertainty into the statistical inference algorithm in order to achieve more ``honest''
uncertainty quantification. I will then generalize the statistical model to explicitly incorporate spatial information, seeking to improve predictive power and allowing for uncertainty to be analyzed across
the spatial dimension. Finally, I will generalize to a full spatiotemporal model. These modeling steps will pose increasingly difficult computational challenges, which I intend to address by developing 
approximation and dimensionality reduction techniques. Throughout this process, I will test these methods on terrestrial carbon models commonly used in ecological forecasting. 
 
 \textbf{1. Propagating Emulator Uncertainty.} I will begin by exploring the ramifications of approximating the computationally expensive model $f$ with the  cheaper Gaussian process emulator $\hat{f}$. 
 The emulation introduces an additional source of uncertainty that is often ignored or accounted for in an ad hoc manner [cite]. 
For example, the naive method is to modify the inference algorithm to replace an evaluation of $f(\theta)$ with the Gaussian process mean $m(\theta)$. However, this fails to account
 for the uncertainty introduced by the emulator approximation, resulting in overly confident posterior estimates. This begs the question: what is the ``correct'' way to propagate the emulator uncertainty through 
 the inference procedure? A few entirely reasonable options are to: 1.) replace the original inverse problem $y = f(\theta) + \epsilon$ with the modified version $y = f(\theta) + \eta_{\text{GP}}(\theta) + \epsilon$, 
 where $\eta_{\text{GP}} \sim N(0, k(\theta, \theta))$ \cite{Cleary}, 2.) marginalize the approximate likelihood $N(\hat{f}(\theta), \sigma^2)$ over $\hat{f}(\theta)$, or 3.) sample from the Gaussian process 
 $f_\theta \sim \mathcal{GP}(m(\theta), k(\theta, \theta))$ and use the sample $f_\theta$ in place of $\theta$ in the inference algorithm \cite{Fer}. 
 
While all seemingly reasonable, these approaches have subtle differences and may only be suitable with certain inference algorithms. Motivated by the fact that well-calibrated uncertainty estimates are essential in applications, I will conduct a rigorous of what constitutes ``honest'' uncertainty quantification in this setting and develop scalable algorithms that correctly propagate emulator uncertainty. 
 
 \textbf{2. Spatial Variability} With methodologically sound and scalable algorithms in hand for the baseline static model, I will then be in a position to begin investigating spatially varying generalizations. 
 In the terrestrial carbon modeling settings, existing studies typically focus on specific sites at which ecological data are collected [cite]. In theory, if the parameters $\theta$ correspond to universal physical 
 processes then the site location should not play a significant role. In practice, this is far from the truth; the underlying physical processes are not fully understood and thus variability in the estimated values
 and uncertainty of $\theta$ across space may reveal gaps in scientific knowledge and lead to future directions for research. The power of hierarchical Bayesian modeling has recently been recognized in 
 formally studying this spatial variation (e.g. \cite{Fer2}). We can extend the simple Gaussian model to consider data collected at different spatial locations $j = 1, \dots, k$:
 \begin{align*}
 &y^{(j)}_i|\theta_j, \sigma_j^2 \sim N(f(\theta_j), \sigma_j^2) \\
 &\theta_j \sim N(\mu, \Sigma) \\
 &\sigma_j^2 \sim N(\gamma, \tau^2) \\
 &(\mu, \Sigma, \gamma, \tau^2) \sim \pi_0
 \end{align*}
 This constitutes a first step towards understanding the spatial variability in $\theta$ and $\sigma^2$, but doesn't incorporate any explicit spatial information such as a concept of ``nearness'' between the sites. 
 
 
 
 

 
 \textbf{TODO}:
 \begin{itemize}
 \item I may want to give more motivation as to why uncertainty quantification is such a priority here, maybe through specific applied examples. 
 \item Similarly with the spatiotemporal stuff - I should probably move all of the motivation in the intro section, and then focus only on the concrete proposals in the research plan section. 
 \end{itemize}



\begin{thebibliography}{20}
\bibitem{Willcox} Willcox, K.E., Ghattas, O. \& Heimbach, P. The imperative of physics-based modeling and inverse theory in computational science. Nat Comput Sci 1, 166–168 (2021). https://doi.org/10.1038/s43588-021-00040-z
\bibitem{Cleary} Emmet Cleary, Alfredo Garbuno-Inigo, Shiwei Lan, Tapio Schneider, Andrew M. Stuart, “Calibrate, emulate, sample”, Journal of Computational Physics, Volume 424, 2021, 109716, ISSN 0021-9991, https://doi.org/10.1016/j.jcp.2020.109716.
\bibitem{Fer} Fer, I., Kelly, R., Moorcroft, P. R., Richardson, A. D., Cowdery, E. M., and Dietze, M. C.: Linking big models to big data: efficient ecosystem model calibration through Bayesian model emulation, Biogeosciences, 15, 5801–5830, https://doi.org/10.5194/bg-15-5801-2018, 2018.
\bibitem{Fer2} Istem Fer, Alexey Shiklomanov, Kimberly A. Novick, Christopher M. Gough, M. Altaf Arain, Jiquan Chen, Bailey Murphy, Ankur R. Desai, Michael C. Dietze: Capturing site-to-site variability through Hierarchical Bayesian calibration of a process-based dynamic vegetation model, bioRxiv 2021.04.28.441243; doi: https://doi.org/10.1101/2021.04.28.441243. 

\end{thebibliography}



\end{document}



