\documentclass[12pt]{article}
\RequirePackage[l2tabu, orthodox]{nag}
\usepackage[main=english]{babel}
\usepackage[rm={lining,tabular},sf={lining,tabular},tt={lining,tabular,monowidth}]{cfr-lm}
\usepackage{amsthm,amssymb,latexsym,gensymb,mathtools,mathrsfs}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{epstopdf,enumitem,microtype,dcolumn,booktabs,hyperref,url,fancyhdr}
\usepackage[margin=0.5in]{geometry}

% Plotting
\usepackage{pgfplots}
\usepackage{xinttools} % for the \xintFor***
\usepgfplotslibrary{fillbetween}
\pgfplotsset{compat=1.8}
\usepackage{tikz}

% Custom Commands
\newcommand*{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand*{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand*{\suchthat}{\,\mathrel{\big|}\,}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\Ker}{\mathrm{Ker}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Cor}{\mathrm{Corr}}
\newcommand{\Prob}{\mathbb{P}}
\DeclarePairedDelimiterX\innerp[2]{(}{)}{#1\delimsize\vert\mathopen{}#2}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\def\R{\mathbb{R}}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\setlist{topsep=1ex,parsep=1ex,itemsep=0ex}
\setlist[1]{leftmargin=\parindent}
\setlist[enumerate,1]{label=\arabic*.,ref=\arabic*}
\setlist[enumerate,2]{label=(\alph*),ref=(\alph*)}

% Specifically for paper formatting 
\renewcommand{\baselinestretch}{1.2} % Spaces manuscript for easy reading

% Formatting definitions, propositions, etc. 
\newtheorem{definition}{Definition}
\newtheorem{condition}{Condition}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{notation}{Notation}

\begin{document}

\begin{center}
Scalable Bayesian Methods for Scientific Machine Learning
\end{center}

\begin{flushright}
Andrew Roberts
\end{flushright} 

The effectiveness of formally combining scientific knowledge with statistical and machine learning methods is increasingly being recognized \cite{Willcox}. Physical models
encapsulate prior scientific knowledge, while a Bayesian statistical framework allows for principled inference, prediction, and uncertainty quantification. This interplay between domain 
knowledge and statistical inference is essential for advancing scientific theory and informing high-consequence decision-making. My research goal is to develop novel methodological 
and computational approaches for model calibration, prediction, and uncertainty quantification for scientifically-informed modeling of complex systems. 

Complex computer models have become essential tools for studying complex phenomena in a wide variety of scientific and engineering disciplines, 
tackling problems ranging from climate projections [cite] to pandemic response [cite]. My proposed research program will contribute methodological 
advances for calibrating, predicting, and quantifying uncertainty using such models. Moreover, this research will support two 
overarching objectives: (i.) improving data-informed decision-making under uncertainty and (ii.) advancing scientific discovery and research. To define the 
setting of interest, let $f: \theta \mapsto \R$ denote the computer model (i.e. a simulator or process model) as a function of some unknown and unobserved
parameters $\theta \in \R^d$. As a motivating example, consider the problem of modeling the terrestrial component of the carbon cycle \cite{Friedlingstein}. 
In this case, $f$ is a process model that predicts the net exchange of carbon between an ecosystem and the atmosphere (\cite{Waring}) and $\theta$ may encompass 
unknown ecological parameters such as the soil respiration rate or seasonal leaf growth \cite{Fer}. The task is then to confront these models with observational data $y$ in 
order to calibrate (i.e. estimate) the unknown parameters $\theta$, utilize the calibrated model for prediction, and carefully track sources of uncertainty throughout the analysis. 

These tasks may be complicated by several features commonly found in real-world applications. First, the runtime of the computer model $f$ may be prohibitively long, 
precluding the tens of thousands of evaluations typically required by the Monte Carlo approaches used for calibration and uncertainty quantification. Second, the 
underlying process being modeled by $f$ often exhibits complex dynamics, and attempts to explicitly model these dynamics lead to even more computationally challenging
models. Terrestrial carbon models have both of these features; sophisticated models often require many hours to run a single execution \cite{Fer} and the ecological 
parameters $\theta$ commonly exhibit spatiotemporal variation. 

[carbon models exhibit both] [balance that must be struck]


These 
tasks are complicated when the underlying process being modeled is dynamic and complex, \textbf{an example being} the spatiotemporal dynamics exhibited by terrestrial carbon fluxes.
In order to accommodate complexities and uncertainties such as these, the process model is typically embedded within a hierarchical Bayesian framework \cite{Clark}. As a basic starting 
point, one might consider the following model. 
\begin{align*}
y|\theta \sim N(f(\theta), \sigma^2) \\
\theta|\mu_{\theta}, \Sigma_{\theta} \sim N(\mu_{\theta}, \Sigma_{\theta}) \\
(\theta, \sigma^2, \mu_{\theta}, \Sigma_{\theta}) \sim \pi_0
\end{align*}
This is a powerful formulation in that it accounts for distinct sources of uncertainty in a principled, probabilistic manner: the first line models the difference between the 
observed data $y$ and the process model output $f(\theta)$ as Gaussian white noise. The second line allows for explicit modeling of variation in the 
parameters $\theta$, which can account for the spatiotemporal dynamics. Finally, the prior distribution $\pi_0$ encodes prior domain knowledge about the values and uncertainty of the process parameters $\theta$ and data parameters 
$\sigma^2, \mu_\theta, \Sigma_{\theta}$. All of this statistical structure is built around the scientifically-informed model $f$, which incorporates domain knowledge of the physical 
process itself. To avoid confusion, I henceforth refer to $f$ and $\theta$ as the 
\textit{process} model and parameters, respectively, to avoid confusion with the statistical model and parameters (e.g. $\sigma^2$, $\mu_\theta$).  
The above formulation can be interpreted as a \textit{Bayesian inverse problem}, in which the un-observed quantity $\theta$ is inferred using the observed data $y$ and prior 
beliefs $\pi_0$. ``Solving'' this inverse problem is interpreted as finding the posterior distribution of interest, either the full joint distribution $p(\theta, \sigma^2, \mu_\theta, \Sigma_\theta|y)$, 
the marginal $p(\theta|y)$, or anything in between. 

While in theory this inverse problem can be solved by inference algorithms like Markov Chain Monte Carlo (MCMC), the use of such iterative algorithms requires running the 
process model $f$ at each iteration. This means MCMC is infeasible in settings where $f$ is very costly to execute, as is the case with certain terrestrial carbon models which can take
many hours to run a single time \cite{Fer}. There is a wide body of literature dedicated to approximation techniques to circumvent this problem, with particular emphasis on the 
flexible approach of fitting a Gaussian process emulator $\hat{f}(\cdot) \sim \mathcal{GP}(m(\cdot), k(\cdot, \cdot))$ to approximate the true model $f$, and then using $\hat{f}$
in place of $f$ in the inference algorithm [cite].

I have detailed two  




 While this approximation is often necessary, the emulator introduces an additional source of uncertainty that is often ignored or accounted for in an ad hoc manner [cite]. My research will emphasize
 principled uncertainty quantification such that all quantifiable sources of uncertainty are meaningfully reflected in the posterior distribution. This is essential in many applications
 where these models are used for forward-looking decision-making, as uncertainty in model estimates and predictions must be a key part of the decision-making process \cite{Dietze}.
 Moreover, a thorough accounting of uncertainty has the potential to advance scientific theory by identifying missing or under-calibrated processes in $f$. 
 
 Second, I plan to address shortcomings of the static statistical model above by allowing parameters (potentially both process and data parameters) to vary across space 
 and time. In practice, the underlying physical process is often dynamic, not fully understood, and exhibits spatio-temporal variability. The static model thus 
 represents a convenient approximation, but this significantly limits its generalizability beyond observed data. The development of novel methodological and computational 
 methods to handle more complex dynamic models will increase generalizability and advance understanding of complex processes that vary across the spatial and 
 temporal dimensons. 

\noindent
\textbf{Research Plan.} To address these problems, I will begin by developing baseline methods for incorporating emulator uncertainty into the statistical inference algorithm in order to achieve more ``honest'' uncertainty quantification. I will then generalize the statistical model to explicitly incorporate spatial information, seeking to improve predictive power and allowing for spatially-indexed uncertainty estimates. Finally, I will generalize to a full spatiotemporal model. These modeling steps will pose increasingly difficult computational challenges, which I intend to address by developing approximation and dimensionality reduction techniques. Throughout this process, I will test these methods on terrestrial carbon models commonly used in ecological forecasting \cite{Dietze}. 
 
 \textbf{1. Propagating Emulator Uncertainty.} I will begin by exploring the ramifications of approximating the computationally expensive model $f$ with the  cheaper Gaussian process emulator $\hat{f}$. Naive emulation would simply use the Gaussian process mean $m(\theta)$ in place of the process model evaluation $f(\theta)$. However, this fails to account
 for the uncertainty introduced by the emulator approximation, resulting in overly confident posterior estimates. This begs the question: what is the ``correct'' way to propagate the emulator uncertainty through 
 the inference procedure? A few entirely reasonable options are to: 1.) replace the original inverse problem $y = f(\theta) + \epsilon$ with the modified version $y = f(\theta) + \eta_{\text{GP}}(\theta) + \epsilon$, 
 where $\eta_{\text{GP}} \sim N(0, k(\theta, \theta))$ \cite{Cleary}, 2.) marginalize the approximate likelihood $N(\hat{f}(\theta), \sigma^2)$ over $\hat{f}(\theta)$, or 3.) sample from the Gaussian process 
 $f_\theta \sim \mathcal{GP}(m(\theta), k(\theta, \theta))$ and use the sample $f_\theta$ in place of $f(\theta)$ in the inference algorithm \cite{Fer}. 
 
While all seemingly reasonable, these approaches have subtle differences and may only be suitable with certain inference algorithms. Motivated by the fact that well-calibrated uncertainty estimates are essential in applications, I will conduct a rigorous study of what constitutes ``honest'' uncertainty quantification in this setting and develop scalable algorithms that correctly propagate emulator uncertainty. 
 
 \textbf{2. Spatial Variability} With methodologically sound and scalable algorithms in hand for the baseline static model, I will then be in a position to begin investigating spatially varying generalizations. I will approach this problem by extending the Bayesian inverse problem to the full hierarchical model,
\begin{align*}
&y \sim N(f(\theta), \sigma^2 I_n) \\
&\theta \sim N(\mu_\theta, \Sigma_\theta) \\
&(\mu_\theta, \Sigma_\theta, \sigma^2) \sim \pi_0
\end{align*}
noting that this could be augmented to include hierarchical structure on $\sigma^2$ and to consider distributions other than Gaussian. As an example, recent studies in the terrestrial 
carbon monitoring literature have taken $\Sigma_\theta = \sigma^2_\theta I$, allowing process parameters $\theta$ to exhibit Gaussian variability across different ecological data collection
sites \cite{Fer2}. This constitutes a first step towards understanding the spatial variability in $\theta$ and $\sigma^2$, but doesn't incorporate any explicit spatial information such as a concept of ``nearness'' between the sites. I will extend this model to explicitly model $\theta$ as a spatial process, for example, by placing a Gaussian process prior on $\theta$ (that is, adopting the 
\textit{kriging} model common in geostatistics). This hierarchical model is straightforward to define
 but complicated by the fact that $f$ must be emulated and its uncertainty quantified, data parameters like $\sigma^2$ may require spatial process priors as well,
and the introduction of new parameters makes inference more difficult in the resulting higher-dimensional space.  I will draw on the baseline research conducted in part one to extend the principled uncertainty quantification and inference scheme to this spatial setting, developing the necessary computational approximations in order to calibrate and predict with this model. 
 
  \textbf{3. Complete Spatio-temporal Model}
   The natural next step is to extend the model to incorporate temporal variation. The general data model I will consider is 
  \[y(s, t) = f(\theta(s, t)) + \epsilon(s, t)\]
  where observed data $y$, process parameters $\theta$, and error $\epsilon$ are all modeled as spatio-temporal processes. 
  To avoid the limitations associated with estimating space-time covariance matrices, I will adopt the modern dynamic model approach \cite{Arab}. As a simple example, 
  we can let $\theta_t := (\theta_t (s_1), \dots, \theta_t (s_m))$ be a spatial process at known locations $s_1, \dots, s_m$ at time $t$ and assume this process is updated
  via the Markovian linear model
  \[\theta_t = T_{\gamma_t} \theta_{t - 1} + \epsilon_t, \text{ where } \epsilon_t \sim N(0, \Sigma_\epsilon)\]
  where $T_{\gamma_t}$ is a transition matrix that potentially depends on some time-varying parameters $\gamma_t$ \cite{Arab}. Clearly, even this simple model leads to a 
  much higher-dimensional parameter space and its associated computational challenges. I will use this model as a starting point, develop methods for more
  complicated generalizations, and extend my previous work to develop efficient methods of quantifying uncertainty across the spatial and temporal dimensions. 
  I will develop dimensionality reduction and approximation methods that draw both from Bayesian inverse problem theory (e.g. \cite{Kugler}), Bayesian filtering \cite{Sarkka},
  and recent advances in dynamic spatiotemporal modeling (e.g. \cite{Hefley}). 

\begin{thebibliography}{20}
\bibitem{Willcox} Willcox, K.E., Ghattas, O. \& Heimbach, P. The imperative of physics-based modeling and inverse theory in computational science. Nat Comput Sci 1, 166–168 (2021). https://doi.org/10.1038/s43588-021-00040-z
\bibitem{Cayelan} Cayelan C. Carey, Whitney M. Woelmer, Mary E. Lofton, Renato J. Figueiredo, Bethany J. Bookout, Rachel S. Corrigan, Vahid Daneshmand, Alexandria G. Hounshell, Dexter W. Howard, Abigail S. L. Lewis et al (2022) Advancing lake and reservoir water quality management with near-term, iterative ecological forecasting, Inland Waters, 12:1, 107-120, DOI: 10.1080/20442041.2020.1816421
\bibitem{Cleary} Emmet Cleary, Alfredo Garbuno-Inigo, Shiwei Lan, Tapio Schneider, Andrew M. Stuart, “Calibrate, emulate, sample”, Journal of Computational Physics, Volume 424, 2021, 109716, ISSN 0021-9991, https://doi.org/10.1016/j.jcp.2020.109716.
\bibitem{Dietze} Dietze et al, “Iterative near-term ecological forecasting: Needs, opportunities, and challenges”, Proceedings of the National Academy of Sciences, 115, 7, 1424-1432, 2018.
\bibitem{Fer} Fer, I., Kelly, R., Moorcroft, P. R., Richardson, A. D., Cowdery, E. M., and Dietze, M. C.: Linking big models to big data: efficient ecosystem model calibration through Bayesian model emulation, Biogeosciences, 15, 5801–5830, https://doi.org/10.5194/bg-15-5801-2018, 2018.
\bibitem{Fer2} Istem Fer, Alexey Shiklomanov, Kimberly A. Novick, Christopher M. Gough, M. Altaf Arain, Jiquan Chen, Bailey Murphy, Ankur R. Desai, Michael C. Dietze: Capturing site-to-site variability through Hierarchical Bayesian calibration of a process-based dynamic vegetation model, bioRxiv 2021.04.28.441243; doi: https://doi.org/10.1101/2021.04.28.441243. 
\bibitem{Arab} Arab, Ali \& Hooten, Mevin \& Wikle, Christopher. (2007). Hierarchical Spatial Models. Encyclopedia of geographical information science.
\bibitem{Friedlingstein} Friedlingstein, et al: Global Carbon Budget 2021, Earth Syst. Sci. Data, 14, 1917–2005, https://doi.org/10.5194/essd-14-1917-2022, 2022.
\bibitem{Hefley} Trevor J. Hefley, Mevin B. Hooten, Ephraim M. Hanks, Robin E. Russell, Daniel P. Walsh, Dynamic spatio-temporal models for spatial data, Spatial Statistics.
\bibitem{Kugler} Benoit Kugler, Florence Forbes, Sylvain Douté. Fast Bayesian Inversion for high dimensional inverse problems. Statistics and Computing, Springer Verlag (Germany), In press. ffhal-02908364v3f
\bibitem{Sarkka} Särkkä, S. (2013). Bayesian Filtering and Smoothing (Institute of Mathematical Statistics Textbooks). Cambridge: Cambridge University Press. doi:10.1017/CBO9781139344203
\bibitem{Waring} Richard H. Waring, Steven W. Running, CHAPTER 10 - Advances in Eddy-Flux Analyses, Remote Sensing, and Evidence of Climate Change, Forest Ecosystems (Third Edition), Academic Press, 2007, Pages 317-344, ISBN 9780123706058, https://doi.org/10.1016/B978-012370605-8.50017-7.
\bibitem{Clark} Clark, James S. 2005. Why environmental scientists are becoming Bayesians. Ecology Letters, Vol. 8: 2-14

\end{thebibliography}



\end{document}



