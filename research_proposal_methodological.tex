\documentclass[12pt]{article}
\RequirePackage[l2tabu, orthodox]{nag}
\usepackage[main=english]{babel}
\usepackage[rm={lining,tabular},sf={lining,tabular},tt={lining,tabular,monowidth}]{cfr-lm}
\usepackage{amsthm,amssymb,latexsym,gensymb,mathtools,mathrsfs}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{epstopdf,enumitem,microtype,dcolumn,booktabs,hyperref,url,fancyhdr}
\usepackage[margin=0.5in]{geometry}

% Plotting
\usepackage{pgfplots}
\usepackage{xinttools} % for the \xintFor***
\usepgfplotslibrary{fillbetween}
\pgfplotsset{compat=1.8}
\usepackage{tikz}

% Custom Commands
\newcommand*{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand*{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand*{\suchthat}{\,\mathrel{\big|}\,}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\Ker}{\mathrm{Ker}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Cor}{\mathrm{Corr}}
\newcommand{\Prob}{\mathbb{P}}
\DeclarePairedDelimiterX\innerp[2]{(}{)}{#1\delimsize\vert\mathopen{}#2}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\def\R{\mathbb{R}}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\setlist{topsep=1ex,parsep=1ex,itemsep=0ex}
\setlist[1]{leftmargin=\parindent}
\setlist[enumerate,1]{label=\arabic*.,ref=\arabic*}
\setlist[enumerate,2]{label=(\alph*),ref=(\alph*)}

% Specifically for paper formatting 
\renewcommand{\baselinestretch}{1.2} % Spaces manuscript for easy reading

% Formatting definitions, propositions, etc. 
\newtheorem{definition}{Definition}
\newtheorem{condition}{Condition}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{notation}{Notation}

\begin{document}

\begin{center}
Scalable Bayesian Methods for Scientific Machine Learning
\end{center}

\begin{flushright}
Andrew Roberts
\end{flushright} 

The effectiveness of formally combining scientific knowledge with statistical and machine learning methods is increasingly being recognized \cite{Willcox, Laubmeier, Wikle}. Physical models
encapsulate prior scientific knowledge, while a Bayesian statistical framework allows for principled inference, prediction, and uncertainty quantification. This interplay between domain 
knowledge and statistical inference is essential for advancing scientific theory and informing high-consequence decision-making. My research goal is to develop novel methodological 
and computational approaches for model calibration, prediction, and uncertainty quantification for scientifically-informed modeling of complex systems. 

Complex computer models have become essential tools for studying complex phenomena in a wide variety of scientific and engineering disciplines, 
tackling problems ranging from climate projections [cite] to pandemic response [cite]. My proposed research program will contribute methodological 
advances for calibrating, predicting, and quantifying uncertainty using such models. Moreover, this research will support two 
overarching objectives: (i.) improving data-informed decision-making under uncertainty and (ii.) advancing scientific discovery and research. To define the 
setting of interest, let $f: \theta \mapsto \R^d$ denote the computer model (i.e. a simulator or process model) as a function of some unknown and unobserved
parameters $\theta \in \R^d$. As a motivating example, consider the problem of modeling the terrestrial component of the carbon cycle \cite{Friedlingstein}. 
In this case, $f$ is a process model that predicts the net exchange of carbon between an ecosystem and the atmosphere \cite{Waring} and $\theta$ may encompass 
unknown ecological parameters such as the soil respiration rate or seasonal leaf growth \cite{Fer}. The task is then to confront these models with observational data $y$ in 
order to calibrate (i.e. estimate) the unknown parameters $\theta$, utilize the calibrated model for prediction, and carefully track sources of uncertainty throughout the analysis. 

These tasks may be complicated by several features commonly found in real-world applications. First, the runtime of the computer model $f$ may be prohibitively long, 
precluding the tens of thousands of evaluations typically required by the Monte Carlo approaches used for calibration and uncertainty quantification. There is a wide body 
of literature dedicated to approximation techniques to circumvent this problem, with particular emphasis on the flexible approach of fitting a Gaussian process 
emulator $\hat{f}(\cdot) \sim \mathcal{GP}(m(\cdot), k(\cdot, \cdot))$ to approximate the true model $f$,and then using $\hat{f}$
in place of $f$ in the inference algorithm [cite]. While this approximation is often necessary, the emulator introduces an additional source of uncertainty that is often ignored 
or accounted for in an ad hoc manner [cite]. A second complication common in computer modeling is that the 
underlying process being modeled by $f$ often exhibits complex dynamics, and attempts to explicitly model these dynamics lead to even more computationally challenging
models. (need a sentence here)Terrestrial carbon models have both of these features; sophisticated simulations often require many hours to run a single execution \cite{Fer} and the ecological 
parameters $\theta$ commonly exhibit spatiotemporal variation [cite]. (include stochastic simulators as third challenge?) These problems share a common theme: the difficulty in striking 
a balance between realism and computational and statistical feasibility in the analysis of computer models. Often, one of these characteristics must be sacrificed in favor of the other [cite].
Given the ubiquity of computer models in informing evidence-based policy, it is essential to develop novel methods to handle more complex models while providing
an accurate accounting of uncertainties. 
Recent calls have highlighted the need for research that brings together state-of-the-art computational methods with modern statistical and machine learning models in order to address these problems [cite Wikle, Gramacy]. My proposed research addresses this need. 
[also should add why UQ is important: scientific discovery and data-driven decision making] [make more clear that I'm working on statistical models, not the process models themselves] [At least state the basic model: $y = f(\theta) + \epsilon$] [somewhere should state why it is important to generalize Gaussian error model]

\noindent
\textbf{Research Plan.} (update this intro) An emphasis throughout this research will be the principled quantification of uncertainty, which is made more difficult
as increasingly complex statistical models demand additional computational approximation techniques. Therefore, I will begin by addressing fundamental issues of uncertainty in standard static, deterministic models. Specifically, I will develop methods for incorporating emulator uncertainty into the statistical inference algorithm in order to achieve more ``honest'' uncertainty quantification. With this foundation established, I will then be in a position to integrate spatiotemporal dynamics into the statistical model, as well as consider the issue of stochastic simulators. These modeling steps will pose increasingly difficult computational challenges, which I intend to address by developing approximation and dimensionality reduction techniques. Throughout this process, I will test these methods on terrestrial carbon models commonly used in ecological forecasting \cite{Dietze}. 
 
 \textbf{1. Propagating Emulator Uncertainty.} I will begin by exploring the ramifications of approximating the computationally expensive model $f$ with the  cheaper Gaussian process emulator $\hat{f}$. Naive emulation would simply use the Gaussian process mean $m(\theta)$ in place of the process model evaluation $f(\theta)$. However, this fails to account
 for the uncertainty introduced by the emulator approximation, resulting in overly confident posterior estimates. This begs the question: what is the ``correct'' way to propagate the emulator uncertainty through the inference procedure? In the current literature, this question is either largely ignored or addressed in an ad hoc manner [cite]. Gaining a rigorous understanding of methods for propagating emulator uncertainty is essential to ensure that model predictions are not inaccurately presented as overly confident. Various papers have offered bespoke solutions to this problem \cite{Cleary, Fer}, but no comprehensive
methodological study has been conducted. Moreover, these existing methods consider only the Gaussian error model and treat emulator uncertainty in an independent fashion, failing to take advantage of the correlation in Gaussian process predictions. I will address these gaps in the literature by (i.) conducting a rigorous comparison of methods for propagating emulator uncertainty under the standard Gaussian noise model, (ii.) developing novel techniques which take advantage of the Gaussian process predictive correlation, and (iii.) generalize these methods to auto-regressive and Laplace error models, both of which 
are common in ecological applications \cite{Fer}. 

 \textbf{2. Spatio-temporal Dynamics}
With methodologically sound and scalable algorithms in hand for the baseline static framework, I will then be in a position to begin investigating spatiotemporal generalizations of the statistical model. Given the spatiotemporal nature of most physical processes, this is an important extension that is often ignored, presumably due to computational challenges (\cite{Fer2}, [another citation]). Parameters $\theta$ may naturally be thought to vary over space
and time; however, even if they are not then quantifying spatiotemporal variation in uncertainty can help identify 
missing processes in physical models, as well as provide a greater understanding as to the generalizability of the process model beyond observed data \cite{Fer2, Dietze} [cite transferability study?]. To address these issues I will extend the data model as 
  \[y(s, t) = f(\theta(s, t)) + \epsilon(s, t)\]
 where observed data $y$, process parameters $\theta$, and error $\epsilon$ are all modeled as spatiotemporal processes. Ad hoc solutions have been developed to deal with computer models with spatially or temporally indexed outputs ([cite]), but it is difficult to find literature that additionally considers an 
 explicit spatiotemporal model for the process parameters $\theta$ ([cite]). As an example, recent studies concerning terrestrial carbon models have 
 allowed process parameters $\theta$ to exhibit Gaussian variability across different ecological data collection
sites \cite{Fer2}. This constitutes a first step towards understanding the spatial variability in $\theta$, but doesn't incorporate any explicit spatial information such as a concept of ``nearness'' between the sites. Researchers in both the spatiotemporal statistics and computer modeling communities
have noted the potential for integrating methods in both fields using modern advances in machine learning and computation \cite{Wikle, Baker}. My proposed research will bridge this divide by (i.) developing novel techniques to integrate modern spatiotemporal dynamical models \cite{Wikle, Hefley}
with the computer modeling framework, (ii.) develop dimensionality reduction and approximation methods that draw from Bayesian inverse problem 
theory \cite{Kugler} and Bayesian filtering \cite{Sarkka}, and (iii.) extend my previous uncertainty quantification methodology to this more complex setting. 
 

\begin{thebibliography}{20}
\bibitem{Willcox} Willcox, K.E., Ghattas, O. \& Heimbach, P. The imperative of physics-based modeling and inverse theory in computational science. Nat Comput Sci 1, 166–168 (2021). https://doi.org/10.1038/s43588-021-00040-z
\bibitem{Cayelan} Cayelan C. Carey, Whitney M. Woelmer, Mary E. Lofton, Renato J. Figueiredo, Bethany J. Bookout, Rachel S. Corrigan, Vahid Daneshmand, Alexandria G. Hounshell, Dexter W. Howard, Abigail S. L. Lewis et al (2022) Advancing lake and reservoir water quality management with near-term, iterative ecological forecasting, Inland Waters, 12:1, 107-120, DOI: 10.1080/20442041.2020.1816421
\bibitem{Cleary} Emmet Cleary, Alfredo Garbuno-Inigo, Shiwei Lan, Tapio Schneider, Andrew M. Stuart, “Calibrate, emulate, sample”, Journal of Computational Physics, Volume 424, 2021, 109716, ISSN 0021-9991, https://doi.org/10.1016/j.jcp.2020.109716.
\bibitem{Dietze} Dietze et al, “Iterative near-term ecological forecasting: Needs, opportunities, and challenges”, Proceedings of the National Academy of Sciences, 115, 7, 1424-1432, 2018.
\bibitem{Fer} Fer, I., Kelly, R., Moorcroft, P. R., Richardson, A. D., Cowdery, E. M., and Dietze, M. C.: Linking big models to big data: efficient ecosystem model calibration through Bayesian model emulation, Biogeosciences, 15, 5801–5830, https://doi.org/10.5194/bg-15-5801-2018, 2018.
\bibitem{Fer2} Istem Fer, Alexey Shiklomanov, Kimberly A. Novick, Christopher M. Gough, M. Altaf Arain, Jiquan Chen, Bailey Murphy, Ankur R. Desai, Michael C. Dietze: Capturing site-to-site variability through Hierarchical Bayesian calibration of a process-based dynamic vegetation model, bioRxiv 2021.04.28.441243; doi: https://doi.org/10.1101/2021.04.28.441243. 
\bibitem{Arab} Arab, Ali \& Hooten, Mevin \& Wikle, Christopher. (2007). Hierarchical Spatial Models. Encyclopedia of geographical information science.
\bibitem{Friedlingstein} Friedlingstein, et al: Global Carbon Budget 2021, Earth Syst. Sci. Data, 14, 1917–2005, https://doi.org/10.5194/essd-14-1917-2022, 2022.
\bibitem{Hefley} Trevor J. Hefley, Mevin B. Hooten, Ephraim M. Hanks, Robin E. Russell, Daniel P. Walsh, Dynamic spatio-temporal models for spatial data, Spatial Statistics.
\bibitem{Kugler} Benoit Kugler, Florence Forbes, Sylvain Douté. Fast Bayesian Inversion for high dimensional inverse problems. Statistics and Computing, Springer Verlag (Germany), In press. ffhal-02908364v3f
\bibitem{Sarkka} Särkkä, S. (2013). Bayesian Filtering and Smoothing (Institute of Mathematical Statistics Textbooks). Cambridge: Cambridge University Press. doi:10.1017/CBO9781139344203
\bibitem{Waring} Richard H. Waring, Steven W. Running, CHAPTER 10 - Advances in Eddy-Flux Analyses, Remote Sensing, and Evidence of Climate Change, Forest Ecosystems (Third Edition), Academic Press, 2007, Pages 317-344, ISBN 9780123706058, https://doi.org/10.1016/B978-012370605-8.50017-7.
\bibitem{Clark} Clark, James S. 2005. Why environmental scientists are becoming Bayesians. Ecology Letters, Vol. 8: 2-14
\bibitem{Laubmeier} Laubmeier et al. Ecological Dynamics: Integrating Empirical, Statistical, and Analytical Methods. Trends Ecol Evol. 2020 Dec;35(12):1090-1099. doi: 10.1016/j.tree.2020.08.006. Epub 2020 Sep 12. PMID: 32933777.
\bibitem{Wikle} Wikle, C.K. (2015), Modern perspectives on statistics for spatio-temporal data. WIREs Comput Stat, 7: 86-98. https://doi.org/10.1002/wics.1341
\bibitem{Baker} Baker et al. "Analyzing Stochastic Computer Models: A Review with Opportunities." Statist. Sci. 37 (1) 64 - 89, February 2022. https://doi.org/10.1214/21-STS822

\end{thebibliography}



\end{document}



