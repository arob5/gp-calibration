---
title: "Higher Dimensional GP-Accelerated MCMC Comparison"
author: "Andrew Roberts"
date: '2023-12-19'
output: html_document
---


```{r, echo = FALSE, include = FALSE}
knitr::opts_chunk$set(echo = FALSE)

library(lhs)
library(hetGP)
library(mlegp)
library(ggplot2)
library(viridis)
library(parallel)
library(gridExtra)
library(data.table)
library(tmvtnorm)
library(RJSONIO)
library(BayesianTools)

base_dir <- getwd()
src_dir <- file.path(base_dir, "src")

source(file.path(src_dir, "sim_study_functions.r"))
source(file.path(src_dir, "mcmc_calibration_functions.r"))
source(file.path(src_dir, "gp_emulator_functions.r"))
source(file.path(src_dir, "gp_mcmc_functions.r"))
source(file.path(src_dir, "sequential_design_optimization.r"))
```


```{r}
# TODOs: 
#    - Write plotting functions suitable for evaluating GP emulators with multivariate input space. Maybe plot y vs. y_hat. 
#    - Update linear Gaussian data generation function: clean up code, improve comments, and allow for non-zero mean Gaussian prior.
#    - Write function to generate linear Gaussian test example in higher dimensions. Include univariate and bivariate 
#      histograms.
#    - Write function analogous to MCMC histogram function that plots bivariate histograms. Will be helpful to get a sense of 
#      how concentrated the true posterior is. 
#    - Step through some MCMC steps to see why it's getting stuck. 
#    - Maybe try putting this model into Stan and seeing how it handles it. 
#    - Try an EKI update to see if this can concentrate in on the posterior in a single time step. 
#    - Try sampling from exact posterior and see how well covariance adaptation does. 
#    - Add MCMC timing information to test function. 
```


```{r}
# Settings: 
#    - For reproducibility these are the only variables that should be modified in this file. 
#    - `write_mode` controls whether existing run is loaded or new run is executed. 

write_mode <- TRUE

settings <- list()
settings <- list(run_id="gp_traj_6d_lin_gaus", # General
                 global_seed=15,
                 data_seed=23,
                 design_seed=40,
                 save_sample_data=FALSE, 
                 N_obs=100,                    # Synthetic data generation
                 sig2_eps=1, 
                 N_design=50,                  # Design
                 design_method="LHS", 
                 N_mcmc=50000)                 # MCMC                  
                 
# Create test folder or load existing one. 
if(write_mode) {
  create_mcmc_run(settings, set_global_variables=TRUE)
} else {
  load_mcmc_run_data(settings$run_id, set_global_variables=TRUE)
}

# TODO: add option to save raw MCMC sample data. 


```


```{r}
# Linear Gaussian Model Setup. 
freqs <- c(0.1, 1, 1.4, 2, 0.5, 1.1)
D <- length(freqs)
G <- matrix(nrow=N_obs, ncol=D)
for(d in 1:D) {
  G[,d] <- matrix(sin(2*pi*freqs[d]*seq(1, N_obs)/N_obs), ncol=1)
}

Sig0 <- diag(rlnorm(D, meanlog=0, sdlog=2))
mu0 <- rnorm(D, mean=0, sd=4)

# Get synthetic data. 
linear_Gaussian_info <- generate_linear_Gaussian_test_data(data_seed, N_obs, D, Sig0, G, sig2_eps=sig2_eps)
computer_model_data <- linear_Gaussian_info$computer_model_data
theta_prior_params <- linear_Gaussian_info$theta_prior_params

plot(1:N_obs, computer_model_data$data_obs, main="Ground Truth and Observed Data", 
     xlab="t", ylab="y")
lines(1:N_obs, computer_model_data$data_ref, col="red")

```

```{r}

#
# Emulator Setup. 
#

# Emulator settings. 
emulator_settings <- data.frame(gp_lib = c("hetGP"), 
                                kernel = "Gaussian", 
                                transformation_method = c("truncated"),
                                emulator_target = "SSR",
                                scale_X = TRUE, 
                                normalize_y = TRUE)

# Generate design. 
input_bounds <- rbind(qnorm(.01, theta_prior_params$param1, theta_prior_params$param2), 
                      qnorm(.99, theta_prior_params$param1, theta_prior_params$param2))
rownames(input_bounds) <- c("min", "max")
colnames(input_bounds) <- computer_model_data$pars_cal_names
theta_prior_params_trunc <- truncate_prior_theta(theta_prior_params, input_bounds)

# Initial Design. 
design_settings <- data.frame(N_design=N_design, design_method=design_method)
init_design_info <- get_input_output_design(N_points=design_settings$N_design,
                                            design_method=design_settings$design_method, 
                                            scale_inputs=TRUE,
                                            param_ranges=input_bounds,  
                                            computer_model_data=computer_model_data, 
                                            theta_prior_params=theta_prior_params, design_seed=design_seed)
init_design_info$lpost <- calc_lpost_theta_product_lik(computer_model_data=computer_model_data, 
                                                       theta_vals=init_design_info$inputs, 
                                                       vars_obs=sig2_eps, 
                                                       SSR=init_design_info$outputs,
                                                       na.rm=TRUE, theta_prior_params=theta_prior_params, 
                                                       return_list=FALSE)
  
# Fit emulators on initial design. 
gp_fits <- fit_independent_GPs(X_train=init_design_info$inputs_scaled, Y_train=init_design_info$outputs_normalized, 
                               gp_lib=emulator_settings$gp_lib, gp_kernel=emulator_settings$kernel)$fits
emulator_info_list <- list(gp_fits=gp_fits, input_bounds=init_design_info$input_bounds, 
                           output_stats=init_design_info$output_stats, settings=emulator_settings)

for(fit in gp_fits) plot(fit)
```

```{r}
# Evaluate emulators. 

# TODO: 
#    - Need to ensure validation sets are being sampled within input_bounds.
#    - Make this code into a function. 
#    - Change so that the error bars are lines centered at the mean prediction (like to hetGP plot). 

N_design_validate <- 5000

# Validation design sampled from true posterior. 
design_post <- t(drop(linear_Gaussian_info$true_posterior$mean) + 
                  t(chol(linear_Gaussian_info$true_posterior$Cov)) %*% 
                    matrix(rnorm(D*N_design_validate), nrow=D, ncol=N_design_validate))
design_post_scaled <- scale_input_data(design_post, input_bounds)
gp_pred_post <- predict_independent_GPs(design_post_scaled, emulator_info_list$gp_fits, emulator_settings$gp_lib, 
                                        include_cov_mat=FALSE, denormalize_predictions=TRUE, 
                                        output_stats=emulator_info_list$output_stats, return_df=TRUE)$df
true_SSR_post <- get_computer_model_SSR(computer_model_data, theta_vals=design_post)                                            

# Validation design sampled from prior. 
design_validate_prior <- get_input_output_design(N_points=N_design_validate,
                                                 design_method="LHS", 
                                                 scale_inputs=TRUE,
                                                 param_ranges=input_bounds,  
                                                 computer_model_data=computer_model_data, 
                                                 theta_prior_params=theta_prior_params)
gp_pred_prior <- predict_independent_GPs(design_validate_prior$inputs_scaled, emulator_info_list$gp_fits,  
                                         emulator_settings$gp_lib, include_cov_mat=FALSE, denormalize_predictions=TRUE, 
                                         output_stats=emulator_info_list$output_stats, return_df=TRUE)$df

# Plots
CI_lower_prior <- qnorm(0.025, gp_pred_prior$mean_output1, sqrt(gp_pred_prior$var_comb_output1))
CI_upper_prior <- qnorm(0.975, gp_pred_prior$mean_output1, sqrt(gp_pred_prior$var_comb_output1))
plot(design_validate_prior$outputs, gp_pred_prior$mean_output1, xlab="true", ylab="pred", main="prior validation")
segments(x0=CI_lower_prior, y0=gp_pred_prior$mean_output1, 
         x1=CI_upper_prior, y1=gp_pred_prior$mean_output1, col=rgb(red=0, green=0, blue=1, alpha=0.3))

CI_lower_post <- qnorm(0.025, gp_pred_post$mean_output1, sqrt(gp_pred_post$var_comb_output1))
CI_upper_post <- qnorm(0.975, gp_pred_post$mean_output1, sqrt(gp_pred_post$var_comb_output1))
plot(true_SSR_post, gp_pred_post$mean_output1, xlab="true", ylab="pred", main="posterior validation")
segments(x0=CI_lower_post, y0=gp_pred_post$mean_output1, 
         x1=CI_upper_post, y1=gp_pred_post$mean_output1, col=rgb(red=0, green=0, blue=1, alpha=0.3))

```

```{r}
# Prepare GP-MCMC tests:
#    - Testing the GP-trajectory appraoch. 
#    - Varying two arguments: `use_gp_cov` and `second_gibbs_step`. 
#    - The current PEcAn algorithm corresponds to `use_gp_cov==FALSE`, `second_gibbs_step==TRUE`. 

# TODO: need to save the MCMC run settings to the run dir. 

# General settings to apply to all runs. 
learn_sig_eps <- FALSE
sig2_eps_init <- sig2_eps

# Fix pre-MCMC estimates of calibration and likelihood parameters based on design data. All MCMC
# algs will use this initialization. 
best_idx <- which.max(init_design_info$lpost)
theta_init <- init_design_info$inputs[best_idx,]

# Initialize proposal covariance using design. 
# TODO: 
#    - Consider more numerically stable way to compute this. 
#    - This is an estimate of the covariance of the true posterior; what we really want here 
#      is an estimate of the covariance of the GP-approximated posterior. I can obtain 
#      weights for the GP integrated out posterior, but not for the GP-trajectory posterior. 
#      Maybe should just use the estimate for the former to initialize both algorithms. 
# cov_prop_init <- cov.wt(init_design_info$inputs, exp(init_design_info$lpost))$cov
cov_prop_init <- diag(0.5*(input_bounds["max",] - input_bounds["min",])) 

# Algorithm-specific settings list. 
run_settings_list <- list()
run_settings_list[["cov_nogibbs2"]] <- list(test_label="cov_nogibbs2", 
                                             alg="ind_gp_trajectory",    
                                             use_gp_cov=TRUE, 
                                             second_gibbs_step=FALSE)
run_settings_list[["cov_gibbs2"]] <- list(test_label="cov_gibbs2",
                                             alg="ind_gp_trajectory",
                                             use_gp_cov=TRUE,
                                             second_gibbs_step=TRUE)
run_settings_list[["nocov_gibbs2"]] <- list(test_label="nocov_gibbs2",
                                             alg="ind_gp_trajectory",
                                             use_gp_cov=FALSE,
                                             second_gibbs_step=TRUE)
run_settings_list[["nocov_nogibbs2"]] <- list(test_label="nocov_nogibbs2",
                                              alg="ind_gp_trajectory",
                                              use_gp_cov=FALSE,
                                              second_gibbs_step=FALSE)
```


```{r}
# Run GP-MCMC algorithms. 
mcmc_test_info <- run_gp_mcmc_tests(run_settings_list, computer_model_data=computer_model_data, 
                                    theta_prior_params=theta_prior_params_trunc,
                                    emulator_info_list=emulator_info_list, theta_init=theta_init,
                                    N_chain=4, N_itr=N_mcmc, burn_ins=NULL, learn_sig_eps=learn_sig_eps,
                                    return_cov_prop_scale=TRUE, return_SSR_samp=TRUE, sig2_eps_init=sig2_eps, 
                                    cov_prop_init=cov_prop_init)
mcmc_samp_dt <- mcmc_test_info$mcmc_samp_dt
burn_ins <- mcmc_test_info$burn_ins
```


```{r}

# Append exact posterior samples. 
N_samp_exact <- N_mcmc
samp_exact <- t(drop(linear_Gaussian_info$true_posterior$mean) + 
                t(chol(linear_Gaussian_info$true_posterior$Cov)) %*% matrix(rnorm(D*N_samp_exact), nrow=D, ncol=N_samp_exact))
samp_exact_dt <- as.data.table(samp_exact)
colnames(samp_exact_dt) <- computer_model_data$pars_cal_names
samp_exact_dt[, c("param_type", "itr", "test_label") := list("theta", 1:N_samp_exact, "exact")]
samp_exact_dt <- melt.data.table(samp_exact_dt, id.vars=c("param_type", "itr", "test_label"), 
                                 variable.name="param_name", value.name="sample")

mcmc_samp_dt <- rbindlist(list(mcmc_samp_dt, samp_exact_dt), use.names=TRUE)
```


```{r}
# TODO: 
#    - add burn-in to plot titles. 
#    - add ability to plot all histograms on same plot, not just 2. 

if(write_mode) {
  trace_plots <- get_trace_plots(mcmc_samp_dt, param_types="theta", save_dir=plot_dir)
  hist_plots <- get_hist_plot_comparisons(mcmc_samp_dt, param_types="theta", test_label_baseline="exact", 
                                          xlab="samples", ylab="density", bins=30, save_dir=plot_dir,
                                          burn_in_start=as.integer(0.5*N_mcmc))
} else {
}

for(plt in trace_plots) plot(plt)
for(plt in hist_plots) plot(plt)
```

```{r}
# Running error measures. 
running_errs <- compute_mcmc_running_err_multivariate(mcmc_samp_dt, linear_Gaussian_info$true_posterior$mean, 
                                                      linear_Gaussian_info$true_posterior$Cov, 
                                                      param_type="theta", test_labels=NULL,
                                                      burn_in_start=100, 
                                                      init_running_err_using_burnin=TRUE) 

fwrite(running_errs, file.path(analysis_dir, "running_errs.csv"))

```

```{r}
plt_mean_err <- ggplot(running_errs, aes(x=itr, y=mean_err, col=test_label)) + geom_line()
plt_cov_err <- ggplot(running_errs, aes(x=itr, y=cov_err, col=test_label)) + geom_line()

plot(plt_mean_err)
plot(plt_cov_err)

plts <- list(mean=plt_mean_err, cov=plt_cov_err)
save_plots(plts, "run_err", plot_dir)
```

```{r}
lbl1 <- "cov_nogibbs2"
lbl2 <- "nocov_nogibbs2"

for(i in 1:6) {
  param_name <- paste0("theta", i)
  name1 <- paste(lbl1, "theta", param_name, sep="_")
  name2 <- paste(lbl2, "theta", param_name, sep="_")

  grid.arrange(hist_plots[[name1]], hist_plots[[name2]], ncol=2)
}
```



