---
title: "Tests for gpWrapper and Classes which Inherit from gpWrapper"
author: "Andrew Roberts"
date: '2023-12-19'
output: html_document
---

```{r, echo = FALSE, include = FALSE}
knitr::opts_chunk$set(echo = FALSE)

library(lhs)
library(ggplot2)
library(viridis)
library(parallel)
library(data.table)

base_dir <- getwd()
src_dir <- file.path(base_dir, "src")

source(file.path(src_dir, "gp_helper_functions.r"))
source(file.path(src_dir, "gpWrapper.r"))
source(file.path(src_dir, "general_helper_functions.r"))
source(file.path(src_dir, "statistical_helper_functions.r"))
source(file.path(src_dir, "plotting_helper_functions.r"))
```

```{r}
set.seed(10)
```

```{r}
#
# Test datasets for code validation. 
#

# 1D input, 2D output, noisy observations. 
N_design1 <- 4
f1 <- function(x) cbind(x^2, 3*x^3)
X1 <- matrix(seq(10,20,length.out=N_design1), ncol=1)
Y1 <- f1(X1) + cbind(15*rnorm(nrow(X1)), 30*rnorm(nrow(X1)))
X1_new <- matrix(seq(10, 20, length.out=101), ncol=1)
Y1_new_noiseless <- f1(X1_new)

# 2D input, 1D output, noiseless observations. 
f2 <- function(x) matrix(3*x[,1]^2 - 2*x[,2]^3, ncol=1)
X2 <- randomLHS(25, 2)
Y2 <- f2(X2)
X2_new <- as.matrix(expand.grid(seq(0,1, length.out=51), 
                                seq(0,1, length.out=51)))
Y2_new <- f2(X2_new)
```


# 1d validation. 

TODO: need to also add check for negative predictive variance in covariance matrix. 

## hetGP
```{r}
# Default fit/predict. 
gpHet1 <- gpWrapperHet(X1, Y1, normalize_output=TRUE, scale_input=TRUE)
gpHet1$fit("Gaussian", "constant")
gpHet1_pred <- gpHet1$predict(X1_new, return_cov=TRUE)
gpHet1$plot_pred_1d(X1_new, pred_list=gpHet1_pred, Y_new=Y1_new_noiseless)

# Fit/predict with fixed small nugget (intended for interpolation). 
gpHet1 <- gpWrapperHet(X1, Y1, normalize_output=TRUE, scale_input=TRUE)
gpHet1$fit("Gaussian", "constant", estimate_nugget=FALSE)
gpHet1_pred <- gpHet1$predict(X1_new, return_cov=TRUE)
gpHet1$plot_pred_1d(X1_new, pred_list=gpHet1_pred, Y_new=Y1_new_noiseless)
```

```{r}
# Test cross covariance calculation by computing the cross covariances between 
# the same set of test points. This should equal the covariance returned when 
# `return_cov` is TRUE and `include_nugget` is FALSE. 

gpHet1_pred_cross <- gpHet1$predict(X1_new, return_mean=FALSE, return_var=FALSE, 
                                    return_cross_cov=TRUE, X_cross=X1_new)
gpHet1_pred_cross_baseline <- gpHet1$predict(X1_new, return_mean=FALSE, return_var=FALSE, 
                                             return_cov=TRUE, include_nugget=FALSE)

cat("Cross covariance calculation is correct: ", all.equal(gpHet1_pred_cross$cross_cov, gpHet1_pred_cross_baseline$cov))
```


```{r}
# Sample 
gpHet1_samp <- gpHet1$sample(X1_new, use_cov=TRUE, N_samp=5, pred_list=gpHet1_pred)
gpHet1_samp_rectified <- gpHet1$sample(X1_new, use_cov=TRUE, N_samp=5, pred_list=gpHet1_pred, adjustment="rectified")
gpHet1_samp_truncated <- gpHet1$sample(X1_new, use_cov=TRUE, N_samp=5, pred_list=gpHet1_pred, adjustment="truncated") # Stability issues when there is little noise. 

for(i in 1:gpHet1$Y_dim) {
  # Gaussian.
  matplot(X1_new, gpHet1_samp[,,i],type="l", main="Gaussian Predictive Samples.")
  lines(X1_new, Y1_new_noiseless[,i], col="red")
  
  # Rectified Gaussian. 
  matplot(X1_new, gpHet1_samp_rectified[,,i],type="l", main="Rectified Gaussian Predictive Samples.")
  lines(X1_new, Y1_new_noiseless[,i], col="red")
  
  # Truncated Gaussian. 
  matplot(X1_new, gpHet1_samp_rectified[,,i],type="l", main="Zero-Truncated Gaussian Predictive Samples.")
  lines(X1_new, Y1_new_noiseless[,i], col="red")
}
```

```{r}
#
# Test `update()` method: 
# Construct the design in two stages and ensure the resulting design is 
# equal to the one-shot design above. Note that the kernel hyperparameters will not be the 
# same as above; the hyperparameter estimation will only use the initial design. Note also 
# that `X_train`, `Y_train` will not necessarily agree across the two methods since 
# `X_bounds`, `Y_mean`, and `Y_std` are based on the initial design (they are not updated), 
# so the scaling/normalization will be different. 
#

N_partial_design1 <- N_design1 - 1

# Fit on partial design. 
gpHet1_update <- gpWrapperHet(X1[1:N_partial_design1,,drop=FALSE], Y1[1:N_partial_design1,,drop=FALSE], 
                              normalize_output=TRUE, scale_input=TRUE)
gpHet1_update$fit("Gaussian", "constant")

# Predictive plot for GPs pre-update. 
gpHet1_update$plot_pred_1d(X1_new, Y_new=Y1_new_noiseless)

# Now update design. 
gpHet1_update$update(X1[(N_partial_design1+1):N_design1,,drop=FALSE], 
                     Y1[(N_partial_design1+1):N_design1,,drop=FALSE],
                     update_hyperpar=FALSE)

# Test. 
print("X agrees:")
all.equal(gpHet1$X, gpHet1_update$X)

print("Y agrees:")
all.equal(gpHet1$Y, gpHet1_update$Y)

print("X_train agrees after inverting scaling:")
all.equal(gpHet1$scale(gpHet1$X_train, inverse=TRUE), 
          gpHet1_update$scale(gpHet1_update$X_train, inverse=TRUE))

print("Y_train agrees after inverting normalization:")
all.equal(gpHet1$normalize(gpHet1$Y_train, inverse=TRUE), 
          gpHet1_update$normalize(gpHet1_update$Y_train, inverse=TRUE))

print("hetGP X agrees with gpWrapper X_train:")
for(i in 1:gpHet1_update$Y_dim) {
  print(all.equal(gpHet1_update$gp_model[[i]]$X0, gpHet1_update$X_train))
}
  
print("hetGP Y agrees with gpWrapper Y_train")
for(i in 1:gpHet1_update$Y_dim) {
  print(all.equal(gpHet1_update$gp_model[[i]]$Z, gpHet1_update$Y_train[,i]))
}

# Predictive plot for updated GPs. With so few training points, the fit often yields a 
# very small nugget that almost interpolates, which leads to overfitting. 
gpHet1_update$plot_pred_1d(X1_new, Y_new=Y1_new_noiseless)
```

## kergp
```{r}

# TODO: TEMP: gpWrapperKerGP currently only supports fixed nugget. 
# Default fit/predict. 
# gpKerGP1 <- gpWrapperKerGP(X1, Y1, normalize_output=TRUE, scale_input=TRUE)
# gpKerGP1$fit("Gaussian", "constant", optimFun="stats::optim")
# gpKerGP1_pred <- gpKerGP1$predict(X_new=X1_new, return_cov=TRUE)
# gpKerGP1$plot_pred_1d(X1_new, pred_list=gpKerGP1_pred, Y_new=Y1_new_noiseless)

# Fit/predict with fixed small nugget (intended for interpolation). 
gpKerGP1 <- gpWrapperKerGP(X1, Y1, normalize_output=TRUE, scale_input=TRUE)
gpKerGP1$fit("Gaussian", "constant", estimate_nugget=FALSE, trace=TRUE, multistart=5)
gpKerGP1_pred <- gpKerGP1$predict(X_new=X1_new, return_cov=TRUE, include_nugget=TRUE)
gpKerGP1$plot_pred_1d(X1_new, pred_list=gpKerGP1_pred, Y_new=Y1_new_noiseless)

# Gaussian plus quadratic kernel. 
gpKerGP1 <- gpWrapperKerGP(X1, Y1, normalize_output=TRUE, scale_input=TRUE)
gpKerGP1$fit("Gaussian_plus_Quadratic", "constant", estimate_nugget=FALSE, trace=TRUE, multistart=5)
gpKerGP1_pred <- gpKerGP1$predict(X_new=X1_new, return_cov=TRUE, include_nugget=TRUE)
gpKerGP1$plot_pred_1d(X1_new, pred_list=gpKerGP1_pred, Y_new=Y1_new_noiseless)

```

```{r}
#
# Test `update()` method: 
# Construct the design in two stages and ensure the resulting design is 
# equal to the one-shot design above. Note that the kernel hyperparameters will not be the 
# same as above; the hyperparameter estimation will only use the initial design. Note also 
# that `X_train`, `Y_train` will not necessarily agree across the two methods since 
# `X_bounds`, `Y_mean`, and `Y_std` are based on the initial design (they are not updated), 
# so the scaling/normalization will be different. 
#

N_partial_design1 <- N_design1 - 1

# Fit on partial design. 
gpKerGP1_update <- gpWrapperKerGP(X1[1:N_partial_design1,,drop=FALSE], 
                                  Y1[1:N_partial_design1,,drop=FALSE], 
                                  normalize_output=TRUE, scale_input=TRUE)
gpKerGP1_update$fit("Gaussian", "constant", estimate_nugget=FALSE)

# Predictive plot for GPs pre-update. 
gpKerGP1_update$plot_pred_1d(X1_new, Y_new=Y1_new_noiseless)

# Now update design. 
gpKerGP1_update$update(X1[(N_partial_design1+1):N_design1,,drop=FALSE], 
                       Y1[(N_partial_design1+1):N_design1,,drop=FALSE],
                       update_hyperpar=FALSE)

# Test. 
print("X agrees:")
all.equal(gpKerGP1$X, gpKerGP1_update$X)

print("Y agrees:")
all.equal(gpKerGP1$Y, gpKerGP1_update$Y)

print("X_train agrees after inverting scaling:")
all.equal(gpKerGP1$scale(gpKerGP1$X_train, inverse=TRUE), 
          gpKerGP1_update$scale(gpKerGP1_update$X_train, inverse=TRUE))

print("Y_train agrees after inverting normalization:")
all.equal(gpKerGP1$normalize(gpKerGP1$Y_train, inverse=TRUE), 
          gpKerGP1_update$normalize(gpKerGP1_update$Y_train, inverse=TRUE))

print("kergp X agrees with gpWrapper X_train:")
for(i in 1:gpKerGP1_update$Y_dim) {
  print(all.equal(gpKerGP1_update$gp_model[[i]]$X, gpKerGP1_update$X_train))
}
  
print("kergp y agrees with gpWrapper Y_train")
for(i in 1:gpKerGP1_update$Y_dim) {
  print(all.equal(gpKerGP1_update$gp_model[[i]]$y, gpKerGP1_update$Y_train[,i]))
}

# Predictive plot for updated GPs. The initial hyperparameter fit was very poor so 
# can lead to some weird results. The lengthscale is so small that even a slight 
# deviation from the design points causes reversion to the prior. This is why 
# the function appears to interpolate the endpoints (which are also design points), 
# but the two interior design points are not a subset of the test points, so the 
# function appears not to be interpolating these points, but it really is. 
gpKerGP1_update$plot_pred_1d(X1_new, Y_new=Y1_new_noiseless)
```









# 2d validation. 

## hetGP
```{r}
# Fit
gpHet2 <- gpWrapperHet(X2, Y2, normalize_output=TRUE, scale_input=TRUE)
gpHet2$fit("Matern3_2", "constant", estimate_nugget=FALSE)
```

```{r}
# TODO: Add plot_pred_2d for 2d heatmap plots so I can validate this plot. 
# TODO: Add prediction intervals. 
# Predict and plot. 
# gpHet2_pred <- gpHet2$predict(X2_new)
gpHet2$plot_pred(X2_new, Y2_new)
```






