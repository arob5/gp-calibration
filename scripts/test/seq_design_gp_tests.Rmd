---
title: "Tests for Sequential Design and Optimization Functions for Gaussian Processes"
author: "Andrew Roberts"
date: '2024-03-19'
output: html_document
---

```{r, echo = FALSE, include = FALSE}
knitr::opts_chunk$set(echo = FALSE)

library(lhs)
library(ggplot2)
library(viridis)
library(parallel)
library(data.table)

base_dir <- getwd()
src_dir <- file.path(base_dir, "src")

source(file.path(src_dir, "gpWrapper.r"))
source(file.path(src_dir, "general_helper_functions.r"))
source(file.path(src_dir, "statistical_helper_functions.r"))
source(file.path(src_dir, "plotting_helper_functions.r"))
source(file.path(src_dir, "llikEmulator.r"))
source(file.path(src_dir, "gp_emulator_functions.r"))
source(file.path(src_dir, "sim_study_functions.r"))
source(file.path(src_dir, "mcmc_calibration_functions.r"))
source(file.path(src_dir, "seq_design_gp.r"))
```


```{r}
global_seed <- 22
data_seed <- 9

set.seed(22)
```


```{r}
#
# Linear Gaussian Model to perform tests.  
#

# 1D input, 2D output, some missing observations.
N_output <- 2
N_obs <- 100
N_missing_output1 <- 4
N_missing_output2 <- 6
u_bound_lower <- 0
u_bound_upper <- 3
u_prior <- data.frame(dist="Uniform", param1=u_bound_lower, param2=u_bound_upper)
output_names <- paste0("output", 1:N_output)
par_names <- "par1"

# Multi-output forward model. `G_old` satisfies the requirements of the `computer_model_list`
# so the old design functions can be used, while `G` vectorizes over multiple 
# inputs and returns a 3-dimensional array of shape Ntimestep x Ninput x Noutput. 
G1_mat <- matrix(sin(2*pi*seq(1,N_obs)/N_obs), ncol=1)
G2_mat <- matrix(cos(2*pi*3*seq(1,N_obs)/N_obs), ncol=1)
G <- function(U, ...) abind(y1=G1_mat %*% t(U), y2=G2_mat %*% t(U), along=3)
G_old <- function(u, ...) cbind(y1=u*G1_mat, y2=u*G2_mat)

# Ground truth.  
u_true <- matrix(runif(1, min=u_bound_lower, max=u_bound_upper), nrow=1)
sig2_true <- setNames(c(1, 1.5), output_names)
G_true <-  G(u_true)[,1,]

# Simulate data, adding missing observations. 
Y <- G_true + cbind(rnorm(N_obs, 0, sqrt(sig2_true[1])),
                    rnorm(N_obs, 0, sqrt(sig2_true[2])))
Y[sample.int(N_obs, N_missing_output1),1] <- NA_real_
Y[sample.int(N_obs, N_missing_output2),2] <- NA_real_
colnames(Y) <- output_names
computer_model_data <- list(data_obs=Y, f=G_old, output_vars=output_names)
computer_model_data$n_obs <- colSums(!is.na(Y))

for(p in 1:ncol(Y)) {
  plot(1:N_obs, Y[,p], main=paste0("Ground Truth and Observed Data: Output ", p),
       xlab="t", ylab=colnames(Y)[p])
  lines(1:N_obs, G_true[,p], col="red")
}

```


```{r}
#
# Generate design points in parameter space. 
#

# Settings. 
N_design <- 4
N_test <- 51

# Bounds on parameter space.  
input_bounds <- rbind(u_bound_lower, u_bound_upper)
rownames(input_bounds) <- c("min", "max")

# Generate latin hypercube designs. 
design_info <- get_input_output_design(N_points=N_design,
                                       design_method="LHS", 
                                       scale_inputs=FALSE,
                                       normalize_response=FALSE,
                                       param_ranges=input_bounds,  
                                       computer_model_data=computer_model_data, 
                                       theta_prior_params=u_prior)
colnames(design_info$inputs) <- par_names

# Validation data. 
test_info <- get_input_output_design(N_points=N_test,
                                     design_method="grid", 
                                     scale_inputs=FALSE,
                                     normalize_response=FALSE,
                                     param_ranges=input_bounds,  
                                     computer_model_data=computer_model_data, 
                                     theta_prior_params=u_prior)
```


```{r}
#
# We consider fitting a GP directly to the log likelihood (assuming fixed likelihood parameters). 
# Here we calculate the true log likelihood at the design inputs for training the 
# emulator, as well as at the test inputs for validation. 
#

design_info$llik <- matrix(llik_product_Gaussian(computer_model_data, sig2_true, SSR=design_info$outputs, normalize=TRUE), ncol=1)
test_info$llik <- matrix(llik_product_Gaussian(computer_model_data, sig2_true, SSR=test_info$outputs, normalize=TRUE), ncol=1)
                                  
```


```{r}
# Fit emulator to log likelihood. 

gp_llik <- gpWrapperHet(design_info$inputs, design_info$llik, normalize_output=TRUE, scale_input=TRUE)
gp_llik$fit("Gaussian", "constant", estimate_nugget=FALSE)

gp_llik$plot_pred_1d(test_info$inputs, Y_new=test_info$llik)
```

```{r}
# Compute log IEVAR acquisition at test inputs, using same grid of inputs as grid points. Evaluating 
# acquisition at one point ata a time, not in batch mode. 

log_IEVAR_vals <- evaluate_acq_func_vectorized(acq_IEVAR_grid, input_mat=test_info$inputs, gp=gp_llik,
                                               grid_points=test_info$inputs, log_scale=TRUE)
```

```{r}
# Plot log IEVAR evaluations. 
plot(test_info$inputs, log_IEVAR_vals, type="l", xlab="test inputs", ylab="log IEVAR", 
     main="log IEVAR single point evals, uniform weights")
```


```{r}
# Repeat but with N(2.5, 0.1^2) weights. 

wts <- dnorm(test_info$inputs, 2.5, 0.1)

log_IEVAR_vals_wt <- evaluate_acq_func_vectorized(acq_IEVAR_grid, input_mat=test_info$inputs, gp=gp_llik,
                                                  grid_points=test_info$inputs, log_scale=TRUE, weights=wts)
```


```{r}
# Plot log IEVAR evaluations with Gaussian weights.
plot(test_info$inputs, log_IEVAR_vals_wt, type="l", xlab="test inputs", ylab="log IEVAR", 
     main="log IEVAR single point evals, N(2.5, 0.1^2) weights")
```

# 2d Input Example. 

```{r}
#
# Define Bayesian inverse problem. 
#

# Non-linear forward model. 
n_obs <- 100
G_fwd <- t(t(chol(matrix(c(1, .7, .7, 1), nrow=2))) %*% matrix(rnorm(n_obs*2), nrow=2, ncol=n_obs))
fwd <- function(u) {
  u <- drop(u)
  G_fwd %*% matrix(c(0.5*u[1], exp(-2*u[2])), ncol=1)
}

# Isotropic Gaussian likelihood. 
sig2_eps <- 0.4

# Ground truth and data. 
u_true <- c(1.216, 0.331)
y <- drop(fwd(u_true)) + rnorm(n_obs, 0, sqrt(sig2_eps)) 

# True log-posterior, using N(0, I) prior. 
lpost_true <- function(u) {
  sum(dnorm(y, mean=fwd(u), sd=sqrt(sig2_eps), log=TRUE)) + sum(dnorm(drop(u), log=TRUE))
}

lpost_true_vect <- function(U) {
  apply(U, 1, lpost_true)
}

computer_model_data <- list(data_obs=y, f=fwd, output_vars="y", n_obs=n_obs)
u_prior <- data.frame(dist="Gaussian", param1=c(0,0), param2=rep(sqrt(sig2_eps), 2))

```

```{r}
#
# Plot contours of true posterior. 
#

# True post contours
rng_u1 <- c(0, 3)
rng_u2 <- c(0, 2)
input_bounds <- cbind(rng_u1, rng_u2)
rownames(input_bounds) <- c("min", "max")
colnames(input_bounds) <- c("u1", "u2")

contour_grid_u1 <- seq(rng_u1[1], rng_u1[2], length.out=50)
contour_grid_u2 <- seq(rng_u2[1], rng_u2[2], length.out=50)
contour_grid <- expand.grid(contour_grid_u1, contour_grid_u2)
lpost_contour <- lpost_true_vect(as.matrix(contour_grid))
lpost_dens_grid <- data.frame(x=contour_grid[,1], y=contour_grid[,2], z=lpost_contour)

test_info <- list(inputs=as.matrix(contour_grid), outputs=matrix(lpost_contour, ncol=1))
                                    
ggplot() + 
  geom_contour(aes(x=x, y=y, z=z), lpost_dens_grid, bins=20) +
  xlab("u1") + ylab("u2") + ggtitle("True log posterior density contours")
                          
```


```{r}
#
# Initial design and fit GP. 
#

N_design <- 4

# Generate latin hypercube design. 
design_info<- get_input_design(N_points=N_design, 
                               theta_prior_params=u_prior, 
                               design_method="LHS", 
                               scale_inputs=FALSE, 
                               param_ranges=input_bounds)
colnames(design_info$inputs) <- colnames(input_bounds)
design_info$outputs <- matrix(lpost_true_vect(design_info$inputs), ncol=1)

# Fit GP emulator to unnormalized lpost density. 
gp_lpost <- gpWrapperHet(design_info$inputs, design_info$outputs, normalize_output=TRUE, scale_input=TRUE)
gp_lpost$fit("Gaussian", "constant", estimate_nugget=FALSE)

```
```{r}
# Predict at grid points. 

lpost_pred_list <- gp_lpost$predict(test_info$inputs, return_mean=TRUE, return_var=TRUE, return_cov=FALSE)

# TODO: add option to pass in multiple scalar values into the `y` argument so that multiple plots will be 
# created. I might already have this implemented - check. 

# TODO: need to think about the log scale here and how to add geom_contour option to `plot_heatmap`. 
plt <- plot_heatmap(test_info$inputs, exp(test_info$outputs), raster=TRUE, log_scale=TRUE)
plt <- plt + geom_contour(mapping=aes(x=x1, y=x2, z=y), bins=20)
plot(plt)


plot_heatmap (X, y, samples_kde=NULL, points_mat=NULL,  
                         raster=FALSE, point_coords=NULL, main_title="Heatmap", 
                         invert_colors=TRUE, legend_label="y", log_scale=FALSE, 
                         point_coords_shape=8, point_coords_col="black", 
                         points_mat_size=1, point_coords_size=3, 
                         samples_kde_lab="KDE", points_mat_lab="points_mat", 
                         KDE_opacity=1.0, xlab="x1", ylab="x2")



```
















