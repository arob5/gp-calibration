---
title: "Tests for Sequential Design and Optimization Functions for Gaussian Processes"
author: "Andrew Roberts"
date: '2024-03-19'
output: html_document
---

```{r, echo = FALSE, include = FALSE}
knitr::opts_chunk$set(echo = FALSE)

library(lhs)
library(ggplot2)
library(viridis)
library(parallel)
library(data.table)

base_dir <- getwd()
src_dir <- file.path(base_dir, "src")

source(file.path(src_dir, "gpWrapper.r"))
source(file.path(src_dir, "general_helper_functions.r"))
source(file.path(src_dir, "statistical_helper_functions.r"))
source(file.path(src_dir, "plotting_helper_functions.r"))
source(file.path(src_dir, "llikEmulator.r"))
source(file.path(src_dir, "gp_emulator_functions.r"))
source(file.path(src_dir, "sim_study_functions.r"))
source(file.path(src_dir, "mcmc_calibration_functions.r"))
source(file.path(src_dir, "seq_design_gp.r"))
```


```{r}
global_seed <- 22
data_seed <- 9

set.seed(22)
```


```{r}
#
# Linear Gaussian Model to perform tests.  
#

# 1D input, 2D output, some missing observations.
N_output <- 2
N_obs <- 100
N_missing_output1 <- 4
N_missing_output2 <- 6
u_bound_lower <- 0
u_bound_upper <- 3
u_prior <- data.frame(dist="Uniform", param1=u_bound_lower, param2=u_bound_upper)
output_names <- paste0("output", 1:N_output)
par_names <- "par1"

# Multi-output forward model. `G_old` satisfies the requirements of the `computer_model_list`
# so the old design functions can be used, while `G` vectorizes over multiple 
# inputs and returns a 3-dimensional array of shape Ntimestep x Ninput x Noutput. 
G1_mat <- matrix(sin(2*pi*seq(1,N_obs)/N_obs), ncol=1)
G2_mat <- matrix(cos(2*pi*3*seq(1,N_obs)/N_obs), ncol=1)
G <- function(U, ...) abind(y1=G1_mat %*% t(U), y2=G2_mat %*% t(U), along=3)
G_old <- function(u, ...) cbind(y1=u*G1_mat, y2=u*G2_mat)

# Ground truth.  
u_true <- matrix(runif(1, min=u_bound_lower, max=u_bound_upper), nrow=1)
sig2_true <- setNames(c(1, 1.5), output_names)
G_true <-  G(u_true)[,1,]

# Simulate data, adding missing observations. 
Y <- G_true + cbind(rnorm(N_obs, 0, sqrt(sig2_true[1])),
                    rnorm(N_obs, 0, sqrt(sig2_true[2])))
Y[sample.int(N_obs, N_missing_output1),1] <- NA_real_
Y[sample.int(N_obs, N_missing_output2),2] <- NA_real_
colnames(Y) <- output_names
computer_model_data <- list(data_obs=Y, f=G_old, output_vars=output_names)
computer_model_data$n_obs <- colSums(!is.na(Y))

for(p in 1:ncol(Y)) {
  plot(1:N_obs, Y[,p], main=paste0("Ground Truth and Observed Data: Output ", p),
       xlab="t", ylab=colnames(Y)[p])
  lines(1:N_obs, G_true[,p], col="red")
}

```


```{r}
#
# Generate design points in parameter space. 
#

# Settings. 
N_design <- 4
N_test <- 51

# Bounds on parameter space.  
input_bounds <- rbind(u_bound_lower, u_bound_upper)
rownames(input_bounds) <- c("min", "max")

# Generate latin hypercube designs. 
design_info <- get_input_output_design(N_points=N_design,
                                       design_method="LHS", 
                                       scale_inputs=FALSE,
                                       normalize_response=FALSE,
                                       param_ranges=input_bounds,  
                                       computer_model_data=computer_model_data, 
                                       theta_prior_params=u_prior)
colnames(design_info$inputs) <- par_names

# Validation data. 
test_info <- get_input_output_design(N_points=N_test,
                                     design_method="grid", 
                                     scale_inputs=FALSE,
                                     normalize_response=FALSE,
                                     param_ranges=input_bounds,  
                                     computer_model_data=computer_model_data, 
                                     theta_prior_params=u_prior)
```


```{r}
#
# We consider fitting a GP directly to the log likelihood (assuming fixed likelihood parameters). 
# Here we calculate the true log likelihood at the design inputs for training the 
# emulator, as well as at the test inputs for validation. 
#

design_info$llik <- matrix(llik_product_Gaussian(computer_model_data, sig2_true, SSR=design_info$outputs, normalize=TRUE), ncol=1)
test_info$llik <- matrix(llik_product_Gaussian(computer_model_data, sig2_true, SSR=test_info$outputs, normalize=TRUE), ncol=1)
                                  
```


```{r}
# Fit emulator to log likelihood. 

gp_llik <- gpWrapperHet(design_info$inputs, design_info$llik, normalize_output=TRUE, scale_input=TRUE)
gp_llik$fit("Gaussian", "constant", estimate_nugget=FALSE)

gp_llik$plot_pred_1d(test_info$inputs, Y_new=test_info$llik)
```

```{r}
# Compute log IEVAR acquisition at test inputs, using same grid of inputs as grid points. Evaluating 
# acquisition at one point ata a time, not in batch mode. 

log_IEVAR_vals <- evaluate_acq_func_vectorized(acq_IEVAR_grid, input_mat=test_info$inputs, gp=gp_llik,
                                               grid_points=test_info$inputs, log_scale=TRUE)
```

```{r}
# Plot log IEVAR evaluations. 
plot(test_info$inputs, log_IEVAR_vals, type="l", xlab="test inputs", ylab="log IEVAR", 
     main="log IEVAR single point evals, uniform weights")
```


```{r}
# Repeat but with N(2.5, 0.1^2) weights. 

wts <- dnorm(test_info$inputs, 2.5, 0.1)

log_IEVAR_vals_wt <- evaluate_acq_func_vectorized(acq_IEVAR_grid, input_mat=test_info$inputs, gp=gp_llik,
                                                  grid_points=test_info$inputs, log_scale=TRUE, weights=wts)
```


```{r}
# Plot log IEVAR evaluations with Gaussian weights.
plot(test_info$inputs, log_IEVAR_vals_wt, type="l", xlab="test inputs", ylab="log IEVAR", 
     main="log IEVAR single point evals, N(2.5, 0.1^2) weights")
```

# 2d Input Example. 

```{r}
#
# Define Bayesian inverse problem. 
#

# Non-linear forward model. 
n_obs <- 100
G_fwd <- t(t(chol(matrix(c(1, .7, .7, 1), nrow=2))) %*% matrix(rnorm(n_obs*2), nrow=2, ncol=n_obs))
fwd <- function(u) {
  u <- drop(u)
  G_fwd %*% matrix(c(0.5*u[1], exp(-2*u[2])), ncol=1)
}

# Isotropic Gaussian likelihood. 
sig2_eps <- 0.4

# Ground truth and data. 
u_true <- c(1.216, 0.331)
y <- drop(fwd(u_true)) + rnorm(n_obs, 0, sqrt(sig2_eps)) 

# True log-posterior, using N(0, I) prior. 
lpost_true <- function(u) {
  sum(dnorm(y, mean=fwd(u), sd=sqrt(sig2_eps), log=TRUE)) + sum(dnorm(drop(u), log=TRUE))
}

lpost_true_vect <- function(U) {
  apply(U, 1, lpost_true)
}

computer_model_data <- list(data_obs=y, f=fwd, output_vars="y", n_obs=n_obs)
u_prior <- data.frame(dist="Gaussian", param1=c(0,0), param2=rep(sqrt(sig2_eps), 2))

```

```{r}
#
# Plot contours of true posterior. 
#

# True post contours
rng_u1 <- c(0, 3)
rng_u2 <- c(0, 2)
input_bounds <- cbind(rng_u1, rng_u2)
rownames(input_bounds) <- c("min", "max")
colnames(input_bounds) <- c("u1", "u2")

contour_grid_u1 <- seq(rng_u1[1], rng_u1[2], length.out=50)
contour_grid_u2 <- seq(rng_u2[1], rng_u2[2], length.out=50)
contour_grid <- expand.grid(contour_grid_u1, contour_grid_u2)
colnames(contour_grid) <- c("u1", "u2")
lpost_contour <- lpost_true_vect(as.matrix(contour_grid))
lpost_dens_grid <- data.frame(x=contour_grid[,1], y=contour_grid[,2], z=lpost_contour)

test_info <- list(inputs=as.matrix(contour_grid), outputs=matrix(lpost_contour, ncol=1))
                                    
ggplot() + 
  geom_contour(aes(x=x, y=y, z=z), lpost_dens_grid, bins=20) +
  xlab("u1") + ylab("u2") + ggtitle("True log posterior density contours")
                          
```


```{r}
#
# Initial design and fit GP. 
#

N_design <- 4

# Generate latin hypercube design. 
design_info<- get_input_design(N_points=N_design, 
                               theta_prior_params=u_prior, 
                               design_method="LHS", 
                               scale_inputs=FALSE, 
                               param_ranges=input_bounds)
colnames(design_info$inputs) <- colnames(input_bounds)
design_info$outputs <- matrix(lpost_true_vect(design_info$inputs), ncol=1)

# Fit GP emulator to unnormalized lpost density. 
gp_lpost <- gpWrapperHet(design_info$inputs, design_info$outputs, normalize_output=TRUE, scale_input=TRUE)
gp_lpost$fit("Gaussian", "constant", estimate_nugget=FALSE)

```
```{r}
# Predict at grid points. 

lpost_pred_list <- gp_lpost$predict(test_info$inputs, return_mean=TRUE, return_var=TRUE, return_cov=FALSE)

# TODO: add option to pass in multiple scalar values into the `y` argument so that multiple plots will be 
# created. I might already have this implemented - check. 
# TODO: I should use facets to make multiple plots. 
# TODO: ggdensity package seems useful. 

df_contour <- data.frame(x1=test_info$inputs[,1], x2=test_info$inputs[,2], y=test_info$outputs, 
                         y_pred=lpost_pred_list$mean[,1], y_sd=sqrt(lpost_pred_list$var))
df_design <- data.frame(x1=design_info$inputs[,1], x2=design_info$inputs[,2], y=design_info$outputs[,1])

plt <- ggplot(df_contour) + 
        geom_contour(aes(x=x1, y=x2, z=y), color="blue") + 
        geom_contour(aes(x=x1, y=x2, z=y_pred), color="red") + 
        geom_point(aes(x=x1, y=x2), data=df_design, color="black") + 
        ggtitle("True lpost vs. GP lpost predictive mean")

plt_uncertainty <- plot_heatmap(test_info$inputs, sqrt(lpost_pred_list$var[,1]), points_mat=design_info$inputs, raster=TRUE, 
                                point_coords=u_true, main_title="lpost predictive SD", points_mat_lab="Design point")

plot(plt)
plot(plt_uncertainty)

# plt <- plot_heatmap(test_info$inputs, test_info$outputs, raster=TRUE)
# plot(plt)

```

```{r}
# Evaluate log IEVAR on grid. 

log_IEVAR_vals <- evaluate_acq_func_vectorized(acq_IEVAR_grid, input_mat=test_info$inputs, gp=gp_lpost,
                                               grid_points=test_info$inputs, log_scale=TRUE)

df_contour$log_IEVAR <- log_IEVAR_vals

selected_point_idx <- which.min(log_IEVAR_vals)
plt_log_IEVAR <- plot_heatmap(test_info$inputs, log_IEVAR_vals, points_mat=design_info$inputs, raster=TRUE, 
                              point_coords=test_info$inputs[selected_point_idx,], main_title="log IEVAR", 
                              points_mat_lab="Design point", point_coords_col="white")
                              
plot(plt_log_IEVAR)
```

```{r}
# IEVAR sequential acquisition loop. 
N_grid_points <- 300
grid_points_idx <- sample.int(nrow(test_info$inputs), size=N_grid_points)

results <- acquire_batch_input_sequentially(gp_lpost, "IEVAR_grid", N_batch=20, model_response_heuristic="none", 
                                            opt_method="grid", f_exact=lpost_true, reoptimize_hyperpar=FALSE, 
                                            grid_points=test_info$inputs[grid_points_idx,], 
                                            candidate_grid=test_info$inputs[grid_points_idx,])
```


```{r}
# Plot acquisitions. 

df_acq_points <- data.frame(results$input_batch)
colnames(df_acq_points) <- c("u1", "u2")
df_acq_points$order <- 1:nrow(df_acq_points)

plt_acq <- plt + geom_text(data=df_acq_points, mapping=aes(x=u1, y=u2, label=order), color="black")
plot(plt_acq)                           
            
```

```{r}
# Same as above but using the integrated variance criterion applied directly to the lpost emulator. 

results_IVAR <- acquire_batch_input_sequentially(gp_lpost, "IVAR_grid", N_batch=20, model_response_heuristic="none", 
                                                 opt_method="grid", f_exact=lpost_true, reoptimize_hyperpar=FALSE, 
                                                 grid_points=test_info$inputs[grid_points_idx,], 
                                                 candidate_grid=test_info$inputs[grid_points_idx,])

df_acq_points_IVAR <- data.frame(results_IVAR$input_batch)
colnames(df_acq_points_IVAR) <- c("u1", "u2")
df_acq_points_IVAR$order <- 1:nrow(df_acq_points_IVAR)

plt_acq_IVAR <- plt + geom_text(data=df_acq_points_IVAR, mapping=aes(x=u1, y=u2, label=order), color="black")
plot(plt_acq_IVAR)   


```

```{r}
# Same as above but using the maximum variance criterion applied directly to the lpost emulator. 
results_neg_var <- acquire_batch_input_sequentially(gp_lpost, "neg_var", N_batch=20, model_response_heuristic="none", 
                                                    opt_method="grid", f_exact=lpost_true, reoptimize_hyperpar=FALSE, 
                                                    candidate_grid=test_info$inputs)

df_acq_points_neg_var <- data.frame(results_neg_var$input_batch)
colnames(df_acq_points_neg_var) <- c("u1", "u2")
df_acq_points_neg_var$order <- 1:nrow(df_acq_points_neg_var)

plt_acq_neg_var <- plt + geom_text(data=df_acq_points_neg_var, mapping=aes(x=u1, y=u2, label=order), color="black")
plot(plt_acq_neg_var)   

```

```{r}
# Same as above but using the maximum variance criterion applied directly to the lpost emulator. 
results_neg_exp_var <- acquire_batch_input_sequentially(gp_lpost, "neg_exp_var", N_batch=20, model_response_heuristic="none", 
                                                        opt_method="grid", f_exact=lpost_true, reoptimize_hyperpar=FALSE, 
                                                        candidate_grid=test_info$inputs)

df_acq_points_neg_exp_var <- data.frame(results_neg_exp_var$input_batch)
colnames(df_acq_points_neg_exp_var) <- c("u1", "u2")
df_acq_points_neg_exp_var$order <- 1:nrow(df_acq_points_neg_exp_var)

plt_acq_neg_exp_var <- plt + geom_text(data=df_acq_points_neg_exp_var, mapping=aes(x=u1, y=u2, label=order), color="black")
plot(plt_acq_neg_exp_var)   

```




