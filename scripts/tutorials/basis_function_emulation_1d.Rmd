---
title: "basis_function_emulation_1d"
author: "Andrew Roberts"
date: "2024-10-22"
output: html_document
---

This file provides an introduction to the functions defined in 
`basis_function_emulation.r`. The functions contained therein are designed 
to approximate the output space of functions with high-dimensional outputs
using a basis representation. It also introduces the `gpWrapperSum` class, 
which encodes a model that is a weighted sum of basis functions, with the 
weights given by independent Gaussian processes (GPs). See 
[this](https://arob5.github.io/blog/2024/06/25/basis-func-emulator/) 
writeup for background on models of this form. Finally, it shows how 
`gpWrapperSum` can be combined with `llikEmulatorGPFwdGauss` to approximate 
the likelihood in a Bayesian inverse problem. This examples considers 
a 1d input (parameter) space, which conveniently allows for easy visualization
and illustration of the concepts. For an example with a multiple dimensional
parameter space, see `basis_function_emulation.Rmd`.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE)

library(lhs)
library(ggplot2)
library(data.table)
library(assertthat)

base_dir <- file.path("/projectnb", "dietzelab", "arober", "gp-calibration")
src_dir <- file.path(base_dir, "src")

# Various helper functions.
source(file.path(src_dir, "seq_design.r"))
source(file.path(src_dir, "gpWrapper.r"))
source(file.path(src_dir, "llikEmulator.r"))
source(file.path(src_dir, "inv_prob_test_functions.r"))
source(file.path(src_dir, "general_helper_functions.r"))
source(file.path(src_dir, "plotting_helper_functions.r"))
source(file.path(src_dir, "statistical_helper_functions.r"))
source(file.path(src_dir, "gp_helper_functions.r"))

# Main functions being illustrated.
source(file.path(src_dir, "basis_function_emulation.r"))
```

# Evaluate function at design points.
```{r}
# The function g() will be the test function used throughout this document.
inv_prob <- get_vsem_test_1d_traj()
g <- inv_prob$par_to_obs_op
```

```{r}
# Create an initial input design.
n_design <- 4L
U <- get_batch_design("LHS", N_batch=n_design, prior_params=inv_prob$par_prior)

# Run forward model g() at the design points.
G <- g(U)
```

```{r}
# Plot the function evaluations.
plt_design <- plot_curves_1d_helper(inv_prob$time_points, t(G), 
                                    plot_title="Model runs at design points",
                                    xlab="days", ylab="LAI")
              
plot(plt_design + theme(legend.position="none"))
```

# Run PCA on model outputs
```{r}
pca_list <- pca(G) 
sdev <- pca_list$sqrt_val # square root of eigenvalues of empirical covariance.
V <- pca_list$vec # eigenvectors.
m <- pca_list$mean
```

```{r}
# Plot empirical mean of model outputs.

plot_curves_1d_helper(inv_prob$time_points, matrix(m,ncol=1), 
                      plot_title="Mean of model outputs",
                      xlab="days", ylab="LAI")
```

```{r}
# Eigenvalue (scree) plot. Different scalings emphasize different features.
scree_plt <- get_scree_plot(pca_list, sdev=TRUE, log_scale=FALSE)
scree_plt_log <- get_scree_plot(pca_list, sdev=FALSE, log_scale=TRUE)

plot(scree_plt)
plot(scree_plt_log)
```

We define a cutoff and only include the dominant principal components. We 
visually compare true evaluations of $g(u)$ versus evaluations of $g(u)$ that 
have been approximated by projecting the functions outputs on the subspace 
spanned by the dominant eigenvalues. The plot considers a set of new 
"out-of-sample" $u$ values. 
```{r}
variance_threshold <- 0.999
pca_list_trunc <- truncate_pca_basis(pca_list, variance_threshold)

# Basis vectors and eigenvalues.
B <- pca_list_trunc$vec
sdev <- pca_list_trunc$sqrt_val

r <- length(sdev)
print(paste0("Number of eigenvectors retained: ", r))
```

```{r}
# Plot the dominant eigenvectors.
for(j in 1:r) {
  plt_eig <- plot_curves_1d_helper(inv_prob$time_points, B[,j, drop=FALSE], 
                                   plot_title=paste0("Eigenvector ", j),
                                   xlab="days", ylab="LAI")
  plot(plt_eig)
}
```

```{r}
# Define the approximate map g(), which evaluates g() and then projects 
# the output onto the subspace spanned by the dominant principal components.

g_hat <- function(U) {
  # Vectorized to work over `m` inputs in the rows of `U`. Returns 
  # matrix that approximates g(U).
  W <- eval_basis_weights(U, g, B, m)
  proj <- tcrossprod(B, W)
  t(add_vec_to_mat_cols(m, proj))
}

```


```{r}
# Test approximation at a set of test points.
n_test <- 5
U_test <- get_batch_design("LHS", N_batch=n_test, prior_params=inv_prob$par_prior)

# Evaluate true vs. approximate function.
G_test <- g(U_test)
G_test_hat <- g_hat(U_test)

# Plot comparison.
df_test <- rbindlist(list(data.table(G_test), data.table(G_test_hat)))
colnames(df_test) <- as.character(1:ncol(df_test))
df_test[, id := as.factor(c(1:n_test, 1:n_test))]
df_test[, type := c(rep("true",n_test), rep("approx",n_test))]
df_test <- melt.data.table(df_test, id.vars=c("id", "type"), 
                           variable.name="time", value.name="value")
df_test[, time := as.integer(time)]

plt_approx <- ggplot(df_test) + 
              geom_line(aes(x=time, y=value, color=id, linetype=type)) + 
              labs(xlab="days", ylab="LAI", title="True vs. Approximated g()")
plot(plt_approx)

```

Below we plot the basis weight functions $w_j(u)$ over a grid of $u$ values. 
```{r}
n_grid <- 100
test_info_grid <- get_init_design_list(inv_prob, "tensor_product_grid",
                                       N_design=n_grid)

w_U <- eval_basis_weights(test_info_grid$input, g, B, m=pca_list$mean)

for(j in 1:ncol(w_U)) {
  plt <- plot_curves_1d_helper(drop(test_info_grid$input), w_U[,j,drop=FALSE],
                               plot_title="w(u)", xlab="u", ylab=par)
  plot(plt)
}
```

# Gaussian Process (GP) Emulators
We now consider approximating the maps $u \mapsto w_j(u)$, $j = 1, \dots, r$
by independent GP emulators. This induces a multi-output GP emulator for 
$g(\cdot)$. Again, see [this](https://arob5.github.io/blog/2024/06/25/basis-func-emulator/)
writeup for more background.

## Fitting the Independent GPs
We consider fitting independent GPs to each weight function $w_j(u)$. We 
use zero mean GP priors with squared exponential (i.e., Gaussian) kernels.

```{r}
# Assemble design (training) data for independent GPs.
design_info <- list(input=U, W=project_orthog_scalar(G, B, m))

# Fit GPs. Note that the empirical mean of each weight vector `w_j(U)` is 
# already 0, but we still set `normalize_output=TRUE` to scale these to have 
# standard deviation 1.
gp_w <- gpWrapperKerGP(design_info$input, design_info$W, scale_input=TRUE,
                       normalize_output=TRUE)
gp_w$set_gp_prior("Gaussian", "quadratic", include_noise=FALSE)
gp_w$fit(multistart=10)
gp_w$summarize()
```

```{r}
# Validation points to test GP fit.
test_info_grid$W <- eval_basis_weights(test_info_grid$input, g, B, m) 
pred_list_w <- gp_w$predict(test_info_grid$input, return_cov=TRUE)
```

```{r}
# Summarize predictions at validation points.
gp_w$plot_pred_1d(test_info_grid$input, Y_new=test_info_grid$W, 
                  pred_list=pred_list_w, include_interval=TRUE, 
                  interval_method="CI", CI_prob=0.99)
```

## Using `gpWrapperSum` to combine the GPs
The `gpWrapperSum` class wraps accepts our object defining a set of independent 
GPs, as well as the basis vectors, to define a multi-output GP where the 
independent GPs are used as weights in a linear combination of the basis 
vectors.

```{r}
# Should update this to set the noise var in a more informed way.
noise_var <- sqrt(.Machine$double.eps)

gp_g <- gpWrapperSum(gp_w, B, shift=m, sig2=noise_var)
gp_g$summarize()
```


```{r}
# Plot samples G(x) at single input x vs. true trajectory G(x).
idcs <- sample(1:nrow(test_info_grid$input), size=5)

for(i in idcs) {
  plt <- gp_g$plot_samp(X_new=test_info_grid$input[i,,drop=FALSE], N_samp=10, 
                        use_cov=TRUE, g_func=g)
  plot(plt)
}
```

```{r}
# Plot one sample of G(x_i) at each input x_1, ..., x_M.
gp_g$plot_samp(X_new=test_info_grid$input[1:15,,drop=FALSE], use_cov=TRUE)
```

```{r}
# Plot pointwise predictive distribution for trajectory G(x) at different 
# inputs x. By pointwise, we mean no covariance is considered across output
# indices.

for(i in idcs) {
  plt <- gp_g$plot_pred(X_new=test_info_grid$input[i,,drop=FALSE], g_func=g)
  plot(plt)
}

```

## Creating a `llikEmulator` object using `gpWrapperSum`.
We know use the GP emulator for the forward model encoded by the `gpWrapperSum`
object, and use it to approximate the true forward model in the context of 
Bayesian inference. The induced approximation to the log-likelihood is encoded
by the below `llikEmulatorGPFwdGauss` object.

```{r}
llik_em <- llikEmulatorGPFwdGauss("llik_em", gp_g, y_obs=inv_prob$y, 
                                  Cov=inv_prob$sig2_model, 
                                  default_conditional=FALSE, 
                                  default_normalize=TRUE,
                                  par_names=inv_prob$par_names)
```

```{r}
# Compute emulator predictions at test points.
em_pred_list <- llik_em$predict_emulator(test_info_grid$input, return_mean=TRUE,
                                         return_var=FALSE, return_cov=FALSE,
                                         return_output_cov=TRUE)
llik_pred_list <- llik_em$predict(test_info_grid$input, return_mean=TRUE,
                                  return_var=TRUE, em_pred_list=em_pred_list)
lik_pred_list <- llik_em$predict_lik(test_info_grid$input, return_mean=TRUE,
                                     return_var=TRUE, em_pred_list=em_pred_list)
```

```{r}
# Log-likelihood surrogate distribution. 
llik_em$plot_pred_1d(test_info_grid$input, pred_list=llik_pred_list,
                     true_llik=test_info_grid$llik, interval_method="pm_std_dev", 
                     N_std_dev=2)
```

```{r}
# Likelihood surrogate distribution (on log scale).
llik_em$plot_pred_1d(test_info_grid$input, pred_list=lik_pred_list, 
                     plot_type="lik", log_scale=TRUE,
                     true_llik=test_info_grid$llik, include_interval=FALSE)

# Predictive (log) standard deviation.
plot(test_info_grid$input, 0.5*lik_pred_list$log_var, main="log std dev",
     xlab=inv_prob$par_names, ylab="log std dev", type="l")
```






