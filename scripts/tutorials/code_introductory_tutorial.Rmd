---
title: "Introduction to the Codebase"
author: "Andrew Roberts"
date: '2023-12-21'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, include=TRUE)
```


```{r}
# Should print the project directory "gp-calibration". If it doesn't, change the 
# RStudio settings in Tools -> Global Options -> R Markdown -> Set "Evaluate
# chunks in directory:" to "Project". 
getwd()
```


```{r, echo = FALSE, include = FALSE}
base_dir <- getwd()
src_dir <- file.path(base_dir, "src")

library(lhs)
library(hetGP)
library(mlegp)
library(ggplot2)
library(viridis)
library(parallel)
library(gridExtra)
library(data.table)
library(BayesianTools)

source(file.path(src_dir, "mcmc_calibration_functions.r"))
source(file.path(src_dir, "gp_emulator_functions.r"))
source(file.path(src_dir, "gp_mcmc_functions.r"))
source(file.path(src_dir, "sequential_design_optimization.r"))
source(file.path(src_dir, "sim_study_functions.r"))
```

# 1D Linear Gaussian Model 

We begin we the simplest possible example, the one-dimensional linear Gaussian model 
$$
\begin{align*}
\mathbf{y} &= \mathbf{g}u + \epsilon \\
\epsilon &\sim \mathcal{N}(0, \sigma_\epsilon^2) \\
u &\sim \mathcal{N}(\mu_0, \sigma^2_0),
\end{align*}
$$
where $\mathbf{g}, \mathbf{y} \in \mathbb{R}^T$ and the noise variance $\sigma_\epsilon^2$ is assumed to be known. This results in an unnormalized log posterior density 
$$
\ell^\pi(u) \propto -\frac{1}{2} \log(2\pi\sigma_{\epsilon}^2) - \frac{\Phi(u)}{2\sigma^2_\epsilon} - \frac{1}{2\sigma^2_0}(u - \mu_0)^2,
$$
where 
$$
\Phi(u) := ||\mathbf{y} - \mathbf{g}u||_2^2
$$
is the model-data misfit.


ecently Ive been setting a "global" random seed at the beginning of these documents, but also 
defining some "local" seeds that gives me a bit more control of the random number generator 
for certain portions of the analysis pipeline. 
```{r}
# Global seed.
set.seed(15)

# Local seeds. 
data_seed <- 23
design_seed <- 40
```


Here we generate synthetic data corresponding to the linear Gaussian inverse problem outlined above. 
This is a common test case (it is convenient for testing since the posterior distribution of the linear  
Gaussian model is known is closed-form) so I have written a function that does most of the work in setting 
up the example. 
```{r}
#
# Linear Gaussian Model Setup.
#

# Number of observations (i.e. N, the dimensionality of the vector y).
N_obs <- 100

# Define the linear forward model "g". In this case I'm just defining g to 
# make the forward model a linear combination of some periodic basis functions. 
# "sig2_eps" is the observation variance (the variance parameter in the 
# Gaussian likelihood). "Sig0" is the variance of the Gaussian prior on 
# the calibration parameter "u" (in the case where "u" is more than 
# one-dimensional this would be the prior covariance matrix). 
freq <- 1
g <- matrix(sin(2*pi*freq*seq(1, N_obs)/N_obs), ncol=1)
sig2_eps <- 1
sig2_0 <- matrix(1)
mu0 <- 0
```

Now we pass in the above parameters to the function which generates the synthetic data from 
the linear Gaussian model. Note that in the code I use "theta" often to mean the calibration 
parameter ("u" in the notation above).
```{r}

# The function gives a list with the data and some other information. 
linear_Gaussian_info <- generate_linear_Gaussian_test_data(data_seed, N_obs=N_obs, D=1,
                                                           Sig_theta=sig2_0, G=g, sig2_eps=sig2_eps)

# "computer_model_data" is a list that most importantly stores the forward model 
# in the inverse problem (I use "forward model" and "computer model") interchangeably. 
# The forward model is accessed via `computer_model_data$f`. This list also stores 
# the observed data "y" accessed via `computer_model_data$data_obs`. I would eventually 
# like to split the data into its own list separate from the forward model. 
computer_model_data <- linear_Gaussian_info$computer_model_data

# A data.frame containing information defining the prior distributions on the calibrations parameters. 
theta_prior_params <- linear_Gaussian_info$theta_prior_params

# Plot observed data and the ground truth (i.e. the signal without any noise). 
plot(1:N_obs, computer_model_data$data_obs, main="Ground Truth and Observed Data", 
     xlab="t", ylab="y")
lines(1:N_obs, computer_model_data$data_ref, col="red")
```

## The exact posterior 

In this toy problem we know the exact posterior $p(\mathbf{u}|\mathbf{y})$ in closed-form (we typically 
are interested in $p(\mathbf{u}, \sigma^2_{\epsilon}|\mathbf{y})$ but recall we are assuming the observation 
variance is fixed/known here). Below I use the known closed-form solution to produce a histogram 
of the posterior, then run an MCMC sampler to show that it recovers the true posterior provided 
it is run for long enough. 

### Produce iid posterior samples. 
```{r}
# Draw iid posterior samples. The true posterior mean and variance is stored in the
# `linear_Gaussian_info` list. 
N_samp_exact <- 50000
samp_exact <- rnorm(n=N_samp_exact, mean=drop(linear_Gaussian_info$true_posterior$mean), 
                    sd=sqrt(drop(linear_Gaussian_info$true_posterior$Cov)))

# Plot histogram.
hist(samp_exact, 30, xlab="u")
```

### Produce posterior samples via MCMC. 

We call this "exact" MCMC to contrast with the approximate MCMC schemes discussed later. 
```{r}
N_mcmc_exact <- 50000

# We set `learn_sig_eps` to FALSE since we are fixing sig2_eps here. This shouldn't 
# take more than ~30 seconds to run. 
mcmc_exact_list <- mcmc_calibrate_product_lik(computer_model_data=computer_model_data, 
                                              theta_prior_params=theta_prior_params, 
                                              learn_sig_eps=FALSE,
                                              sig_eps_init=sig2_eps, 
                                              N_mcmc=N_mcmc_exact)

# I've written some functions that format the MCMC output in a specific format, which I then use 
# for plotting and subsequent analysis. 
mcmc_samp_dt <- format_mcmc_output(samp_list=mcmc_exact_list[c("theta")], test_label="exact_mcmc")

```


```{r}
# We can now produce some MCMC plots using functions I have written. 
trace_plot <- get_trace_plots(mcmc_samp_dt)[[1]]
hist_plot <- get_hist_plot_comparisons(mcmc_samp_dt, xlab="samples", ylab="density", bins=30)[[1]]
                                        
plot(trace_plot)
plot(hist_plot)
```


```{r}
# We can actually append the exact iid samples to the MCMC samples object and then the plot 
# functions will automatically compare them. 

# Append exact samples. 
samp_exact_dt <- data.table(param_type="theta", 
                            itr=seq_len(N_samp_exact), 
                            param_name=computer_model_data$pars_cal_names, 
                            sample=samp_exact,
                            test_label="exact")
mcmc_samp_dt <- rbindlist(list(mcmc_samp_dt, samp_exact_dt), use.names=TRUE)

# Now plot the histograms. In general, these functions return a list of plots. In this 
# case there's only one plot but if there were more parameters or algorithms being 
# compared there would be more. We see that MCMC recovers the true posterior. 
hist_plots <- get_hist_plot_comparisons(mcmc_samp_dt, test_label_baseline="exact", 
                                        xlab="samples", ylab="density", bins=30)
for(plt in hist_plots) plot(plt)               
```















