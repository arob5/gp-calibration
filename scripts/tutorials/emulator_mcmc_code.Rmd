---
title: "Emulator MCMC Code"
author: "Andrew Roberts"
date: "2024-10-22"
output: html_document
---

This file provides an introduction to the functions defined in 
`gp_mcmc_functions.r`. The functions in this file implement various MCMC 
algorithms based on likelihood approximations given by a log-likelihood 
emulator object (as defined in `llikEmulator.r`). Note that the name 
`gp_mcmc_functions.r` will ultimately be changed to something like 
`emulator_mcmc_functions.r` to better reflect this.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE)

library(lhs)
library(ggplot2)
library(data.table)
library(assertthat)

base_dir <- file.path("/projectnb", "dietzelab", "arober", "gp-calibration")
src_dir <- file.path(base_dir, "src")

# Source required files.
source(file.path(src_dir, "seq_design.r"))
source(file.path(src_dir, "gpWrapper.r"))
source(file.path(src_dir, "llikEmulator.r"))
source(file.path(src_dir, "inv_prob_test_functions.r"))
source(file.path(src_dir, "general_helper_functions.r"))
source(file.path(src_dir, "statistical_helper_functions.r"))
source(file.path(src_dir, "plotting_helper_functions.r"))
source(file.path(src_dir, "gp_helper_functions.r"))
source(file.path(src_dir, "mcmc_helper_functions.r"))
source(file.path(src_dir, "gp_mcmc_functions.r"))
```

# Running Exact MCMC

## Setting up example inverse problem
```{r}
# Example inverse problem based on calibrating parameters of the 
# VSEM vegetation model.
inv_prob <- get_vsem_test_1()

# Exact likelihood (no emulation).
llik_obj_exact <- inv_prob$llik_obj
llik_exact <- llik_obj_exact$get_llik_func()

# Priors. 
par_prior <- inv_prob$par_prior

# Noise variance parameter (assumed fixed here).
sig2 <- inv_prob$sig2_model
```

## Summarizing the prior distribution.
```{r}
print(inv_prob$par_info)
prior_plots <- plot_prior_samp(par_prior)
for(plt in prior_plots) plot(plt)
```

## Exact MCMC
By exact MCMC we mean MCMC using an exact likelihood, rather than an emulated/
approximation likelihood. The `llikEmulator` class was primarily designed to 
encapsulate noisy (log) likelihood approximations, but can all encode exact 
(log) likelihoods, which is useful for code testing and validation. The 
`exact_llik` attribute in the `llikEmulator` class is a logical indicator of 
whether the likelihood being encapsulated represents an approximation or not.
```{r}
# Confirming this is an "exact" log-likelihood object.
print(paste0("Exact log-likelihood: ", llik_obj_exact$exact_llik))
```

## Running exact MCMC
We demonstrate running MCMC using the `mcmc_bt_wrapper()` function, which 
provides access to the MCMC samplers implemented in the BayesianTools package.
When `llik_em$exact_llik` is `TRUE`, then `mcmc_bt_wrapper()` will 
run MCMC using the true/exact likelihood, as expected. We access the 
`mcmc_bt_wrapper()` function through the interface provided by 
`run_mcmc_chains()`. For more information on running exact MCMC, see the 
tutorial `scripts/tutorials/inv_prob_exact_mcmc.Rmd`. 

```{r}
# Number of MCMC iterations.
n_mcmc <- 50000L

# Run exact MCMC.
mcmc_exact_list <- run_mcmc_chains("mcmc_bt_wrapper", llik_obj_exact, 
                                   defer_ic=TRUE, sampler="DEzs", n_chain=4L, 
                                   par_prior=par_prior, n_itr=n_mcmc, 
                                   test_label="exact", try_parallel=FALSE)

# Table storing MCMC samples.
samp_dt <- mcmc_exact_list$samp

# Table storing auxiliary info (log-likelihood and log-prior evaluations, 
# proposal covariance standard deviations).
info_dt <- mcmc_exact_list$info 
```


## MCMC Plotting and Diagnostics
We briefly demonstrate some of the plotting and MCMC diagnostic helper functions
that are designed to be compatible with the `samp_dt` `data.table`. These 
functions are explained in more detail in 
`gp-calibration/scripts/tutorials/mcmc_helper_functions.Rmd`.

### R-hat 
```{r}
# Burn-in cutoff.
burn_in_start <- ceiling(n_mcmc/2)

# Compute R-hat for each parameter.
rhat_list <- calc_R_hat(samp_dt, itr_start=burn_in_start)
rhat_list$R_hat_vals
```

### Trace Plots

First we plot trace plots for each parameter. 
```{r}
trace_plots <- get_trace_plots(samp_dt, itr_start=burn_in_start)
for(plt in trace_plots) plot(plt)
```

We can similarly produce trace plots for the associated log-likelihood 
evaluations. 
```{r}
trace_plots_llik <- get_trace_plots(info_dt, param_names="llik", 
                                    itr_start=burn_in_start)
plot(trace_plots_llik[[1]])
```

# Fitting a Log-Likelihood Emulator
We now construct an emulator (i.e., surrogate) for the log-likelihood of the 
Bayesian inverse problem. There are many ways to do this; for example, an 
approximation of the forward model induces an approximation of the likelihood.
Here we consider the approach of fitting a Gaussian process (GP) directly to
the log-likelihood itself.

## Generate design for fitting emulator.
```{r}
design_method <- "LHS"
n_design <- 100L

# Construct design.
design_info <- get_init_design_list(inv_prob, design_method, n_design)
```

## Fit GP emulator of log-likelihood.
```{r}
# Fit GP. 
gp_obj <- gpWrapperKerGP(design_info$input, matrix(design_info$llik, ncol=1), 
                         normalize_output=TRUE, scale_input=TRUE)
gp_obj$fit("Gaussian_plus_Quadratic", "constant", estimate_nugget=FALSE, 
           optimFun="nloptr::nloptr", trace=TRUE, multistart=10)
```

```{r}
# Create log-likelihood surrogate object.
llik_em <- llikEmulatorGP("em_llik_const_GaussQuad", gp_obj, 
                          lik_par=inv_prob$sig2_model, use_fixed_lik_par=TRUE,
                          default_conditional=FALSE, default_normalize=TRUE)
```

## Summarize GP fit
```{r}
# Validation test points.
n_test <- 400L
test_info <- get_init_design_list(inv_prob, design_method, n_test)
```

```{r}
# Summarize predictions at test points.
llik_em$plot_pred_validation(test_info$input, true_llik=test_info$llik, 
                             plot_type="llik", include_interval=TRUE, 
                             interval_method="pm_std_dev", N_std_dev=2, 
                             plot_title="Log-Likelihood Predictions")
```

```{r}
# One dimensional projections.
llik_em$plot_1d_projection(llik_func_true=llik_obj_exact)
```
# GP-Accelerated MCMC with Deterministic Likelihood Approximation
We start by considering approximate MCMC, whereby the true likelihood has been 
replaced by a deterministic approximation that can be evaluated pointwise.
This can also be viewed as MCMC with an approximate unnormalized posterior 
density. Given that the approximate unnormalized posterior density can be 
evaluated pointwise in this setting, one can consider applying any standard
MCMC algorithm for inference. An interesting setting is where the 
log-likelihood surrogate `llik_em` is stochastic. In this case, there is not 
a single obvious deterministic approximation that can be derived from 
`llik_em`. The below examples consider a few different such options, which are 
controlled by the `approx_type` argument. There are a couple different 
functions capable of running MCMC schemes with deterministic log-likelihood 
approximations. The first is `mcmc_gp_unn_post_dens_approx()`, which simply 
implemented an adaptive random walk Metropolis-Hastings algorithm. The 
function `mcmc_bt_wrapper()` is a wrapper that provides access to the 
samplers implemented in the `BayesianTools` package. We demonstrate the use 
of both of these options below. Given that the GP-approximated posteriors 
tend to be quite multimodal, we also emphasize different ways to initialize and 
post-process MCMC runs to deal with the multimodality. 

## Truncating the Prior
Before proceeding with MCMC, we mention an additional practical consideration. 
In general, extrapolation with GPs is challenging. Sometimes undesirable 
behavior can occur when MCMC chains wander outside of the design point bounds, 
and thus into regions that rely on GP extrapolations. One heuristic approach to 
deal with this is to simply define the prior support based on the design 
point bounds; i.e., to approximate the original prior with a truncated version.
This can be accomplished via the `truncate_prior()` function.
```{r}
par_prior_trunc <- truncate_prior(par_prior, design_info$bounds)
par_prior_trunc
```

We can simulate from the truncated and original priors to visually inspect 
the effect of the truncation.
```{r}
n_prior_samp <- 10000L
prior_samp <- as.data.table(sample_prior(par_prior, n=n_prior_samp))
prior_trunc_samp <- as.data.table(sample_prior(par_prior_trunc, n=n_prior_samp))
prior_samp[, test_label := "original"]
prior_trunc_samp[, test_label := "trunc"]
prior_compare_dt <- data.table::rbindlist(list(prior_samp, prior_trunc_samp), use.names=TRUE)
prior_compare_dt <- data.table::melt.data.table(prior_compare_dt, 
                                                id.vars="test_label", 
                                                variable.name="param_name", 
                                                value.name="sample")
prior_compare_dt[, `:=`(param_type=NA_character_, chain_idx=NA_integer_, 
                        itr=NA_integer_, param_name=as.character(param_name))]

prior_compare_plots <- get_hist_plot_comparisons(prior_compare_dt, 
                                                 test_label_baseline="original")

for(plt in prior_compare_plots) plot(plt)
```

## Plug-In Mean Approximation
We start by considering the simplest option, `approx_type = "mean"`, which 
implies that the mean of the log-likelihood surrogate is simply plugged-in in 
place of the exact log-likelihood. Notice that we emphasize this is a 
"plug-in mean" since this differs from taking the mean of the likelihood 
emulator (see "marginal" approximation below).

We start by defining an initial proposal covariance to be used by adaptive 
Metropolis schemes.
```{r}
cov_prop_init <- cov(design_info$input)
```

We next define initial conditions for the MCMC chains. The `run_mcmc_chains()`
method can also accomplish this behind the scenes (see function comments), 
but we define them explicitly here to show how this is done. The below code 
attempts to place the initial conditions in "important" regions of parameter 
space. The first two initial conditions are set to the design inputs with 
the largest observed log-likelihood values. The second two are set by sampling 
a set of inputs from the prior, computing the plug-in mean likelihood 
approximation for each input, then extracting the two inputs with the largest 
likelihood predictions.

```{r}
n_chain <- 4L
par_init_mean <- get_mcmc_ic(llik_em, par_prior_trunc, n_ic=n_chain, 
                             n_ic_by_method=c(design_max=2, approx_max=2),
                             design_info=design_info, approx_type="mean", 
                             n_test_inputs=500L)

print("Initial conditional for MCMC chains for plug-in mean approximation:")
print(par_init_mean)

print("Associated approximate log-likelihood values:")
llik_em$calc_lik_approx("mean", input=par_init_mean)
```

```{r}
# Run MCMC with plug-in mean approximation.
mcmc_mean_approx_list <- run_mcmc_chains("mcmc_gp_unn_post_dens_approx", llik_em, 
                                         approx_type="mean", par_init=par_init_mean,
                                         sampler="DEzs", n_chain=4L, try_parallel=FALSE,
                                         par_prior=par_prior, n_itr=n_mcmc, 
                                         test_label="mean", cov_prop=cov_prop_init)

# Append to samples table.
samp_dt_new <- combine_samp_dt(samp_dt, mcmc_mean_approx_list$samp)
samp_dt <- combine_samp_dt(samp_dt, mcmc_mean_approx_list$samp)
```

### Compute R-hat for each parameter.
Given the multimodality of the posterior, it is not surprising that running 
random walk Metropolis-Hastings for the plug-in mean approximation results in 
the chains getting stuck in local modes of the distribution. The R-hat 
values (which by default use between-chain information) can look really bad 
here. 

```{r}
rhat_list <- calc_R_hat(samp_dt, test_labels="mean", itr_start=burn_in_start)
rhat_list$R_hat_vals
```

An alterantive perspective is to accept the fact that getting the different 
chains to mix is very difficult, and instead focus on "exploring" the dominant 
modes of the posterior with different chains. In this case, we might instead 
focus on verifying that each chain individually is well-mixed with respect 
to a local region of the posterior. To do so, we can compute "within chain" 
R-hat statistics.

```{r}
rhat_list_within_chain <- calc_R_hat(samp_dt, test_labels="mean", 
                                     within_chain=TRUE, itr_start=burn_in_start)
rhat_list_within_chain$R_hat_vals
```

### Log-likelihood Evaluations
To compare the modes, it is also useful to investigate the log-likelihood 
evaluations associated with the MCMC chains.

```{r}
trace_plots_llik_mean <- get_trace_plots(mcmc_mean_approx_list$info, 
                                         itr_start=burn_in_start,
                                         param_names="llik",
                                         test_labels="mean")

for(plt in trace_plots_llik_mean) plot(plt)
```

### Trace Plots
```{r}
trace_plots_mean <- get_trace_plots(samp_dt, itr_start=burn_in_start, 
                                    test_labels="mean")
for(plt in trace_plots_mean) plot(plt)
```

### Histogram Comparisons relative to Exact
```{r}
hist_plots <- get_hist_plot_comparisons(samp_dt_test, test_label_baseline="exact",
                                        test_labels="mean",
                                        itr_start=burn_in_start)
for(plt in hist_plots) plot(plt)
```

## Quantile Approximation
We next consider `approx_type = "quantile"`, in which the approximate 
likelihood is set to a quantile of the likelihood surrogate. This requires 
choosing a quantile level, which is called `alpha` below. Note that 
for a GP log-likelihood surrogate, the choice of `alpha = 0.5` is equivalent 
to the plug-in mean approximation. We choose a larger quantile below, which 
serves to "inflate" the posterior in regions where the GP is more 
uncertain.

```{r}
# Run MCMC with quantile approximation.
alpha <- 0.9
mcmc_quantile_approx_list <- run_mcmc_chains("mcmc_bt_wrapper", llik_em, 
                                             defer_ic=TRUE, approx_type="quantile", 
                                             alpha=0.9, sampler="DEzs", 
                                             n_chain=4L, par_prior=par_prior, 
                                             n_itr=n_mcmc, test_label="quantile", 
                                             try_parallel=FALSE)

# Append to samples table.
samp_dt <- combine_samp_dt(samp_dt, mcmc_quantile_approx_list$samp)
```

### Trace Plots
```{r}
burn_in_start <- c(burn_in_start, quantile=25000L)
trace_plots <- get_trace_plots(samp_dt, itr_start=burn_in_start, test_labels="quantile")
for(plt in trace_plots) plot(plt)
```

### Histogram Comparisons relative to Exact
```{r}

hist_plots <- get_hist_plot_comparisons(samp_dt, test_label_baseline="exact",
                                        test_labels="quantile", plot_type="freqpoly", 
                                        itr_start=burn_in_start)

for(plt in hist_plots) plot(plt)
```

```{r}
# TODO: add option to center all of the lines plotted, since its only in a relative 
# sense that the values matter.
# TODO: add legend
llik_em$plot_1d_projection(input_names_proj="KEXT", 
                           plot_type="lik_approx", 
                           approx_type=c("mean", "quantile"), 
                           alpha=0.99, llik_func_true=llik_obj_exact)
```


## Marginal Approximation


# Noisy MCMC
We next consider algorithms that only require the ability to sample from a 
stochastic log-likelihood surrogate, rather than the requirement of having 
a closed-form unnormalized posterior density approximation.

## Monte Carlo within Metropolis
We start with a Monte Carlo within Metropolis (mcwm) algorithm, which is 
a modification of (adaptive) Metropolis-Hastings where the log-likelihood 
evaluations are replaced by samples from the log-likelihood emulator, such 
that the samples are produced independently at each iteration.

```{r}
# TODO: check that parameter ordering isn't getting messed up. Seems like it 
#       might be. Maybe not actually, but should check this. 


# Run mcwmh-joint.
samp_exact <- select_mcmc_samp_mat(samp_dt, test_label="exact", param_type="par", 
                                   itr_start=burn_in_start)
cov_prop_init <- cov(samp_exact)
par_init1 <- colMeans(samp_exact)
par_init2 <- design_info$input[which.max(design_info$llik),]

mcmc_mcwmh_list <- run_mcmc_chains("mcmc_noisy_llik", llik_em, 
                                   par_prior=par_prior, n_chain=2L, n_itr=n_mcmc, 
                                   test_label="mcwmh-joint", try_parallel=FALSE, 
                                   use_joint=TRUE, cov_prop=cov_prop_init, 
                                   adapt_cov_prop=FALSE)

# Append to samples table.
samp_dt_test <- mcmc_mcwmh_list$samp
samp_dt <- combine_samp_dt(samp_dt, mcmc_mcwmh_list$samp)
```

### Trace Plots
```{r}
burn_in_start <- c(burn_in_start, `mcwmh-joint`=25000L)
trace_plots <- get_trace_plots(samp_dt_test, itr_start=25000L, 
                               test_labels="mcwmh-joint", overlay_chains=TRUE)
for(plt in trace_plots) plot(plt)
```












