# test_gp_r_stan_conversion.R
# Tests the Gaussian Process (GP) Stan functions by comparing their output to 
# that of supported GP packages in R. The Stan functions assume a fixed form 
# of the Gaussian covariance function, which may differ from the exact 
# parameterization used by various R packages. This file tests that the 
# parameterizations are correctly mapped. 
#
# Andrew Roberts

# TODO:
#   1.) Compare Stan function gp_approx() to function defined in R (as in likelihood comparison script).
#   2.) 3-way comparison: predictive means/variances using Stan code, R implementation, and mlegp functions. 
#       Can help diagonose why mlegp results are choppy. 

library(cmdstanr)
library(rstan)
library(mlegp)

options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)

source("stan.helper.functions.R")
source("gaussian.process.functions.R")

# -----------------------------------------------------------------------------
# Generate Test Data: Toy example
# -----------------------------------------------------------------------------

# # Dimension of input space
k <- 2

# Design points
nx <- 20
x <- seq(0, 2*pi, length = nx)
X.df <- expand.grid(x, x)
X <- as.matrix(X.df)
y <- sin(X[,1] + 4*X[,2])
N <- length(y)

# Test points
N.pred.x <- 30
x.pred <- seq(-0.5, 2*pi + 0.5, length = N.pred.x)
X.pred.df <- expand.grid(x.pred, x.pred)
X.pred <- as.matrix(X.pred.df)
N.pred <- nrow(X.pred)

# Plot test data (will only work if k = 1)
# matplot(X, y, pch = 20, cex = 2, xlab = 'Design Points', ylab = 'Model Value')


# -----------------------------------------------------------------------------
# R package: mlegp
# -----------------------------------------------------------------------------

# Fit GP model
gp.mlegp <- mlegp(X, y, nugget.known = 0, constantMean = 1)

# Map kernel parameters to Stan parameterization
gp.stan.params <- create.gp.params.list(gp.mlegp, "mlegp")
gp.obj <- create.gp.obj(gp.mlegp, "mlegp", X, y)

# Compile and run Stan code
stan.model.path <- "test_gp_r_stan_conversion.stan"
model <- stan_model(stan.model.path)
# model <- cmdstan_model(stan.model.path)

stan.list <- as.list(c(gp.stan.params, 
                       list(N = N, 
                            N_pred = N.pred, 
                            k = k,
                            X = X, 
                            y = as.vector(y), 
                            X_pred = X.pred)))

stan.fit <- sampling(model, data = stan.list, warmup = 0, iter = 1, chains = 1, 
                     seed = 494838, refresh = 4000, algorithm = "Fixed_param")
stan.output <- extract(stan.fit)
                     
# Test the covariance matrix generated by the design points
K.mlegp <- calcVarMatrix(X, gp.mlegp$beta, gp.mlegp$a, gp.mlegp$nugget, gp.mlegp$sig2, 0, gp.mlegp$numObs)
K.stan <- stan.output$K_out[1,,]
print(paste0("Cov matrices K(X) equal: ", all.equal(K.mlegp, K.stan)))

# Test the covariance matrix generated by the test/prediction points
K.pred.mlegp <- calcVarMatrix(X.pred, gp.mlegp$beta, gp.mlegp$a, gp.mlegp$nugget, gp.mlegp$sig2, 0, N.pred)
K.pred.stan <- stan.output$K_pred[1,,]
print(paste0("Cov matrices K(X_pred) equal: ", all.equal(K.pred.mlegp, K.pred.stan)))

# Test the expression inv(K(X)) %*% (y - gp_mean). mlegp directly inverts the covariance matrix, while my Stan code
# instead opts for a Cholesky decomposition. If the above tests pass but the results are significantly 
# different here, then this points to an ill-conditioned matrix, which will mean the predictive means
# and variances will differ in the below tests. 
K.inv.y.mlegp <- as.vector(solve(K.mlegp) %*% (y - gp.mlegp$Bhat))
K.inv.y.stan <- as.vector(stan.output$K_inv_y_out)
print(paste0("Vector K_inv_y equal: ", all.equal(K.inv.y.mlegp, K.inv.y.stan)))

# Investigating possible ill-conditioning of the covariance matrix
K.mlegp.svd <- svd(K.mlegp)
K.mlegp.cond.num <- max(K.mlegp.svd$d) / min(K.mlegp.svd$d)
K.inv.K.mlegp <- solve(K.mlegp) %*% K.mlegp

print(paste0("Condition number for inverse of K(X): ", K.mlegp.cond.num))
print(paste0("K.inv.K equal to identity matrix: ", all.equal(K.inv.K.mlegp, diag(rep(1.0, N)))))

# Test the covariances between the design and prediction points
K.cross.mlegp <- matrix(NA, nrow = N, ncol = N.pred)
for(j in seq(1, N.pred)) {
  K.cross.mlegp[,j] <- gp.mlegp$sig2 * calcCorOneObs(X, gp.mlegp$beta, gp.mlegp$a, X.pred[j,])
}

K.cross.stan <- stan.output$K_cross[1,,]
print(paste0("Cov matrices K(X, X_pred) equal: ", all.equal(K.cross.mlegp, K.cross.stan)))

# Test that mean prediction gives same results
gp.pred.mlegp <- predict(gp.mlegp, newData = X.pred)
gp.pred.stan <- t(stan.output$mean_pred)
print(paste0("Max absolute error in mean predictions: ", max(abs(gp.pred.mlegp - gp.pred.stan))))

# Test variance predictions
gp.var.mlegp <- predict(gp.mlegp, X.pred, se.fit = TRUE)$se.fit^2 + gp.mlegp$nugget
gp.var.stan <- t(stan.output$var_pred)
print(paste0("Max absolute error in variance predictions: ", max(abs(gp.var.mlegp - gp.var.stan))))


# -----------------------------------------------------------------------------
# Comparing Stan functions to R functions
# -----------------------------------------------------------------------------

# Test the covariance matrix generated by the design points
K.r <- cov_exp_quad(X, rho = gp.obj$gp_rho, alpha = gp.obj$gp_alpha) + diag(rep(gp.obj$nugget, gp.obj$N))
print(paste0("Stan and R cov matrices K(X) equal: ", all.equal(K.r, K.stan)))

# Test the covariances between the design and test points
K.cross.r <- cov_exp_quad(X, X.pred, rho = gp.obj$gp_rho, alpha = gp.obj$gp_alpha)
print(paste0("Stan and R cov matrices K(X, X_pred) equal: ", all.equal(K.cross.r, K.cross.stan)))

# Test the predictive means
gp.pred.r <- predict_gp(X.pred, gp.obj)
print(paste0("Stan and R predictive means equal: ", all.equal(as.vector(gp.pred.r$mean), as.vector(gp.pred.stan))))

# Test the predictive variances
print(paste0("Stan and R predictive variances equal: ", all.equal(as.vector(gp.pred.r$var), as.vector(gp.var.stan))))

# Test that R predictive covariance matrix recovers the predictive variances calculated above
pred.cov.r<- predict_gp(X.pred, gp.obj, pred.cov = TRUE)$cov
print(paste0("R predictive variances equal to diagonal of predictive covariance matrix: ", 
             all.equal(as.vector(gp.pred.r$var), diag(pred.cov.r))))


# -----------------------------------------------------------------------------
# General Stan tests
# -----------------------------------------------------------------------------

# Compare user defined covariance function to Stan's internal cov_exp_quad()
K.stan.test <- stan.output$K_stan_test[1,,]
K.test <- stan.output$K_out_test[1,,]
print(paste0("Cov matrix agrees with Stan's cov_exp_quad(): ", all.equal(K.test, K.stan.test)))

# Test that predictive mean interpolates observed data (when nugget is 0). If nugget is not 0, then 
# the predictive mean will not exactly equal the observed value. 
mean.design.stan <- as.vector(stan.output$mean_design)
print(paste0("Max absolute error of mean at design points: ", max(abs(mean.design.stan - y))))

# Test that predictive variance is equal to the nugget at observed data points
var.design.stan <- as.vector(stan.output$var_design)
print(paste0("Max absolute error of variance at design points: ", max(abs(var.design.stan - gp.mlegp$nugget))))


# -----------------------------------------------------------------------------
# Log-normal MGF approximations (user-defined Stan functions)
# -----------------------------------------------------------------------------

stan.model.code <- 
"

functions {
#include gaussian_process_functions.stan
}

data {
  int N_mgf_test;
  vector[N_mgf_test] mgf_test_vals;
  int mgf_num_eval; 
  real mgf_tol; 
  real mgf_M; 
  real log_norm_mu; 
  real log_norm_sigma; 
}

generated quantities {
  vector[N_mgf_test] mgf_test_out; 
  for(i in 1:N_mgf_test) {
    mgf_test_out[i] = lognormal_mgf_numerical_approx(mgf_test_vals[i], log_norm_mu, log_norm_sigma, mgf_num_eval, mgf_tol, mgf_M);
  }
}

"

# Run Stan code to approximate log-normal MGF
test.vals <- c(.0001, .001, .01, .1, 1, 10, 100, 1000)
stan.mgf.list <- list(N_mgf_test = length(test.vals), 
                      mgf_test_vals = test.vals, 
                      mgf_num_eval = 100000, 
                      mgf_tol = 1e-10, 
                      mgf_M = 1, 
                      log_norm_mu = log(200),
                      log_norm_sigma = log(20))

stan.mgf.model <- stan_model(model_code = stan.model.code)
stan.mgf.fit <- stan.fit <- sampling(stan.mgf.model, data = stan.mgf.list, warmup = 0, iter = 1, chains = 1, 
                                     seed = 494838, refresh = 4000, algorithm = "Fixed_param")
stan.mgf.output <- as.vector(extract(stan.mgf.fit)$mgf_test_out)

# To check approximation, estimate MGF via Monte Carlo
lnorm.mgf.estimate <- function(s, mu, sigma, N = 100000) {
  lnorm.samples <- rlnorm(N, meanlog = mu, sdlog = sigma)
  return(mean(exp(-s * lnorm.samples)))
}

mgf.mc <- sapply(test.vals, function(s) lnorm.mgf.estimate(s, stan.mgf.list$log_norm_mu, stan.mgf.list$log_norm_sigma))


