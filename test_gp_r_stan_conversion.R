# test_gp_r_stan_conversion.R
# Tests the Gaussian Process (GP) Stan functions by comparing their output to 
# that of supported GP packages in R. The Stan functions assume a fixed form 
# of the Gaussian covariance function, which may differ from the exact 
# parameterization used by various R packages. This file tests that the 
# parameterizations are correctly mapped. 
#
# Andrew Roberts

library(rstan)
library(mlegp)

options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)

source("stan.helper.functions.R")

# -----------------------------------------------------------------------------
# Generate Test Data: Toy example
# -----------------------------------------------------------------------------

# Design points
N <- 8
X <- matrix(seq(0, 2*pi, length=N), ncol=1)
y <- sin(X)

# Test points
N.pred <- 100
X.pred <- matrix(seq(-0.5, 2*pi + 0.5, length=N.pred), ncol=1)

# Plot test data
matplot(X, y, pch = 20, cex = 2, xlab = 'Design Points', ylab = 'Model Value')



# -----------------------------------------------------------------------------
# Generate Test Data:
#   Mimicking data generation process for computer simulation model with  
#   Gaussian noise.
# -----------------------------------------------------------------------------

# Function acting as the computer model
f <- function(u) {
  return( (7.0 + 4.0*u + u^2)/147 )
}


# True calibration parameter value
u <- 4

# Define test values of calibration parameter at which to evaluate the GP 
u.min <- -2
u.max <- 10
u.extrapolation.width <- 5

n.u <- 1000
u.vals <- seq(u.min - u.extrapolation.width, u.max + u.extrapolation.width, length = n.u)

# Simulate observed data
n <- 1000
sigma <- .1
tau <- 1 / sigma^2
y.obs <- matrix(NA, nrow = n, ncol = 1)
y.obs[,1] <- rnorm(n, f(u), sigma)

# Evaluate exact likelihood for comparison during subsequent tests

# Log unnormalized isotropic multivariate normal density
dmvnorm.log.unnorm <- function(y, u, tau) {
  n.y <- length(y)
  mu.vec <- rep(f(u), n.y)
  (n.y/2)*log(tau) - (tau/2)*sum((y - mu.vec)^2)
}


likelihood.exact <- matrix(NA, nrow = n.u, ncol = 1)
for(i in seq(1, n.u)) {
  likelihood.exact[i, 1] <- dmvnorm.log.unnorm(y.obs, u.vals[i], tau)
}

# Design matrix, evaluate model at design points
N <- 10
design.points <- c(seq(-2, 0, length=4), seq(0.5, 2, length=7), 
                   seq(2.5, 5, length=3), seq(5.5, 8, length=3), seq(8.5, 10, length=10))
N <- length(design.points)
X <- matrix(design.points, ncol = 1)
y_model <- f(X)

# Define the sufficient statistic (SS)
SS <- matrix(NA, nrow = N, ncol = 1)
for(i in seq(1, N)) {
  SS[i, 1] <- sum((y_model[i] - y.obs[,1])^2)
}



# -----------------------------------------------------------------------------
# R package: mlegp
# -----------------------------------------------------------------------------

# Fit GP model
gp.mlegp <- mlegp(X, y, nugget.known = 0, constantMean = 1)

# Map kernel parameters to Stan parameterization
gp.stan.params <- create.gp.params.list(gp.mlegp, "mlegp")

# Compile and run Stan code
stan.list <- as.list(c(gp.stan.params, 
                       list(N = N, 
                            N_pred = N.pred, 
                            X = X, 
                            y = y, 
                            X_pred = X.pred)))
                     




# Test the covariance matrix generated by the design points
K.mlegp <- calcVarMatrix(X, gp$beta, gp$a, gp$nugget, gp$sig2, 0, gp$numObs)
print(paste0("Cov matrices K(X) equal: ", all.equal(K.r, K.mlegp)))

# 2.) Test that mean prediction gives same results
gp.mean.test <- predict_mean(X, X, SS[, 1],
                             rho = rho,
                             alpha = alpha,
                             sigma = sigma,
                             mu = mu)

gp.pred <- predict(gp, se.fit = TRUE)
gp.mean <- gp.pred$fit
max(abs(as.vector(gp.mean.test) - as.vector(gp.mean)))

# 3.) Test that standard error prediction gives same results
c <- calcCorOneObs(X, gp$beta, gp$a, X[1,]) * alpha^2
c.test <- K(as.matrix(X[1,]), X, rho, alpha)
print(paste0("Covariance between design and one test point equal: ", all.equal(c, c.test)))

K.inv.test <- solve(K(X, X, rho, alpha) + diag(rep(sigma^2, nrow(X))))
print(paste0("Inverse cov matrices equal: ", all.equal(K.inv.test, gp.fits[[1]]$invVarMatrix)))

v <- calcPredictionError(gp, X[1,], nugget = gp$nugget)
v.test <- sqrt(predict_var(X[1,,drop=FALSE], X, SS[,1], rho, alpha, sigma))
print(paste0("Predictive standard errors equal (single test point): ", all.equal(v, v.test)))

gp.se.test <- sqrt(diag(predict_var(X, X, SS[, 1], rho, alpha, sigma)))
gp.se <- as.vector(gp.pred$se.fit)
max(abs(gp.se.test - gp.se))



# TODO: test mlegp 'eps' jitter vs. R 'eps' jitter

