---
title: "Code Validation"
author: "Andrew Roberts"
date: '2023-04-08'
output: html_document
---

```{r, echo = FALSE, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(lhs)
library(hetGP)
library(mlegp)
library(ggplot2)
library(gridExtra)
library(data.table)
library(BayesianTools)

source("mcmc_calibration_functions.r")
source("gp_emulator_functions.r")
```

# This document contains code validation for emulator/calibration related functions. 

## Testing GP Transformation Functions
#### Comparing Rectified Gaussian moment calculations with sample estimates. 
```{r}
Gaussian_means <- c(0, -.5)
Gaussian_vars <- c(1, 2)

# Compute means and variances
moments <- transform_GP_to_rectified_GP(Gaussian_means, Gaussian_vars)

# Monte Carlo approximation of means and variances
N_samp <- 10000000
samp <- matrix(nrow = N_samp, ncol = length(Gaussian_means))
for(j in seq_along(Gaussian_means)) {
  samp[,j] <- pmax(rnorm(N_samp, mean = Gaussian_means[j], sd = sqrt(Gaussian_vars[j])), 0)
}
sample_means <- colMeans(samp)
sample_vars <- colMeans(samp^2) - sample_means^2

print("Means computed:")
print(moments$mean)

print("Sample means:")
print(sample_means)

print("Variances:")
print(moments$var)

print("Sample vars:")
print(sample_vars)

```
#### Comparing two different functions that compute truncated Gaussian moments. 
```{r}
print("Method 1:")
print(transform_GP_to_truncated_GP(Gaussian_means, Gaussian_vars))

print("Method 2:")
print(compute_zero_truncated_Gaussian_moments(Gaussian_means, Gaussian_vars))
```
#### Validating quantile function for rectified Gaussian distribution using simulation. 
```{r}
p_vals <- seq(0.1, 1.0, by = 0.1)

# Sample quantiles.
print("Sample Quantiles:")
for(j in seq(1, ncol(samp))) {
  print(quantile(samp[,j], p_vals))
}

# Computed quantiles.
print("Computed Quantiles:")
computed_quantiles <- sapply(p_vals, function(p) quantile_rectified_norm(p, mean = Gaussian_means, sd = sqrt(Gaussian_vars)))
print(computed_quantiles)
```

# One-Dimensional GP Tests

### No output transformation
```{r}

# Training and testing input data. 
prior_params <- data.frame(dist = "Uniform", param1 = -10, param2 = 10)
train_data <- get_input_design(12, prior_params, "LHS", scale_inputs = TRUE, order_1d = TRUE)
test_data <- get_input_design(151, prior_params, "grid", scale_inputs = TRUE, param_ranges = train_data$input_bounds)

# Latent function; producing training and testing response data. 
f <- function(x) cos(x) + sin(x)
train_data$outputs <- f(train_data$inputs)
test_data$outputs <- f(test_data$inputs)
train_data[c("outputs_normalized", "output_stats")] <- prep_GP_training_data(Y = train_data$outputs, normalize_Y = TRUE)[c("Y", "output_stats")]
test_data[["outputs_normalized"]] <- normalize_output_data(test_data$outputs, output_stats = train_data$output_stats)

# Fit GP. 
gp <- fit_GP(train_data$inputs_scaled, train_data$outputs_normalized, "mlegp", "Gaussian")$fit

# Predict at test locations. 
gp_pred <- predict_GP(test_data$inputs_scaled, gp, "mlegp", denormalize_predictions = TRUE, output_stats = train_data$output_stats) 
           
# Plot GP.
gp_plot <- plot_gp_fit_1d(test_data$inputs, test_data$outputs, train_data$inputs, train_data$outputs, gp_pred$mean, gp_pred$var,
                          xlab = "X", ylab = "Y", main_title = "GP Plot", CI_prob = 0.9)
print(gp_plot)
```
### Truncated Gaussian 
```{r}
# Plot truncated GP.
gp_trunc_plot <- plot_gp_fit_1d(test_data$inputs, test_data$outputs, train_data$inputs, train_data$outputs, gp_pred$mean, gp_pred$var,
                          xlab = "X", ylab = "Y", main_title = "Truncated GP Plot", CI_prob = 0.9, transformation_method = "truncated")
print(gp_trunc_plot)
```

### Rectified Gaussian 
```{r}
# Plot rectified GP.
gp_rect_plot <- plot_gp_fit_1d(test_data$inputs, test_data$outputs, train_data$inputs, train_data$outputs, gp_pred$mean, gp_pred$var,
                          xlab = "X", ylab = "Y", main_title = "Rectified GP Plot", CI_prob = 0.9, transformation_method = "rectified")
print(gp_rect_plot)
```

### Log-Normal Process (shifting training data up so everything is positive). 
```{r}
# Add log-transformed responses to training and testing input data. 
shift <- abs(min(train_data$outputs)) + 1
train_data[["log_outputs"]] <- log(train_data[["outputs"]] + shift)
train_data[c("log_outputs_normalized", "log_output_stats")] <- prep_GP_training_data(Y = train_data$log_outputs, normalize_Y = TRUE)[c("Y", "output_stats")]
test_data[["log_outputs"]] <- log(test_data[["outputs"]] + shift)
test_data[["log_outputs_normalized"]] <- normalize_output_data(test_data$log_outputs, output_stats = train_data$log_output_stats)

# Fit log-normal process. 
lnp <- fit_GP(train_data$inputs_scaled, train_data$log_outputs_normalized, "mlegp", "Gaussian")$fit

# Predict at test locations. 
lnp_pred <- predict_GP(test_data$inputs_scaled, lnp, "mlegp", denormalize_predictions = TRUE, output_stats = train_data$log_output_stats) 

# Plot rectified GP.
lnp_plot <- plot_gp_fit_1d(test_data$inputs, test_data$outputs + shift, train_data$inputs, train_data$outputs + shift, lnp_pred$mean, lnp_pred$var,
                           xlab = "X", ylab = "Y", main_title = "LNP Plot", CI_prob = 0.9, transformation_method = "LNP")

print(lnp_plot)
```

### Multiple Output Independent GP Testing using VSEM test case. 
```{r}

# VSEM data. 
computer_model_data <- generate_vsem_test_case(1)

# Prior. 
theta_prior_params <- computer_model_data$ref_pars[computer_model_data$pars_cal_sel,]
theta_prior_params[, "dist"] <- "Uniform"
theta_prior_params[,"param1"] <- 0.4
theta_prior_params[,"param2"] <- 0.6
theta_prior_params <- theta_prior_params[, c("dist", "param1", "param2")]

# Get design points. 
N_train <- 5
design_list <- get_input_output_design(N_train, theta_prior_params, computer_model_data$ref_pars, computer_model_data$pars_cal_sel, computer_model_data$data_obs, 
                                       computer_model_data$PAR_data, computer_model_data$output_vars, design_method = "LHS", transformation_method = "LNP")

# Get test points. 
N_test <- 301
test_list <- get_input_output_design(N_test, theta_prior_params, computer_model_data$ref_pars, computer_model_data$pars_cal_sel, computer_model_data$data_obs, 
                                     computer_model_data$PAR_data, computer_model_data$output_vars, 
                                     param_ranges = design_list$input_bounds, output_stats = design_list$output_stats, design_method = "grid", 
                                     transformation_method = "LNP", log_output_stats = design_list$log_output_stats)

# Fit GPs. 
gp_fits <- fit_independent_GPs(design_list$inputs_scaled, design_list$log_outputs_normalized, "hetGP", "Gaussian")$fits

# Predict with GPs. 
gp_pred_list <- predict_independent_GPs(test_list$inputs_scaled, gp_fits, "hetGP", include_cov_mat = FALSE, denormalize_predictions = TRUE,
                                        output_stats = design_list$log_output_stats)

# Plot GPs. 
for(j in seq_along(gp_pred_list)) {
  gp_plot <- plot_gp_fit_1d(test_list$inputs, test_list$outputs[,j], design_list$inputs, design_list$outputs[,j], gp_pred_list[[j]]$mean, gp_pred_list[[j]]$var, log_scale = FALSE,
                            xlab = "Parameter", ylab = "SSR", main_title = computer_model_data$output_vars[j], CI_prob = 0.9, transformation_method = "LNP")
  print(gp_plot)
}

```

# Sampling GPs

#### Sample at input locations, not taking into account correlations so will not be smooth. 
```{r}
j <- 2
gp_sample <- sample_GP_pointwise(gp_pred_list[[j]]$mean, gp_pred_list[[j]]$var, transformation_method = "LNP")
plot(test_list$inputs, gp_sample, type = "l")
```

#### Sample independent GPs at single input location
```{r}
input_loc_inx <- floor(nrow(test_list$inputs) / 2)
print(paste0("Input location: ", test_list$inputs[input_loc_inx,1]))

N_samples <- 10000
samples <- matrix(nrow = N_samples, ncol = length(gp_pred_list))
for(i in seq_len(N_samples)) {
  samples[i,] <- sample_independent_GPs_pointwise(gp_pred_list, transformation_methods = "LNP", idx_selector = input_loc_inx)
}

# Histograms
for(j in seq_along(gp_pred_list)) {
  print(hist(samples[,j], 50, main = paste0("Output #", j, " at input loc ", test_list$inputs[input_loc_inx,1])))
}
```

# Testing MCMC Calibration Algorithm (Exact, No GP Approximation). 

## Linear Gaussian Posterior Test
Here we choose the forward model to be linear, with Gaussian observation noise, and a Gaussian prior on the calibration parameters. This 
results in a closed-form Gaussian posterior which can be used to validate the MCMC schemes. 
```{r}
N_obs <- 1000
d <- 2
G <- matrix(rnorm(N_obs*d), nrow = N_obs, ncol = d)
Sig_theta <- diag(c(0.1, 1.0))
sig2_eps <- 1.0
linear_Gaussian_data <- generate_linear_Gaussian_test_data(random_seed = 3, N_obs = N_obs, D = d, sig2_eps = sig2_eps, 
                                                           Sig_theta = Sig_theta, G = G, pars_cal_sel = NULL)
computer_model_data <- linear_Gaussian_data$computer_model_data
theta_prior_params <- linear_Gaussian_data$theta_prior_params
true_post <- linear_Gaussian_data$true_posterior
```


```{r}
N_mcmc <- 50000
samp <- mcmc_calibrate(computer_model_data, theta_prior_params, diag_cov = TRUE, learn_Sig_eps = FALSE, 
                       Sig_eps_init = diag(1.0), N_mcmc = N_mcmc, adapt_cov_method = "AM", adapt_scale_method = "MH_ratio")
```

```{r}
burn_in <- 25000
plot(samp$theta[burn_in:N_mcmc,1], type="l")
plot(samp$theta[burn_in:N_mcmc,2], type="l")
```


```{r}
# Sample moments. 
print("MCMC Estimates:")
print("Cov:")
print(stats::cov(samp$theta[burn_in:N_mcmc,]))

print("Mean:")
print(colMeans(samp$theta[burn_in:N_mcmc,]))
```

```{r}
# True moments. 
print("True Posterior Moments:")
print("Cov:")
print(true_post$Cov)

print("Mean:")
print(true_post$mean)
```
```{r}
# Samples from true posterior. 
N_exact_samples <- N_mcmc - burn_in + 1
L_Cov_exact <- t(chol(true_post$Cov))
theta_samp_exact <- t(true_post$mean[,1] + L_Cov_exact %*% matrix(rnorm(N_exact_samples*d), nrow = d, ncol = N_exact_samples))
```

```{r}
theta_samp_keep <- samp$theta[burn_in:N_mcmc,]

# Histograms.
for(j in seq(1,d)) {
  hist_plot <- get_hist_plot(list(theta_samp_exact, theta_samp_keep), col_sel = j, bins = 50, xlab = computer_model_data$pars_cal_name[j],
                             main_title = "Histogram of MCMC Samples", data_names = c("True", "MCMC"), 
                             vertical_line = computer_model_data$theta_true[j])
  print(hist_plot)
}

```

```{r}
# 2d KDE contour plots. 

countour_plots <- get_2d_density_contour_plot(list(theta_samp_exact, theta_samp_keep), col_sel = c(1,2),  
                                              xlab = computer_model_data$pars_cal_name[1], ylab = computer_model_data$pars_cal_name[2], 
                                              main_titles = paste0("2D KDE Countours: ", c("True", "MCMC")))
for(j in seq_along(countour_plots)) print(countour_plots[[j]])
```

## Linear Gaussian Posterior Test, also learning the noise variance.

### Highly informative prior. 
```{r}
# Prior on noise variance. 
Sig_eps_prior_params <- list(dist = "IG", 
                             IG_shape = 10000, 
                             IG_scale = 10000)
```


```{r}
samp <- mcmc_calibrate(computer_model_data, theta_prior_params, diag_cov = TRUE, learn_Sig_eps = TRUE, 
                       Sig_eps_prior_params = Sig_eps_prior_params, N_mcmc = N_mcmc, adapt_cov_method = "AM", adapt_scale_method = "MH_ratio")
```

```{r}
burn_in <- 25000
plot(samp$theta[burn_in:N_mcmc,1], type="l")
plot(samp$theta[burn_in:N_mcmc,2], type="l")
plot(samp$Sig_eps[burn_in:N_mcmc,1], type="l")
```
```{r}
# Gibbs sampler for comparison
Gibbs_samp <- MCMC_Gibbs_linear_Gaussian_model(computer_model_data, theta_prior_params, Sig_eps_prior_params, N_mcmc = N_mcmc)
```


```{r}
# Sample moments. 
print("MCMC Calibration Code Estimates:")
print("theta Cov:")
print(stats::cov(samp$theta[burn_in:N_mcmc,]))

print("theta Mean:")
print(colMeans(samp$theta[burn_in:N_mcmc,]))

print("sig2_eps Var:")
print(mean(samp$Sig_eps[burn_in:N_mcmc,1]^2) - mean(samp$Sig_eps[burn_in:N_mcmc,1])^2)

print("sig2_eps Mean:")
print(mean(samp$Sig_eps[burn_in:N_mcmc,1]))

```

```{r}
# True moments. 
print("Gibbs Sampling Estimates:")
print("theta Cov:")
print(stats::cov(Gibbs_samp$theta[burn_in:N_mcmc,]))

print("theta Mean:")
print(colMeans(Gibbs_samp$theta[burn_in:N_mcmc,]))

print("sig2_eps Var:")
print(mean(Gibbs_samp$sig2_eps[burn_in:N_mcmc,1]^2) - mean(Gibbs_samp$sig2_eps[burn_in:N_mcmc,1])^2)

print("sig2_eps Mean:")
print(mean(Gibbs_samp$sig2_eps[burn_in:N_mcmc,1]))
```

```{r}

# Histograms.
for(j in seq(1,d)) {
  hist_plot <- get_hist_plot(list(Gibbs_samp$theta[burn_in:N_mcmc,], samp$theta[burn_in:N_mcmc,]), col_sel = j, bins = 50, xlab = computer_model_data$pars_cal_name[j],
                             main_title = "Histogram of MCMC Samples: theta", data_names = c("Gibbs Baseline", "MCMC Calibration Function"), 
                             vertical_line = computer_model_data$theta_true[j])
  print(hist_plot)
}

```

```{r}

# Histograms.

sig2_eps_hist_plot <- get_hist_plot(list(Gibbs_samp$sig2_eps[burn_in:N_mcmc,,drop=FALSE], samp$Sig_eps[burn_in:N_mcmc,,drop=FALSE]), bins = 50, xlab = "sig2_eps",
                                    main_title = "Histogram of MCMC Samples: sig2_eps", data_names = c("Gibbs Baseline", "MCMC Calibration Function"), 
                                    vertical_line = diag(computer_model_data$Sig_eps))
print(sig2_eps_hist_plot)


```






