---
title: "GP Emulator Error Metrics"
author: "Andrew Roberts"
date: '2023-03-12'
output: html_document
---

```{r, echo = FALSE, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(lhs)
library(hetGP)
library(mlegp)
library(data.table)
library(BayesianTools)

source("mcmc_calibration_functions.r")
```


# Some Initial Thoughts on Error Metrics to Evaluate GP Emulators
I suppose there are really two questions here: 1.) developing error metrics to use in tests when the likelihood parameters are known and 
2.) developing error metrics that can be used in practice to evaluate the emulator fit. Though we are emulating the sum of squared errors $T(\theta)$ 
(or its log $L(\theta)$) what we are really interested in is the effect of the emulator approximation on the posterior distribution 
$$\pi(\theta) \propto \pi_0(\theta)p(Y|\theta) = \pi_0(\theta)\mathcal{L}(\theta) = \pi_0(\theta)\prod_{j = 1}^{p} \mathcal{N}_{N_j}(Y_j|f_j(\theta), \sigma_j^2 I_{N_j})$$
Note that I've started writing $N_j$ to specify that sample sizes may be different across outputs, given the fact that significant data imbalances are something we will have to be thinking about. 

In particular, the posterior approximation $\hat{\pi}(\theta) \propto \pi_0(\theta)\hat{\mathcal{L}}(\theta)$ is utilized in the MCMC calibration algorithm in two places. 

1. The Accept-Reject ratio in Metropolis-Hastings.
2. Sampling from an Inverse Gamma in the Gibbs update for the observation variance.  

I briefly review this below. 

## Review of MCMC scheme
We assume priors 
$$\sigma_j^2 \overset{ind}{\sim} \mathcal{IG}(a_j, b_j)$$
which results in a conditional posterior $p(\sigma_j^2|\theta, Y) = \mathcal{IG}(\sigma_j^2|a_j + N_j/2, b_j + T_j(\theta)/2)$ and thus the opportunity for a Metropolis-within-Gibbs scheme. 

### Updating $\theta$
Conditional on $\Sigma := \text{diag}(\sigma_1^2, \dots, \sigma_p^2)$, the Metropolis step for updating $\theta$ results in an acceptance ratio (where $q(\cdot, \cdot)$ is the proposal density), 
$$
\begin{align*}
\alpha(\theta, \theta^\prime) &= \frac{p(\theta^\prime|Y, \Sigma)q(\theta^\prime, \theta)}{p(\theta|Y, \Sigma)q(\theta, \theta^\prime)} \\
                              &= \frac{\pi_0(\theta^\prime)p(Y|\theta^\prime, \Sigma)q(\theta^\prime, \theta)}{\pi_0(\theta)p(Y|\theta, \Sigma)q(\theta, \theta^\prime)} \\
                              &= \frac{\pi_0(\theta^\prime)q(\theta^\prime, \theta) \prod_{j = 1}^{p} \mathcal{N}_{N_j}(Y_j|f_j(\theta^\prime), \sigma_j^2 I_{N_j})}{\pi_0(\theta)q(\theta, \theta^\prime) \prod_{j = 1}^{p} \mathcal{N}_{N_j}(Y_j|f_j(\theta), \sigma_j^2 I_{N_j})} \\
                              &= \frac{\pi_0(\theta^\prime)q(\theta^\prime, \theta)}{\pi_0(\theta)q(\theta, \theta^\prime)}\prod_{j = 1}^{p} \exp\left\{-\frac{1}{2\sigma_j^2} (T_j(\theta^\prime) - T_j(\theta))\right\} \\
                              &= \frac{\pi_0(\theta^\prime)q(\theta^\prime, \theta)}{\pi_0(\theta)q(\theta, \theta^\prime)} \exp\left\{ -\frac{1}{2} \sum_{j = 1}^{p} \left(\frac{T_j(\theta^\prime) - T_j(\theta)}{\sigma_j^2}\right)\right\}
\end{align*}
$$
I will denote the predictive distribution of the GP using asterisks, as $T^* \sim \mathcal{GP}(\mu^*, k^*)$ or likelwise for the log L2 error $L^*$. The current PEcAn approach is to approximate $\alpha(\theta, \theta^\prime)$ by sampling from the GP emulator

$$
\begin{align*}
&\tilde{T}_j^\prime \sim \mathcal{N}(\mu^*_j(\theta^\prime), k_j^*(\theta^\prime)) \\
&\tilde{T}_j \sim \mathcal{N}(\mu^*_j(\theta), k_j^*(\theta))
\end{align*}
$$

and then replacing $T_j(\theta^\prime)$ and $T_j(\theta)$ with $\tilde{T}_j^\prime$ and $\tilde{T}_j$, respectively. We have also discussed accounting for GP predictive covariance by drawing these samples jointly 
$$
\begin{pmatrix}T_j(\theta^\prime) \\ T_j(\theta) \end{pmatrix} \sim \mathcal{N}_2\left(\mu_j^*\begin{pmatrix} \theta^\prime \\ \theta\end{pmatrix}, k_j^*\begin{pmatrix} \theta^\prime \\ \theta\end{pmatrix} \right)
$$

Note that a deterministic, rather than a sampling-based approach is also possible here since we know the distribution

$$T^*_j(\theta^\prime) - T^*_j(\theta) \sim \mathcal{N}\left(\mu_j^*(\theta^\prime) - \mu_j^*(\theta), k_j^*(\theta^\prime) + k_j^*(\theta) - 2k_j^*(\theta^\prime, \theta)\right)$$
In fact, under the GP approximations, the whole exponential term in $\alpha(\theta, \theta^\prime)$ is log-normal

$$\exp\left\{ -\frac{1}{2} \sum_{j = 1}^{p} \left(\frac{T^*_j(\theta^\prime) - T^*_j(\theta)}{\sigma_j^2}\right)\right\} \sim \mathcal{LN}$$
so we could consider an analytical approximation like

$$
\hat{\alpha}(\theta, \theta^\prime) = \frac{\pi_0(\theta^\prime)q(\theta^\prime, \theta)}{\pi_0(\theta)q(\theta, \theta^\prime)} \mathbb{E} \left[\exp\left\{ -\frac{1}{2} \sum_{j = 1}^{p} \left(\frac{T^*_j(\theta^\prime) - T^*_j(\theta)}{\sigma_j^2}\right)\right\}\right]
$$

When we instead emulate the log of the $T_j$ using a GP, these analytical expressions go away, but the sampling approach still works. 

### Updating $\Sigma$
Recall that the observation variances are updated by sampling 
$$\sigma_j^2 \sim \mathcal{IG}(a_j + N_j/2, b_j + T_j(\theta)/2)$$
The current PEcAn approach is to sample $\tilde{T}_j \sim T_j^*(\theta)$ and then sample from the resulting approximate distribution
$$ \sigma_j^2 \sim \mathcal{IG}(a_j + N_j/2, b_j + \tilde{T}_j/2) $$
The same procedure can of course be used if we instead emulate the log of $T_j$. 

## Thinking about the posterior approximation
Here we take a step back to think a bit more generally about how errors in approximating the sum of squared errors can propagate to 
errors in the posterior approximation. I'm considering the likelihood parameters $\sigma_j^2$ to be fixed here. 
For now, let $\pi(\theta)$ denote the unnormalized posterior
$$ 
\begin{align*}
\pi(\theta) &:= \pi_0(\theta) \prod_{j = 1}^{p} \mathcal{N}_{N_j}(Y_j|f_j(\theta), \sigma_j^2 I_{N_j}) \\
            &= \pi_0(\theta) \left[\prod_{j = 1}^{p} \frac{1}{\sqrt{2\pi\sigma_j^2}}\right]\exp\left(-\frac{1}{2} \sum_{j = 1}^{p} \frac{T_j(\theta)}{\sigma_j^2} \right) \\
            &= C\pi_0(\theta)\exp\left(-\frac{1}{2} \sum_{j = 1}^{p} \frac{T_j(\theta)}{\sigma_j^2} \right) \\
\end{align*}
$$
Now let's consider additive errors in the GP approximation of the sum of squared errors. In particular, define the residuals
$$ \epsilon_j(\theta) := T_j(\theta) - T_j^*(\theta)$$
Note that $T_j^*(\theta)$ is random so $\epsilon_j(\theta)$ is as well. Now we consider the posterior approximation with these errors.
$$
\begin{align*}
\hat{\pi}(\theta) &= C\pi_0(\theta)\exp\left(-\frac{1}{2} \sum_{j = 1}^{p} \frac{T_j^*(\theta)}{\sigma_j^2} \right) \\
                  &= C\pi_0(\theta)\exp\left(-\frac{1}{2} \sum_{j = 1}^{p} \frac{T_j(\theta) - \epsilon_j(\theta)}{\sigma_j^2} \right) \\
                  &= C\pi_0(\theta)\exp\left(-\frac{1}{2} \sum_{j = 1}^{p} \frac{T_j(\theta)}{\sigma_j^2} \right) \exp\left(\frac{1}{2}\sum_{j = 1}^{p} \frac{\epsilon_j(\theta)}{\sigma_j^2}\right) \\
                  &= \pi(\theta)\exp\left\{E(\theta; \epsilon, \sigma)\right\}
\end{align*}
$$
where I have defined $E(\theta; \epsilon, \sigma) := \frac{1}{2}\sum_{j = 1}^{p} \frac{\epsilon_j(\theta)}{\sigma_j^2}$. Thus,
$$ \frac{\hat{\pi}(\theta)}{\pi(\theta)} = \exp\left\{E(\theta; \epsilon, \sigma)\right\}$$
So, in the aggregate, if the GPs underestimate the squared error (i.e. the sum of the $\epsilon_j(\theta)$ is positive, modulo the effect of the variances) then the resulting estimate of $\hat{\pi}(\theta)$ will, unsurprisingly, be too large. The ratio of the approximate to the true posterior at $\theta$ varies exponentially in the additive error $\epsilon_j(\theta)$, for each output $j$. The rate of these exponential deviations is governed by the variances $\sigma_j^2$. If the signal-to-noise ratio is very low for output $j$, then $\sigma_j^2$ will be large, and the approximation error induced in the posterior will be more forgiving. We can also consider the relative error
$$ \frac{\pi(\theta) - \hat{\pi}(\theta)}{\pi(\theta)} = 1 - \exp\left\{E(\theta; \epsilon, \sigma)\right\}$$



## Some general notes on potential metrics

* We want the metric to take into account the GP predictive distribution 
* Also want the metric to take into account the true posterior (in the case when it is known)
* We want the error metric to target the use cases (two places where the approximation is used in MCMC)
* Multiple outputs/data constraints define the likelihood, with the error for each output being approximated by independent GPs. When the observation variances are unknown, it is not clear how to weight the emulator error for each output. 

## Metrics for numerical experiments (known posterior)
I begin by focusing on a single output $(p = 1)$ for simplicity. 

### Incorporating the Predictive GP Distribution 
The approximation $T^*(\theta)$ to the sum of squared errors $T(\theta)$ is a stochastic approximation, with 
distribution $T^*(\theta) \sim \mathcal{N}(\mu^*(\theta), k^*(\theta))$. Thus, in evaluating the emulator performance we seek 
to integrate whatever error metric we're using over this randomness. To begin, we consider pointwise measures of error 
between $T^*(\theta)$ and $T(\theta)$. The natural measure to use here is to evaluate the the predictive density of 
$T^*(\theta)$ at the point $T(\theta)$:

$$ p_{T^*(\theta)}(T(\theta)) = \mathcal{N}(T(\theta)|\mu^*(\theta), k^*(\theta)) = \frac{1}{\sqrt{2\pi k^*(\theta)}} \exp\left(-\frac{(T(\theta) - \mu^*(\theta))^2}{2k^*(\theta)} \right)$$

We can also consider predictive density of other functions of $T^*(\theta)$, in order to define specialized error metrics that capture the error in specific use cases of the approximation. For example, note that in the acceptance ratio $\alpha(\theta, \theta^\prime)$, we require an approximation 
of the difference $T(\theta^\prime) - T(\theta)$. The predictive density of this quantity is given by 

$$ p_{T^*(\theta^\prime) - T^*(\theta)}(T(\theta^\prime) - T(\theta)) = \mathcal{N}\left(T(\theta^\prime) - T(\theta)|\mu^*(\theta^\prime) - \mu^*(\theta), k^*(\theta^\prime) + k^*(\theta) - 2k(\theta^\prime, \theta)\right)$$

We can actually extend this idea to derive the distribution of $\alpha^*(\theta, \theta^\prime)$, the approximation of $\alpha(\theta, \theta^\prime)$ resulting from replacing $T$ with $T^*$. Using properties of Gaussians and log-normals we have,

$$ \alpha^*(\theta, \theta^\prime) \sim \mathcal{LN}\left(\log\left[\frac{\pi_0(\theta^\prime)q(\theta^\prime, \theta)}{\pi_0(\theta) q(\theta, \theta^\prime)}\right] - \frac{1}{2\sigma^2}\left[\mu^*(\theta^\prime) - \mu^*(\theta)\right], \frac{k^*(\theta^\prime, \theta)}{\sigma^2} - \frac{1}{2\sigma^2} \left[k^*(\theta^\prime) + k^*(\theta)\right]\right)$$
So larger values of $p_{\alpha^*}(\alpha(\theta, \theta^\prime))$ indicate a better approximation of $\alpha(\theta, \theta^\prime)$. 

### Weighting by the True Posterior
We also want to weight the error based on the relative frequencies of the $\theta$, captured by the true posterior $\pi(\theta)$. I first consider the error between $T^*(\theta)$ and $T(\theta)$ averaged over $\theta \sim \pi$. The pointwise error on a $\theta$-by-$\theta$ basis will be measured by the predictive density $p_{T^*(\theta)}(T(\theta))$, as detailed above. In this sense, the average error is given by the integral 

$$ 
\begin{align*}
\mathbb{E}_\pi \left[p_{T^*(\theta)}(T(\theta))\right] &= \int p_{T^*(\theta)}(T(\theta)) \pi(\theta) d\theta \\
                                                       &= \int \frac{1}{\sqrt{2\pi k^*(\theta)}} \exp\left(-\frac{(T(\theta) - \mu^*(\theta))^2}{2k^*(\theta)} \right) \pi(\theta) d\theta \\
                                                       &= \frac{1}{\sqrt{2\pi}} \int \frac{1}{\sqrt{k^*(\theta)}} \exp\left(-\frac{(T(\theta) - \mu^*(\theta))^2}{2k^*(\theta)} \right) \pi(\theta) d\theta
\end{align*}
$$
Or we can consider error on the log-scale via the expected log-density
$$
\begin{align*}
\mathbb{E}_\pi \left[\log p_{T^*(\theta)}(T(\theta))\right] &= -\frac{1}{2}\log(2\pi) - \int \log k^*(\theta) + \frac{1}{2}\left(\frac{T(\theta) - \mu^*(\theta)}{\sqrt{k^*(\theta)}}\right)^2 \pi(\theta)d\theta
\end{align*}
$$

We notice that the predictive variance $k^*(\theta)$ plays two roles. The error is smaller when $\int \log k^*(\theta) \pi(\theta) d\theta$ is small; all else equal, this would favor emulators with smaller predictive variances, even if those predictive variances were overconfident. However, the predictive variance appears again in the second term of the integral as $\frac{1}{k^*(\theta)}$, meaning that the squared deviations are penalized less when the predictive variance is larger. Define the loss 

$$ \ell_{T^*}(\theta) := \log k^*(\theta) + \frac{1}{2} \left(\frac{T(\theta) - \mu^*(\theta)}{\sqrt{k^*(\theta)}}\right)^2$$
which modifies $\log p_{T^*(\theta)}$ by dropping the constant and then flipping the sign so that now smaller $\ell_{T^*}(\theta)$ is better. To estimate $\mathbb{E}[\ell_{T^*}(\theta)]$ we can utilize a Monte Carlo estimate
$$ 
\begin{align*}
\left(\overline{\ell}_{T^*}\right)_K &:= \sum_{i = 1}^{K} \ell_{T^*}(\theta_i) && \theta_i \overset{iid}{\sim} \pi \\
                                     &= \sum_{i = 1}^{K} \left[\log k^*(\theta_i) + \frac{1}{2} \left(\frac{T(\theta_i) - \mu^*(\theta_i)}{\sqrt{k^*(\theta_i)}}\right)^2\right]
\end{align*}
$$
The above derivations have been for a single output, but since we are considering independent GPs the predictive density is a product of the predictive densities for each output, so we can simply add the losses. 
$$ \int \sum_{j = 1}^{p} \ell_{T^*} \pi(\theta) d\theta = \sum_{j = 1}^{p} \int \ell_{T^*}(\theta) \pi(\theta) d\theta$$




Next we consider an error metric for the difference $T(\theta^\prime) - T(\theta)$, where $\theta^\prime \sim q(\theta, \cdot)$. Recalling the pointwise predictive density $p_{T^*(\theta^\prime) - T^*(\theta)}(T(\theta^\prime) - T(\theta))$, we again take the log, drop the constant, and flip the sign in defining the loss
$$
\begin{align*}
\ell_{T^*}(\theta, \theta^\prime) &:= \frac{1}{2}\log k^*_{\theta, \theta^\prime} + \frac{1}{2k^*_{\theta, \theta^\prime}} \left[(T(\theta^\prime) - T(\theta)) - (\mu^*(\theta^\prime) - \mu^*(\theta))\right]^2 \\
                                  &= \frac{1}{2}\log k^*_{\theta, \theta^\prime} + \frac{1}{2}\left(\frac{T(\theta^\prime) - \mu^*(\theta^\prime)}{\sqrt{k^*_{\theta, \theta^\prime}}}\right)^2 + \frac{1}{2}\left(\frac{T(\theta) - \mu^*(\theta)}{\sqrt{k^*_{\theta, \theta^\prime}}}\right)^2 - \frac{(T(\theta^\prime) - \mu^*(\theta^\prime))(T(\theta) - \mu^*(\theta))}{k^*_{\theta, \theta^\prime}}
\end{align*}
$$

where $k^*_{\theta, \theta^\prime} := k^*(\theta^\prime) + k^*(\theta) - 2k^*(\theta^\prime, \theta)$. Instead of directly integrating $\ell_{T^*}(\theta, \theta^\prime)$ with respect to $\pi$ we must recall that we are interested in the error introduced in the Metropolis accept-reject ratio. Thus, we should consider 
$\theta \sim \pi$ and $\theta^\prime \sim q(\theta, \cdot)$. This amounts to defining the error as
$$ \int \int \ell_{T^*}(\theta, \theta^\prime) q(\theta, \theta^\prime)\pi(\theta) d\theta^\prime d\theta$$



### GP Emulator

I'll just consider a single-output case for simplicity, where the true vector of squared errors evaluated over some validation points $\theta_1, \dots, \theta_M$, 
$$ T := \left[T(\theta_1), \dots, T(\theta_M) \right]^T$$
is approximated by 
$$ T^* = \left[T^*(\theta_1), \dots, T^*(\theta_M) \right]^T$$ 
where $T^* \sim \mathcal{GP}(\mu^*, k^*)$. Note that the approximation $T^*$ is stochastic, so metrics will be defined by integrating over the randomness in some manner. A first-pass idea that takes into account the true posterior $\pi(\theta)$ and the pointwise predictive GP distributions (but not GP predictive covariances) is: 
$$ 
\begin{align*}
\sum_{i = 1}^{M} \pi(\theta_i) \mathbb{E}_{T^*(\theta_i)}\left[\left(T(\theta_i) - T^*(\theta_i)\right)^2\right] &= \sum_{i = 1}^{M} \pi(\theta_i) \left[k^*(\theta_i) + (\mu_*(\theta_i) - T(\theta_i))^2 \right] \\ 
\end{align*}
$$
However, this has the effect of punishing emulators with larger predictive variances, even if those predictive variances are well-calibrated; in other words, it can favor over-confident emulators. Moreover, the concept of "well-calibrated variances" isn't really even well-defined in the setting of emulating deterministic functions. An alternative metric that treats the predictive variances in the opposite way is
$$ \sum_{i = 1}^{M} \pi(\theta_i) \left(\frac{T(\theta_i) - \mu^*(\theta_i)}{\sqrt{k^*(\theta_i)}}\right)^2$$ 
Now the squared deviation is penalized more when the predictive variance is smaller. 

Other than this weighted L2 type approach, we might consider metrics quantifying the posterior approximation to $\pi(\theta)$ directly, or alternatives that specialize the error metric to the use cases of the approximation detailed above. 

# Numerical Tests

## VSEM Test case 1
```{r}
# Synthetic data generation
random_seed_1 <- 1
test1_list <- generate_vsem_test_1(random_seed_1)
```

```{r, echo = FALSE}
# Emulator settings
emulator_settings <- data.frame(gp_lib = c("mlegp", "hetGP"), 
                                target = c("SSR", "SSR"), 
                                kernel = "Gaussian", 
                                scale_X = TRUE, 
                                normalize_y = TRUE)
print(emulator_settings)
```

```{r, echo = FALSE}
# Priors 
theta_prior_params <- test1_list$ref_pars[test1_list$pars_cal_sel,]
theta_prior_params[, "dist"] <- "Uniform"
theta_prior_params[,"param1"] <- theta_prior_params[,"lower"]
theta_prior_params[,"param2"] <- theta_prior_params[,"upper"]
theta_prior_params <- theta_prior_params[, c("dist", "param1", "param2")]

print(theta_prior_params)
```

```{r}
# Training/Design data
N_design_points <- 10
N_test <- 1000
train_test_data <- get_train_test_data(N_design_points, N_test, theta_prior_params, joint = FALSE, extrapolate = TRUE, 
                                       ref_pars = test1_list$ref_pars, pars_cal_sel = test1_list$pars_cal_sel,
                                       data_obs = test1_list$data_obs, PAR = test1_list$PAR_data, 
                                       output_vars = test1_list$output_vars, 
                                       scale_X = TRUE, normalize_Y = TRUE, log_SSR = FALSE)
```


```{r, include = FALSE}
# Fit GPs
gp_lib <- emulator_settings[1, "gp_lib"]
gp_kernel <- emulator_settings[1, "kernel"]
gp_fits <- fit_independent_GPs(train_test_data$X_train_preprocessed, train_test_data$Y_train_preprocessed, gp_lib, gp_kernel)$fits
```

```{r, echo = FALSE}
#
# True posterior at test points
#

# Grid points for numerical approximation of integral over loss
rng <- qunif(c(.01, .99), min = theta_prior_params$param1, max = theta_prior_params$param2)
theta_grid <- matrix(seq(rng[1], rng[2], length.out = N_test), ncol = 1)
theta_grid_preprocessed <- scale_input_data(theta_grid, train_test_data$input_bounds)
d_theta <- theta_grid[2,] - theta_grid[1,]

# True SSR and log-likelihood evaluations
model_outputs_list <- lapply(theta_grid, function(theta) run_VSEM(theta, test1_list$ref_pars, test1_list$pars_cal_sel, test1_list$PAR_data, test1_list$output_vars))
SSR_test <- calc_SSR(test1_list$data_obs, model_outputs_list, na.rm = TRUE) 
llik_test_per_output <- sapply(seq(1, nrow(SSR_test)), 
                               function(i) llik_Gaussian_SSR(SSR_test[i,], 
                                                             diag(test1_list$Sig_eps), test1_list$n_obs, normalize = TRUE))
llik_test<- colSums(llik_test_per_output)
llik_train<- sapply(seq(1, nrow(train_test_data$Y_train)), 
                                function(i) llik_product_Gaussian_SSR(train_test_data$Y_train[i,], 
                                                                      diag(test1_list$Sig_eps), test1_list$n_obs, normalize = TRUE))

# Log-prior evaluations
lprior_theta_test <- sapply(seq(1, nrow(theta_grid)), function(i) calc_lprior_theta(theta_grid[i,], theta_prior_params))
lprior_theta_train <- sapply(seq(1, nrow(train_test_data$X_train)), 
                             function(i) calc_lprior_theta(train_test_data$X_train[i,], theta_prior_params))

# Posterior evaluations
lpost_theta_test <- llik_test + lprior_theta_test
lpost_theta_train <- llik_train + lprior_theta_train
```


```{r, include = FALSE}
# Predict with GPs
gp_pred <- predict_independent_GPs(theta_grid_preprocessed, gp_fits, gp_lib, cov_mat = FALSE, denormalize_predictions = TRUE,
                                   output_stats = train_test_data$output_stats, exponentiate_predictions = FALSE)
```

```{r, echo = FALSE}
#
# GP Predictive Density and GP approximated posterior
#

SSR_pred <- sapply(gp_pred, function(pred) pred$mean)
GP_pred_vars <- sapply(gp_pred, function(pred) pred$sd2)

# Predictive negative log density evaluated at true values
GP_losses <- matrix(nrow = N_test, ncol = length(test1_list$output_vars))
for(j in seq(1, ncol(GP_losses))) {
  GP_losses[,j] <- GP_pointwise_loss(SSR_test[,j], SSR_pred[,j], GP_pred_vars[,j])  
}

# GP approximated posterior
llik_pred_per_output <- sapply(seq(1, nrow(SSR_pred)), 
                               function(i) llik_Gaussian_SSR(SSR_pred[i,], diag(test1_list$Sig_eps), test1_list$n_obs, normalize = TRUE))
llik_pred <- colSums(llik_pred_per_output)
lpost_theta_pred <- llik_pred + lprior_theta_test
```

```{r, echo = FALSE}
#
# Evaluate loss integrals 
#

# integrate_loss_1d(GP_losses[,j], exp(post), d_theta)

```


```{r, echo = FALSE}
plot(theta_grid, lpost_theta_test, type = "l", col = "red", main = "True vs. Approximate Log Posterior", xlab = "theta", ylab = "Log Posterior")
lines(theta_grid, lpost_theta_pred, col = "blue")
points(train_test_data$X_train, lpost_theta_train, col = "red")
```


```{r}
j <- 4
plot(theta_grid, SSR_pred[,j], type = "l", col = "blue")
lines(theta_grid, SSR_test[,j], col = "red")
points(train_test_data$X_train, train_test_data$Y_train[,j], col = "red")
```

```{r, echo = FALSE}
plot(theta_grid, GP_losses[,4], type = "l")
```







