---
title: "VSEM Test Cases"
author: "Andrew Roberts"
date: '2023-03-05'
output: html_document
---

```{r, echo = FALSE, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(lhs)
library(hetGP)
library(mlegp)
library(data.table)
library(BayesianTools)

source("mcmc_calibration_functions.r")
```

# Review of the VSEM model

## Variables and Parameters

* $C_v$: Quantity of carbon ($C/m^2$) in above-ground vegetation pool. 
* $C_r$: Quantity of carbon ($C/m^2$) in below-ground vegetation (roots) pool.
* $C_s$: Quantity of carbon ($C/m^2$) in soil pool.
* $GPP$: Gross primary productivity. 
* $NPP$: Net Primary Productivity = GPP - Autotrophic Respiration. 
* $NEE$: -Net Ecosystem Exchange = NPP - Heterotrophic Respiration (notice differing sign convention between NPP and NEE). 
* $\tau_v$: Residence time of above-ground vegetation (days). 
* $\tau_r$: Residence time of above-ground vegetation (days). 
* $\tau_s$: Residence time of soil organic matter (days).
* $\alpha_v$: Fixed proportion of NPP allocated to above-ground vegetation. 
* $k$: The extinction coefficient in the Beer-Lambert law. 
* $LAI$: Leaf-Area Index, the ratio of one-sided leaf area per unit of ground area.
* $\gamma$: Fixed proportion of GPP lost to autotrophic respiration. 
* $LAR$: Leaf-area ratio. 

## State Equations

$$
\begin{align*}
\dot{C}_v(t) &= \alpha_v \times \text{NPP}(t) - \frac{C_v(t)}{\tau_v} \\
\dot{C}_r(t) &= (1.0 - \alpha_v) \times \text{NPP}(t) - \frac{C_r(t)}{\tau_r} \\
\dot{C}_s(t) &= \frac{C_r(t)}{\tau_r} + \frac{C_v(t)}{\tau_v} - \frac{C_s(t)}{\tau_s} 
\end{align*}
$$

## Model Driver
VSEM is driven by Photosynthetically Active Radiation (PAR) (MJ/$m^2$/day). GPP is assumed to result from a product of 

* The amount of light available for photosynthesis (PAR) (MJ/$m^2$/day).
* Light-use efficiency (LUE), a measure of the efficiency at which vegetation can use light for photosynthesis. 
* The rate at which the available light decays as it passes downwards through a canopy of leaves (described by the Beer-Lambert Law). 
$$
\begin{align*}
\text{LAI}(t) &= \text{LAR} \times C_v(t) \\
\text{GPP}(t) &= \text{PAR}(t) \times \text{LUE} \times \left(1 -  \exp\left(-k \times \text{LAI}(t) \right) \right) \\
\text{NPP}(t) &= (1 - \gamma) \times \text{GPP}(t)
\end{align*}
$$


# Test Examples

## Test 1
This is the simplest test case, with a single calibration parameter, all outputs observed daily with no missing values, and no output correlation. However, this test case still comes with a variety of challenges, including a high dynamic range for the sum of square errors. The observation variances represent a choice of low signal-to-noise ratio for the carbon pools; this is the same choice given in the example code provided by the BayesianTools package. Also note that the number of time steps (1000) here is smaller than all of the residence time/longevity parameters ($\tau_r$, $\tau_v$, $\tau_s$) so the quantity of carbon in each of the pools generally increases over this time frame. 
```{r}
random_seed_1 <- 1
test1_list <- generate_vsem_test_1(random_seed_1)
```


```{r, echo = FALSE}
print("Calibration Parameters:")
print(test1_list$ref_pars)
```

```{r, echo = FALSE}
print("Observation Covariance: ")
print(test1_list$Sig_eps)
```

### Plots of Simulated Observational Data and VSEM outputs.
The red line is the output from the numerical solution of the VSEM model, while the black plus signs are corrupted 
versions of this output, representing simulated observation noise. 
```{r echo = FALSE}
for(output_var in test1_list$output_vars) {
 plotTimeSeries(observed = test1_list$data_obs[, output_var],
                predicted = test1_list$data_ref[, output_var], main = output_var) 
}
```


## Test 2
Another one-parameter test (LUE), but adds complexity by varying the observation frequency. Also increases the number of days and varies the observation variances.  
```{r}
random_seed_2 <- 2
test2_list <- generate_vsem_test_2(random_seed_2)
```

```{r, echo = FALSE}
print("Calibration Parameters:")
print(test2_list$ref_pars)
```

```{r, echo = FALSE}
print("Observation Covariance: ")
print(test2_list$Sig_eps)
```

```{r echo = FALSE}
for(output_var in test2_list$output_vars) {
 plotTimeSeries(observed = test2_list$data_obs[, output_var],
                predicted = test2_list$data_ref[, output_var], main = output_var) 
}
```


## Test 3
This test builds on complexity to test 1 by adding an additional calibration parameter $\tau_v$, increasing the number of days, and varying the frequency of observed data. 
```{r}
random_seed_3 <- 3
test3_list <- generate_vsem_test_3(random_seed_3)
```

```{r, echo = FALSE}
print("Calibration Parameters:")
print(test3_list$ref_pars)
```

```{r, echo = FALSE}
print("Observation Covariance: ")
print(test3_list$Sig_eps)
```

```{r echo = FALSE}
for(output_var in test3_list$output_vars) {
 plotTimeSeries(observed = test3_list$data_obs[, output_var],
                predicted = test3_list$data_ref[, output_var], main = output_var) 
}
```


## Test 4
Another two-parameter test case ($C_v$, $\tau_r$), with a longer time series, varying observation frequencies, increased noise on NEE observations, and small correlation between observation errors for $C_s$ and $C_r$. 
```{r}
random_seed_4 <- 4
test4_list <- generate_vsem_test_4(random_seed_4)
```

```{r, echo = FALSE}
print("Calibration Parameters:")
print(test4_list$ref_pars)
```

```{r, echo = FALSE}
print("Observation Covariance: ")
print(test4_list$Sig_eps)
```

```{r echo = FALSE}
for(output_var in test4_list$output_vars) {
 plotTimeSeries(observed = test4_list$data_obs[, output_var],
                predicted = test4_list$data_ref[, output_var], main = output_var) 
}
```

# Some Initial Thoughts on Error Metrics to Evaluate GP Emulators
I suppose there are really two questions here: 1.) developing error metrics to use in tests when the likelihood parameters are known and 
2.) developing error metrics that can be used in practice to evaluate the emulator fit. Though we are emulating the sum of squared errors $T(\theta)$ 
(or its log $L(\theta)$) what we are really interested in is the effect of the emulator approximation on the posterior distribution 
$$\pi(\theta) \propto \pi_0(\theta)p(Y|\theta) = \pi_0(\theta)L(\theta) = \pi_0(\theta)\prod_{j = 1}^{p} N_{N_j}(Y_j|f_j(\theta), \sigma_j^2 I_{N_j})$$
Note that I've started writing $N_j$ to specify that sample sizes may be different across outputs, given the fact that significant data imbalances are something we will have to be thinking about. 

In particular, the posterior approximation $\hat{\pi}(\theta) \propto \pi_0(\theta)\hat{L}(\theta)$ is utilized in the MCMC calibration algorithm in two places. 

1. The Accept-Reject ratio in Metropolis-Hastings.
2. Sampling from an Inverse Gamma in the Gibbs update for the observation variance.  

I briefly review this below. 

## Review of MCMC scheme
We assume priors 
$$\sigma_j^2 \overset{ind}{\sim} IG(a_j, b_j)$$
which results in a conditional posterior $p(\sigma_j^2|\theta, Y) = IG(\sigma_j^2|a_j + N_j/2, b_j + T_j(\theta)/2)$ and thus the opportunity for a Metropolis-within-Gibbs scheme. 

### Updating $\theta$
Conditional on $\Sigma := \text{diag}(\sigma_1^2, \dots, \sigma_p^2)$, the Metropolis step for updating $\theta$ results in an acceptance ratio (where $q(\cdot, \cdot)$ is the proposal density), 
$$
\begin{align*}
\alpha(\theta, \theta^\prime) &= \frac{p(\theta^\prime|Y, \Sigma)q(\theta^\prime, \theta)}{p(\theta|Y, \Sigma)q(\theta, \theta^\prime)} \\
                              &= \frac{\pi_0(\theta^\prime)p(Y|\theta^\prime, \Sigma)q(\theta^\prime, \theta)}{\pi_0(\theta)p(Y|\theta, \Sigma)q(\theta, \theta^\prime)} \\
                              &= \frac{\pi_0(\theta^\prime)q(\theta^\prime, \theta) \prod_{j = 1}^{p} N_{N_j}(Y_j|f_j(\theta^\prime), \sigma_j^2 I_{N_j})}{\pi_0(\theta)q(\theta, \theta^\prime) \prod_{j = 1}^{p} N_{N_j}(Y_j|f_j(\theta), \sigma_j^2 I_{N_j})} \\
                              &= \frac{\pi_0(\theta^\prime)q(\theta^\prime, \theta)}{\pi_0(\theta)q(\theta, \theta^\prime)}\prod_{j = 1}^{p} \exp\left\{-\frac{1}{2\sigma_j^2} (T_j(\theta^\prime) - T_j(\theta))\right\} \\
                              &= \frac{\pi_0(\theta^\prime)q(\theta^\prime, \theta)}{\pi_0(\theta)q(\theta, \theta^\prime)} \exp\left\{ -\frac{1}{2} \sum_{j = 1}^{p} \left(\frac{T_j(\theta^\prime) - T_j(\theta)}{\sigma_j^2}\right)\right\}
\end{align*}
$$
The current PEcAn approach is to approximate $\alpha(\theta, \theta^\prime)$ by sampling from the GP emulator
$$
&\tilde{T}_j^\prime \sim N(\mu^*_j(\theta^\prime), k_j^*(\theta^\prime)) \\
&\tilde{T}_j \sim N(\mu^*_j(\theta), k_j^*(\theta))
$$
and then replacing $T_j(\theta^\prime)$ and $T_j(\theta)$ with $\tilde{T}_j^\prime$ and $\tilde{T}_j$, respectively. We have also discussed accounting for GP predictive covariance by drawing these samples jointly 
$$
\begin{pmatrix}T_j(\theta^\prime) \\ T_j(\theta) \end{pmatrix} \sim N_2\left(\mu_j^*\begin{pmatrix} \theta^\prime \\ \theta\end{pmatrix}, k_j^*\begin{pmatrix} \theta^\prime \\ \theta\end{pmatrix} \right)

$$


Some things to think about: 

* Metric takes into account 
* Multiple outputs/data constraints define the likelihood, with the error for each output being approximated by independent GPs. When the observation 
variances are unknown, it is not clear how to weight the emulator error for each output. 



# Questions/Next Steps

* Making test cases more realistic/useful. 
    + Magnitude of observation noise? Correlation in observation errors across output variables? 
    + Altering observation frequencies. 
    + Staggering observation frequencies so they don't occur on regular periodic schedule. 
    + Add in missing data due to simulate sensor issues, etc. 



