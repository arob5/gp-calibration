---
title: "VSEM Test Cases"
author: "Andrew Roberts"
date: '2023-03-05'
output: html_document
---

```{r, echo = FALSE, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(lhs)
library(hetGP)
library(mlegp)
library(data.table)
library(BayesianTools)

source("mcmc_calibration_functions.r")
```

# Review of the VSEM model

## Variables and Parameters

* $C_v$: Quantity of carbon ($C/m^2$) in above-ground vegetation pool. 
* $C_r$: Quantity of carbon ($C/m^2$) in below-ground vegetation (roots) pool.
* $C_s$: Quantity of carbon ($C/m^2$) in soil pool.
* $GPP$: Gross primary productivity. 
* $NPP$: Net Primary Productivity = GPP - Autotrophic Respiration. 
* $NEE$: -Net Ecosystem Exchange = NPP - Heterotrophic Respiration (notice differing sign convention between NPP and NEE). 
* $\tau_v$: Residence time of above-ground vegetation (days). 
* $\tau_r$: Residence time of above-ground vegetation (days). 
* $\tau_s$: Residence time of soil organic matter (days).
* $\alpha_v$: Fixed proportion of NPP allocated to above-ground vegetation. 
* $k$: The extinction coefficient in the Beer-Lambert law. 
* $LAI$: Leaf-Area Index, the ratio of one-sided leaf area per unit of ground area.
* $\gamma$: Fixed proportion of GPP lost to autotrophic respiration. 
* $LAR$: Leaf-area ratio. 

## State Equations

$$
\begin{align*}
\dot{C}_v(t) &= \alpha_v \times \text{NPP}(t) - \frac{C_v(t)}{\tau_v} \\
\dot{C}_r(t) &= (1.0 - \alpha_v) \times \text{NPP}(t) - \frac{C_r(t)}{\tau_r} \\
\dot{C}_s(t) &= \frac{C_r(t)}{\tau_r} + \frac{C_v(t)}{\tau_v} - \frac{C_s(t)}{\tau_s} 
\end{align*}
$$

## Model Driver
VSEM is driven by Photosynthetically Active Radiation (PAR) (MJ/$m^2$/day). GPP is assumed to result from a product of 

* The amount of light available for photosynthesis (PAR) (MJ/$m^2$/day).
* Light-use efficiency (LUE), a measure of the efficiency at which vegetation can use light for photosynthesis. 
* The rate at which the available light decays as it passes downwards through a canopy of leaves (described by the Beer-Lambert Law). 
$$
\begin{align*}
\text{LAI}(t) &= \text{LAR} \times C_v(t) \\
\text{GPP}(t) &= \text{PAR}(t) \times \text{LUE} \times \left(1 -  \exp\left(-k \times \text{LAI}(t) \right) \right) \\
\text{NPP}(t) &= (1 - \gamma) \times \text{GPP}(t)
\end{align*}
$$


# Test Examples

## Test 1
This is the simplest test case, with a single calibration parameter, all outputs observed daily with no missing values, and no output correlation. However, this test case still comes with a variety of challenges, including a high dynamic range for the sum of square errors. The observation variances represent a choice of low signal-to-noise ratio for the carbon pools; this is the same choice given in the example code provided by the BayesianTools package. Also note that the number of time steps (1000) here is smaller than all of the residence time/longevity parameters ($\tau_r$, $\tau_v$, $\tau_s$) so the quantity of carbon in each of the pools generally increases over this time frame. 
```{r}
random_seed_1 <- 1
test1_list <- generate_vsem_test_1(random_seed_1)
```


```{r, echo = FALSE}
print("Calibration Parameters:")
print(test1_list$ref_pars)
```

```{r, echo = FALSE}
print("Observation Covariance: ")
print(test1_list$Sig_eps)
```

### Plots of Simulated Observational Data and VSEM outputs.
The red line is the output from the numerical solution of the VSEM model, while the black plus signs are corrupted 
versions of this output, representing simulated observation noise. 
```{r echo = FALSE}
for(output_var in test1_list$output_vars) {
 plotTimeSeries(observed = test1_list$data_obs[, output_var],
                predicted = test1_list$data_ref[, output_var], main = output_var) 
}
```


## Test 2
Another one-parameter test (LUE), but adds complexity by varying the observation frequency. Also increases the number of days and varies the observation variances.  
```{r}
random_seed_2 <- 2
test2_list <- generate_vsem_test_2(random_seed_2)
```

```{r, echo = FALSE}
print("Calibration Parameters:")
print(test2_list$ref_pars)
```

```{r, echo = FALSE}
print("Observation Covariance: ")
print(test2_list$Sig_eps)
```

```{r echo = FALSE}
for(output_var in test2_list$output_vars) {
 plotTimeSeries(observed = test2_list$data_obs[, output_var],
                predicted = test2_list$data_ref[, output_var], main = output_var) 
}
```


## Test 3
This test builds on complexity to test 1 by adding an additional calibration parameter $\tau_v$, increasing the number of days, and varying the frequency of observed data. 
```{r}
random_seed_3 <- 3
test3_list <- generate_vsem_test_3(random_seed_3)
```

```{r, echo = FALSE}
print("Calibration Parameters:")
print(test3_list$ref_pars)
```

```{r, echo = FALSE}
print("Observation Covariance: ")
print(test3_list$Sig_eps)
```

```{r echo = FALSE}
for(output_var in test3_list$output_vars) {
 plotTimeSeries(observed = test3_list$data_obs[, output_var],
                predicted = test3_list$data_ref[, output_var], main = output_var) 
}
```


## Test 4
Another two-parameter test case ($C_v$, $\tau_r$), with a longer time series, varying observation frequencies, increased noise on NEE observations, and small correlation between observation errors for $C_s$ and $C_r$. 
```{r}
random_seed_4 <- 4
test4_list <- generate_vsem_test_4(random_seed_4)
```

```{r, echo = FALSE}
print("Calibration Parameters:")
print(test4_list$ref_pars)
```

```{r, echo = FALSE}
print("Observation Covariance: ")
print(test4_list$Sig_eps)
```

```{r echo = FALSE}
for(output_var in test4_list$output_vars) {
 plotTimeSeries(observed = test4_list$data_obs[, output_var],
                predicted = test4_list$data_ref[, output_var], main = output_var) 
}
```

# Some Initial Thoughts on Error Metrics to Evaluate GP Emulators
I suppose there are really two questions here: 1.) developing error metrics to use in tests when the likelihood parameters are known and 
2.) developing error metrics that can be used in practice to evaluate the emulator fit. Though we are emulating the sum of squared errors $T(\theta)$ 
(or its log $L(\theta)$) what we are really interested in is the effect of the emulator approximation on the posterior distribution 
$$\pi(\theta) \propto \pi_0(\theta)p(Y|\theta) = \pi_0(\theta)L(\theta) = \pi_0(\theta)\prod_{j = 1}^{p} N_{N_j}(Y_j|f_j(\theta), \sigma_j^2 I_{N_j})$$
Note that I've started writing $N_j$ to specify that sample sizes may be different across outputs, given the fact that significant data imbalances are something we will have to be thinking about. 

In particular, the posterior approximation $\hat{\pi}(\theta) \propto \pi_0(\theta)\hat{L}(\theta)$ is utilized in the MCMC calibration algorithm in two places. 

1. The Accept-Reject ratio in Metropolis-Hastings.
2. Sampling from an Inverse Gamma in the Gibbs update for the observation variance.  

I briefly review this below. 

## Review of MCMC scheme
We assume priors 
$$\sigma_j^2 \overset{ind}{\sim} IG(a_j, b_j)$$
which results in a conditional posterior $p(\sigma_j^2|\theta, Y) = IG(\sigma_j^2|a_j + N_j/2, b_j + T_j(\theta)/2)$ and thus the opportunity for a Metropolis-within-Gibbs scheme. 

### Updating $\theta$
Conditional on $\Sigma := \text{diag}(\sigma_1^2, \dots, \sigma_p^2)$, the Metropolis step for updating $\theta$ results in an acceptance ratio (where $q(\cdot, \cdot)$ is the proposal density), 
$$
\begin{align*}
\alpha(\theta, \theta^\prime) &= \frac{p(\theta^\prime|Y, \Sigma)q(\theta^\prime, \theta)}{p(\theta|Y, \Sigma)q(\theta, \theta^\prime)} \\
                              &= \frac{\pi_0(\theta^\prime)p(Y|\theta^\prime, \Sigma)q(\theta^\prime, \theta)}{\pi_0(\theta)p(Y|\theta, \Sigma)q(\theta, \theta^\prime)} \\
                              &= \frac{\pi_0(\theta^\prime)q(\theta^\prime, \theta) \prod_{j = 1}^{p} N_{N_j}(Y_j|f_j(\theta^\prime), \sigma_j^2 I_{N_j})}{\pi_0(\theta)q(\theta, \theta^\prime) \prod_{j = 1}^{p} N_{N_j}(Y_j|f_j(\theta), \sigma_j^2 I_{N_j})} \\
                              &= \frac{\pi_0(\theta^\prime)q(\theta^\prime, \theta)}{\pi_0(\theta)q(\theta, \theta^\prime)}\prod_{j = 1}^{p} \exp\left\{-\frac{1}{2\sigma_j^2} (T_j(\theta^\prime) - T_j(\theta))\right\} \\
                              &= \frac{\pi_0(\theta^\prime)q(\theta^\prime, \theta)}{\pi_0(\theta)q(\theta, \theta^\prime)} \exp\left\{ -\frac{1}{2} \sum_{j = 1}^{p} \left(\frac{T_j(\theta^\prime) - T_j(\theta)}{\sigma_j^2}\right)\right\}
\end{align*}
$$
The current PEcAn approach is to approximate $\alpha(\theta, \theta^\prime)$ by sampling from the GP emulator

$$
\begin{align*}
&\tilde{T}_j^\prime \sim N(\mu^*_j(\theta^\prime), k_j^*(\theta^\prime)) \\
&\tilde{T}_j \sim N(\mu^*_j(\theta), k_j^*(\theta))
\end{align*}
$$
and then replacing $T_j(\theta^\prime)$ and $T_j(\theta)$ with $\tilde{T}_j^\prime$ and $\tilde{T}_j$, respectively. We have also discussed accounting for GP predictive covariance by drawing these samples jointly 
$$
\begin{pmatrix}T_j(\theta^\prime) \\ T_j(\theta) \end{pmatrix} \sim N_2\left(\mu_j^*\begin{pmatrix} \theta^\prime \\ \theta\end{pmatrix}, k_j^*\begin{pmatrix} \theta^\prime \\ \theta\end{pmatrix} \right)
$$
Note that a deterministic, rather than a sampling-based approach is also possible here since we know the distribution
$$T^*_j(\theta^\prime) - T^*_j(\theta) \sim N\left(\mu_j^*(\theta^\prime) - \mu_j^*(\theta), k_j^*(\theta^\prime) + k_j^*(\theta) - 2k_j^*(\theta^\prime, \theta)\right)$$
In fact, under the GP approximations, the whole exponential term in $\alpha(\theta, \theta^\prime)$ is log-normal

$$\exp\left\{ -\frac{1}{2} \sum_{j = 1}^{p} \left(\frac{T^*_j(\theta^\prime) - T^*_j(\theta)}{\sigma_j^2}\right)\right\} \sim \text{LN}$$
so we could consider an analytical approximation like
$$
\hat{\alpha}(\theta, \theta^\prime) = \frac{\pi_0(\theta^\prime)q(\theta^\prime, \theta)}{\pi_0(\theta)q(\theta, \theta^\prime)} \mathbb{E} \left[\exp\left\{ -\frac{1}{2} \sum_{j = 1}^{p} \left(\frac{T^*_j(\theta^\prime) - T^*_j(\theta)}{\sigma_j^2}\right)\right\}\right]
$$
When we instead emulate the log of the $T_j$ using a GP, these analytical expressions go away, but the sampling approach still works. 

### Updating $\Sigma$
Recall that the observation variances are updated by sampling 
$$\sigma_j^2 \sim IG(a_j + N_j/2, b_j + T_j(\theta)/2)$$
The current PEcAn approach is to sample $\tilde{T}_j \sim T_j^*(\theta)$ and then sample from the resulting approximate distribution
$$ \sigma_j^2 \sim IG(a_j + N_j/2, b_j + \tilde{T}_j/2) $$
The same procedure can of course be used if we instead emulate the log of $T_j$. 


## Some general notes on potential metrics

* We want the metric to take into account the GP predictive distribution 
* Also want the metric to take into account the true posterior (in the case when it is known)
* Multiple outputs/data constraints define the likelihood, with the error for each output being approximated by independent GPs. When the observation variances are unknown, it is not clear how to weight the emulator error for each output. 

## Metrics for numerical experiments (known posterior)
### GP Emulator

I'll just consider a single-output case for simplicity, where the true vector of squared errors evaluated over some validation points $\theta_1, \dots, \theta_M$, 
$$ T := \left[T(\theta_1), \dots, T(\theta_M) \right]^T$$
is approximated by 
$$ T^* = \left[T^*(\theta_1), \dots, T^*(\theta_M) \right]^T$$ 
where $T^* \sim \mathcal{GP}(\mu^*, k^*)$. Note that the approximation $T^*$ is stochastic, so metrics will be defined by integrating over the randomness in some manner. A first-pass idea that takes into account the true posterior $\pi(\theta)$ and the pointwise predictive GP distributions (but not GP predictive covariances) is: 
$$ 
\begin{align*}
\sum_{i = 1}^{M} \pi(\theta_i) \mathbb{E}_{T^*(\theta_i)}\left[\left(T(\theta_i) - T^*(\theta_i)\right)^2\right] &= \sum_{i = 1}^{M} \pi(\theta_i) \left[k^*(\theta_i) + (\mu_*(\theta_i) - T(\theta_i))^2 \right] \\ 
\end{align*}
$$
However, this has the effect of punishing emulators with larger predictive variances, even if those predictive variances are well-calibrated; in other words, it can favor over-confident emulators. Moreover, the concept of "well-calibrated variances" isn't really even well-defined in the setting of emulating deterministic functions. An alternative metric that treats the predictive variances in the opposite way is
$$ \sum_{i = 1}^{M} \pi(\theta_i) \left(\frac{T(\theta_i) - \mu^*(\theta_i)}{\sqrt{k^*(\theta_i)}}\right)^2$$ 
Now the squared deviation is penalized more when the predictive variance is smaller. 

Other than this weighted L2 type approach, we might consider metrics quantifying the posterior approximation to $\pi(\theta)$ directly, or alternatives that specialize the error metric to the use cases of the approximation detailed above. 

### LNP Emulator


# Questions/Next Steps

* Making test cases more realistic/useful. 
    + Magnitude of observation noise? Correlation in observation errors across output variables? 
    + Altering observation frequencies. 
    + Staggering observation frequencies so they don't occur on regular periodic schedule. 
    + Add in missing data due to simulate sensor issues, etc. 
* Error metrics. 
    + Need to do a literature review. 



